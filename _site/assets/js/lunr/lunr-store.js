var store = [{
        "title": "Useful tool for multiple language deployment",
        "excerpt":" For a current project I had to deploy some generated lists (stp templates) on another version of SharePoint (Dutch version). Because I’m a beginning programmer in SharePoint I didn’t know that that would cause a problem. Deploying generated list files or sites cannot be deployed to a SharePoint environment in another language.  I could upload the files but using the list template wasn’t able because it was created in another LocaleID. I googled and I found a really useful tool for converting stp template files.  The tool is called .STP Language Converter and can be downloaded from this site:  http://www.kwizcom.com/ContentPage.asp?PageId=165   ","categories": ["SharePoint","Tools"],
        "tags": ["Deployment","Multiple languages","Tools"],
        "url": "/2008/03/useful-tool-for-multiple-language-deployment/",
        "teaser": null
      },{
        "title": "Silverlight in SharePoint",
        "excerpt":" At the SharePoint Conference 2008, Bill Gates announced that it is possible to display applications that are developed in Silverlight in SharePoint 2007.  Employees of U2U developed a blueprint for displaying these applications. At the following site: SharePoint Blueprint you will find different samples and the explanation to get Silverlight working in you MOSS environment.   ","categories": ["SharePoint"],
        "tags": ["SharePoint 2007","Silverlight"],
        "url": "/2008/03/silverlight-in-sharepoint/",
        "teaser": null
      },{
        "title": "Disabling SharePoint errors (An unexpected error has occurred)",
        "excerpt":" One of the most annoying things I came across when I was developing for SharePoint 2007 is the error: “An unexpected error has occurred”. You can get rid of this by disabling the custom errors.  You can do this in two steps:  Step 1: Change  &lt;SafeMode MaxControls=\"200\" CallStack=\"false\" &gt; to  &lt;SafeMode MaxControls=\"200\" CallStack=\"true\" &gt; Step 2:  &lt;customerrors mode=\"Off\" /&gt; If you change these settings in the web.config file you will get the ASP.Net error page with the call stack. You can change this in de web.config file of you web application or you can just edit the default web.config file on you development machine.  You can find your default web.config file in:  C:\\Program Files\\Common Files\\Microsoft Shared\\web server extensions\\12\\CONFIG   ","categories": ["SharePoint","Troubleshooting"],
        "tags": ["Error","SharePoint"],
        "url": "/2008/04/disabling-sharepoint-errors-an-unexpected-error-has-occurred/",
        "teaser": null
      },{
        "title": "Useful things for your SharePoint VPC",
        "excerpt":" On several sites there are postings about tools you could have on you SharePoint development VPC. That’s why I thought lets post what I have got on my development VPC. If you do not have a Virtual PC with SharePoint you can take a look at the following posts of Tony Zink. He made 20 posts for making your own VPC with SharePoint (I made my VPC with that article):  How to Create a MOSS 2007 VPC Image, The whole nine yards  In the article you will see that you have to run windows update so that you have all the current updates available for windows 2003. I mainly work in an environment where I can’t access windows update, when you are also in an environment like this you can try: Autopatcher  If you have you SharePoint VPC some useful tools can be installed on it to make development much easier:   SharePoint Server SDK: SharePoint Software Development Kit.   Windows SharePoint services SDK: WSS Software Development Kit.   Windows SharePoint Services 3.0 Tools: Visual Studio 2005 Extensions: Extension for Visual Studio 2005 with Web part and Site Templates Templates.   U2U Caml Creator: Useful tool for creating Caml Queries.   SPDisposeCheck : Checking tool for checking if you do not have a memory leak in your code.   MOSS Feature Generator: Useful tool for generation of features.   Visual Studio 2005 extensions for .NET Framework 3.0 (Windows Workflow Foundation): Extension for Visual studio 2005 for working with Workflows.   .STP Language convertor: Convertor of STP files if you want to deploy you STP files on another language version of SharePoint.   ","categories": ["Administration"],
        "tags": ["SharePoint","Tools","VPC"],
        "url": "/2008/04/useful-things-for-your-sharepoint-vpc/",
        "teaser": null
      },{
        "title": "Enabling and Using site information in an ItemAdding event of a document library",
        "excerpt":"In SharePoint 2007 you are able to write custom event handlers for a document library. Some of the different events you can rewrite are:  ItemAdding: Before an item is added to the list.   ItemAdded: When an item is added to the list.  Besides these item events there are also field events. When you would like to use your own custom events you have to populate the document library with your custom event handler. But first we will try to write an itemAdding event with information from the session and put that information in a Lookup field. In visual studio you have to create a class file and name it however you like. In this example I have called it ItemEventReceiver. The class has to inherit from SPItemEventReceiver.   public class ItemEventReceiver : SPItemEventReceiver  {     private HttpContext hContext = null;      public ItemEventReceiver()          : base()     {        hContext = HttpContext.Current;     }  } &nbsp;  In the base class you have the hContext to retrieve the current HttpContext. This I have done so that you can retrieve information from the session. The next step is to create the event for the ItemAdding event like below.   //ItemAdding event  public override void ItemAdding(SPItemEventProperties properties)  {     try     {        //Retrieve the value from the Session and Convert it to an integer        int ID = Convert.ToInt32(hContext.Session[\"SessionValue\"]);         //If the Value is 0 make the value 1        if (ID == 0)        {           ID = 1;        }         string InternalName = ““;        using (SPWeb web = properties.OpenWeb())        {           //Retrieve the internalname of you field were you would like to put the information in.           InternalName = web.Lists[properties.ListId].Fields[\"InternalFieldName\"].InternalName;        }         //Create a new SPFieldLookupValue        SPFieldLookupValue Value = new SPFieldLookupValue(ID, ID.ToString());         //Save the SPFieldLookupValue.ToString() in the afterproperties        properties.AfterProperties[InternalName] = Value.ToString();        base.ItemAdding(properties);     }     catch(Exception x)     {         //Write to the EventLog with my own logging class         Logging.logmessage(x.Message, System.Diagnostics.EventLogEntryType.Error, “Sharepoint“);     }  } Place you code in a try catch block like the code above. When something goes wrong the error is written to the EventLog. The integer ID is used to save the ID that is saved in the session under the name ‘SessionValue’. The value of ID is checked if it is not 0 if it is 0 the value is changed to 1. After that the field where you want to input an item is retrieved from the SPWeb. The next step is to make a new SPFieldLookupValue with the ID of the item you want to use and the String Value of that item. The field values for the list item are saved in the SPItemEventProperties.afterproperties.  They are saved by the internal name of the columns. This means you have to use the Internal name of the field item were in you want to fill in the information and use the SPFieldLookupValue you made to fill this item. You only have to convert it to a string. After that you perform the base actions of the event.  Now we have created the event you have to add the event to the document library. You can do this trough code and make it into a feature. To do this as a feature you will have to create a SPFeatureReceiver and override the FeatureActivated and FeatureDeactivating. Besides that you also have to implement the FeatureInstalled and FeatureDeinstalled and leave them empty.   //Feature activating and feature deactivating  public override void FeatureActivated(SPFeatureReceiverProperties properties)  {     SPSite siteCollection = SPContext.Current.Site;     SPWeb web = (SPWeb)properties.Feature.Parent;     SPList list = web.Lists[\"DocumentLibrary\"];      //AssemblyName     string ReceiverAssembly = “AssemblyName, Version=1.0.0.0, Culture=neutral, PublicKeyToken=e31c161dfa4a406af“;      //ClassName     string ReceiverClass = “ClassName“;      //Add the EventReceiver to the list     list.EventReceivers.Add(SPEventReceiverType.ItemAdding, ReceiverAssembly, ReceiverClass);      //Update the list     list.Update();  }   public override void FeatureDeactivating(SPFeatureReceiverProperties properties)  {     SPSite siteCollection = SPContext.Current.Site;     SPWeb web = (SPWeb)properties.Feature.Parent;     SPList list = web.Lists[\"DocumentList\"];      //AssemblyName     string ReceiverAssembly =”AssemblyName, Version=1.0.0.0, Culture=neutral, PublicKeyToken=e31c161dfa4a406af“;      //Loop trough the EventReceivers ","categories": ["Development","SharePoint"],
        "tags": ["Document Library","Events","ItemAdding"],
        "url": "/2008/05/enabling-and-using-site-information-in-an-itemadding-event-of-a-document-library/",
        "teaser": null
      },{
        "title": "Inserting initial information in a site definition",
        "excerpt":" For a current project I had to figure out how to place information in a list in SharePoint when you are creating a new site from a site template and place documents in a document library.  After a lot of searching on the internet I found some helpful stuff. Placing initial information in a list can also be found on the MSDN site.  First step is to create a new site definition. This can be done by copying an out of the box Template and adjusting it the way you would like it to be. More information about creating a site definition you can find at:  http://www.sharepointblogs.com/tbaginski/archive/2007/08/16/creating-a-custom-site-definition-in-wss-v3-moss.aspx  This can also be a helpful link:  http://blah.winsmarts.com/2006-12-Registering_your_custom_site_definitions_in_SharePoint_2007.aspx  The way for placing initial information in a SharePoint list is done by finding the right list in the onet.xml were in you would like to place the information.  For example you have to find a tag in your ONET.xml like the one below:   &lt;!--List information--&gt;  &lt;List FeatureId=&quot;1BFC6CD6-4941-4964-A143-25E62AAEF642&quot; Type=&quot;107&quot; Title=&quot;Taken&quot; Url=&quot;Lists/Taken&quot; QuickLaunchUrl=&quot;Lists/Taken/AllItems.aspx&quot; &gt;&lt;/List&gt; For placing information in that list you have to know the columns that that list uses and place it between the &lt;list&gt; and &lt;/list&gt; with the &lt;data&gt; tag.  &lt;!--Inserting initial information--&gt;  &lt;List FeatureId=&quot;1BFC6CD6-4941-4964-A143-25E62AAEF642&quot; Type=&quot;107&quot; Title=&quot;Taken&quot; Url=&quot;Lists/Taken&quot; QuickLaunchUrl=&quot;Lists/Taken/AllItems.aspx&quot; &gt;  &lt;Data&gt;   &lt;Rows&gt;    &lt;Row&gt;     &lt;Field Name=&quot;Title&quot;&gt;Task 1&lt;/Field&gt;    &lt;/Row&gt;    &lt;Row&gt;     &lt;Field Name=&quot;Title&quot;&gt;Task 2&lt;/Field&gt;    &lt;/Row&gt;    &lt;Row&gt;     &lt;Field Name=&quot;Title&quot;&gt;Task 3&lt;/Field&gt;    &lt;/Row&gt;    &lt;Row&gt;     &lt;Field Name=&quot;Title&quot;&gt;Task 4&lt;/Field&gt;    &lt;/Row&gt;    &lt;Row&gt;     &lt;Field Name=&quot;Title&quot;&gt;Task 5&lt;/Field&gt;    &lt;/Row&gt;   &lt;/Rows&gt;  &lt;/Data&gt;  &lt;/List&gt; This example will place five new lines in the Task list with all different titles. If you would like to fill in more items for a task, you would just have to add more field items between a row tag. Placing documents into a site is done a little bit different.  For placing these documents in the right document library you have to add a folder to you site definition with a name you are just thinking off. For this example I choice the folder name doctemp. To place the documents into a document library you have to make a module in the onet.xml file. And add it to the right configuration.   WordDocuments is the name I created for my module. You have to specify the module at the end of the onet.xml between the &lt;modules&gt; tag just before the end tag of projects. The module you could enter there could be something like this:   &lt;!--Inplementing module--&gt;  &lt;Module Name=&quot;WordDocuments&quot; List=&quot;101&quot; Url=&quot;Documents&quot; Path=&quot;docTemp&quot;&gt;  &lt;File Name=&quot;Test.doc&quot;  Url=&quot;Test.doc&quot; Type=&quot;GhostableInLibrary&quot; /&gt;  &lt;/Module&gt; The explanation of the used properties:    Name: The name of the module.  List: The list type of the list were you would like to insert the documents.  Url: The URL to the list were you would like to insert the documents.  Path: The path to the files  File Name: The file name you would like to use.  File Url: The file name you would like to insert.  Type GhostableInLibrary: tells SharePoint to put an entry in the document library for this file, but this file will not be stored in the database it will stay on the file system.  So now you are ready to make a site template with initial information.   ","categories": ["Design"],
        "tags": ["Onet","Site definition"],
        "url": "/2008/05/inserting-initial-information-in-a-site-definition/",
        "teaser": null
      },{
        "title": "Placing webparts in the webpart gallery when you create a site.",
        "excerpt":" Normally when you develop webparts you have to place the webparts in the webpart gallery before you can add them to your page. When you develop webparts for a customer this isn’t something you want. So I googled around and found out that you can also add them to the webpart gallery in your onet.xml of the site you are going to create. To do this you have to add a module just like I explained in my blog post Inserting initial information in a site definition,so this is kind of a part two of that blog post. First of all you add a module to the configuration in the onet.xml that you are going to use like this:  &lt;Module Name=\"WebPartPopulation\" /&gt; ‘WebPartGalleryPopulation’ is the name I created for my module. You have to specify the module at the end of the onet.xml between the tag just before the end tag of projects. The module you could enter there could be something like this:    &lt;Module Name=\"WebPartPopulation\" List=\"113\" Url=\"_catalogs/wp\" Path=\"lists\\wplib\\dwp\" RootWebOnly=\"TRUE\"&gt;     &lt;File Url=\"MSContentEditor.dwp\" Type=\"GhostableInLibrary\" /&gt;  &lt;/Module&gt; [sourcecode lang=\"xml\"] [/sourcecode] The attributes that you add are described below:    URL:The directory off the web part gallery.  Path: Specifies the path of the webpart files. This can be files with the dwp extension or the webpart extension. To get this files you can create a export of your webpart in the user environment of SharePoint.  RootWebOnly: If you set this option to true is specifies that the files will be created only for the top-level site of a site collection.  So know you can create a site definition with webpart already populated in the webpart gallery.  &nbsp;  Happy SharePointing.   ","categories": ["Development","SharePoint"],
        "tags": ["Onet","Site definition","Web Part"],
        "url": "/2008/07/placing-webparts-in-the-webpart-gallery-when-you-create-a-site/",
        "teaser": null
      },{
        "title": "How to add and read properties from the feature.xml",
        "excerpt":" I knew it is was possible to provide features with properties but I never found a useful situation till a few days ago.   So know is the question how do you add properties to your feature.xml? You can accomplish this by adding a properties tag to the feature.   &lt;?xml version=\"1.0\" encoding=\"utf-8\" ?&gt;  &lt;Feature Id=\"FeatureID\"               Title=\"FeaturePropertiesExample\"               Description=\"Example for working with feature properties\"      Version=\"1.0.0.1\"      Scope=\"Web\"      Hidden=\"FALSE\"      ReceiverAssembly=\"Assembly, Version=1.0.0.0, Culture=neutral, PublicKeyToken=Token\"               ReceiverClass=\"The Receiver class\"      xmlns=\"http://schemas.microsoft.com/sharepoint/\"&gt;  &lt;ElementManifests /&gt;      &lt;Properties&gt;          &lt;Property Key =\"Variation\" Value=\"Dutch\" /&gt;          &lt;Property Key =\"VariationLabel\" Value=\"nl-NL\" /&gt;      &lt;/Properties&gt;  &lt;/Feature&gt; In this example I saved the variation label I want to add to the variation.  If you would Activate the feature you can read out the Property Variation and Variation Label like this:  /// &lt;summary&gt;  /// Feature Activation  /// &lt;/summary&gt;  ///&lt;param name=\"properties\"&gt;Feature properties&lt;/param&gt;  public override void FeatureActivated(SPFeatureReceiverProperties properties)  {     //Get property collection     SPFeaturePropertyCollection featureProperties = properties.Feature.Properties;      //Get seperate properties with the key     string variationDisplayName = feautureProperties[Variation].toString();     string variationLabel= feautureProperties[VariationLabel].toString();  }   ","categories": ["Development","SharePoint"],
        "tags": ["Feature"],
        "url": "/2008/08/how-to-add-and-read-properties-from-the-feature-xml/",
        "teaser": null
      },{
        "title": "Windows SharePoint Services on Vista",
        "excerpt":" It has been there for a while WSS on Vista but I never tried it out. Last night I had some spare time so I thought let’s give it a try on my home desktop machine where I’m running Vista Business.  At first I thought I would get in a lot of trouble installing WSS on my Vista Machine, but after installing IIS it all went like a charm. After IIS I downloaded SQL 2008 Express edition from the Microsoft site and installed it on my machine. After that I was ready to begin with the installation of WSS.  I followed the steps that are explained on the Community site of Bamboo:  BamBoo How To Install WSS on Vista  After I followed those steps it worked great and I am now using my desktop PC for development against the SharePoint object model.  Tanks Bamboo for this great solution, and for all the SharePoint guys that have vista installed, you should really try this out!   ","categories": ["Administration"],
        "tags": ["Windows Vista","WSS 3.0"],
        "url": "/2008/09/windows-sharepoint-services-on-vista/",
        "teaser": null
      },{
        "title": "This Page has been modified since you opened it. You must open the page again.",
        "excerpt":" In one of our development environments we came across a very annoying error in SharePoint, when we were using a custom site definition:  “This Page has been modified since you opened it. You must open the page again.”  This error was getting very annoying because we came across it on a lot of site, and we had no idea how to solve this error.  After searching a lot and trying out various things we came across a difference between our custom definitions and the default definitions from Microsoft.  When you are using publishing in SharePoint you have a pages library with pages that have a specific page layout. Those page layouts have to inherit from Microsoft.SharePoint.Publishing.PublishingLayoutPage and not from Microsoft.Sharepoint.Pages.WebpartPages as what we had done. Besides that we also forgot to insert the pages in the pages library with a Page Layout.  Below you will see the mistake we made in the onet.xml file of our custom site definition.   &lt;Module Path=\"\" Url=\"$Resources:cmscore,List_Pages_UrlName;\" Name=\"DefaultBlank\"&gt;     &lt;File Url=\"default.aspx\" Level=\"Approved\" Type=\"GhostableInLibrary\"&gt;       &lt;Property Name=\"Title\" Value=\"Default\"&gt;&lt;/Property&gt;       &lt;Property Name=\"ContentType\" Value=\"$Resources:cmscore,contenttype_pagelayout_name;\"&gt;&lt;/Property&gt;     &lt;/File&gt;  &lt;/Module&gt;  You have to add a property to the page you are adding. That property is called ‘PublishingPageLayout’ and should have the page layout you want to use as the value. The page layout you want to use has to inherit from Microsoft.SharePoint.Publishing.PublishingLayoutPage, and must reside in the masterpages gallery of your site collection.  &lt;Module Path=\"\" Url=\"$Resources:cmscore,List_Pages_UrlName;\" Name=\"DefaultBlank\"&gt;     &lt;File Url=\"default.aspx\" Level=\"Approved\" Type=\"GhostableInLibrary\"&gt;       &lt;Property Name=\"Title\" Value=\"Default\"&gt;&lt;/Property&gt;       &lt;Property Name=\"PublishingPageLayout\" Value=\"~SiteCollection/_catalogs/masterpage/WelcomeLinks.aspx, ~SiteCollection/_catalogs/masterpage/Contact.aspx\"&gt;&lt;/Property&gt;       &lt;Property Name=\"ContentType\" Value=\"$Resources:cmscore,contenttype_pagelayout_name;\"&gt;&lt;/Property&gt;     &lt;/File&gt;  &lt;/Module&gt; So the most Important thing you should remember is that the Page Layout Inherits from Microsoft.SharePoint.Publishing.PublishingLayoutPage.   ","categories": ["Administration","Development","SharePoint"],
        "tags": ["Modified","Page","Publishing","SharePoint"],
        "url": "/2008/09/this-page-has-been-modified-since-you-opened-it-you-must-open-the-page-again/",
        "teaser": null
      },{
        "title": "Testing Expiration Policy",
        "excerpt":" When you add an Expiration Policy to a library in SharePoint you can only test your policy once a day. This is because SharePoint only runs the TimerJob for the expiration policy only once a day.   The TimerJob can’t be run with stsadm so the timer job has to be accessed trough the object model. For people who have problem running the expiration policy you can use my Console Application that I have created to make sure the Expiration Policy TimerJob runs when I want it too.   &nbsp;  Run Expiration Policy   ","categories": ["Administration","SharePoint"],
        "tags": ["Expiration","Testing"],
        "url": "/2008/09/testing-expiration-policy/",
        "teaser": null
      },{
        "title": "Setting up SharePoint search",
        "excerpt":" This is a walkthrough for the configuration of SharePoint Search.  Central Administration  Navigate to Central Administration  Shared Service Provider  Select you ‘Shared Service Provider’. They are listed at the left side of you screen in the middle.  Search Settings  In the main window of the Shared Service Provider select ‘Search Settings’    Content Sources and Crawl Schedules  In the ‘Search Settings’ screen the main settings off SharePoint search can be changed or configured. We will continue by selecting ‘Content Sources and Crawl Schedules’    In the screen that follows content sources are defined. Standard there is one content source, this is the ‘Local Office SharePoint Sites’ content source.    Select the content source or create a new content source for crawling your sites.    In this screen the settings of the content sources have to be filled in. Besides that you have to schedule a crawl before items appear when you start searching.  The information that can be filled in is described below:     Name The name off the content source.   Content Source Details The details off the content source are displayed here.   Start Addresses Here you have to fill in the urls off you web applications that you want to crawl. Addresses can be filled in with the http:// prefix or the sps3:// prefix. When you use the sps3:// prefix your users will be crawled.   Crawl Settings   A choice field you can choose the following options:    Crawl everything under de hostname for each start address.   Crawl only the SharePoint site of each start address.      Crawl Schedules The crawls can be scheduled here. You can schedule an incremental crawl or a Full crawl.   Start Full Crawl Option for starting a full crawl.    &nbsp;  Creating Search Scopes  On the ‘Search Settings’ screen select the option ‘View Scopes’.    In the screen what appears select new scope:      In the screen shown above you can fill in the information off the search scope:      Title and Description    The title and description you want to give to the search scope.      Target Results Page    The target page where the search results have to be shown. You have two possibilities:    Use The Default Search Results Page   Specify a different page for searching the scope (Custom Search Center)       When the scope is created you have to define a role for the scope. When you have clicked ok to save the scope you will return to the ‘View Scopes’ page were you new scope will be displayed. The scope only has an update status of ‘Empty – Add Rule’.    Select ‘Add Rule’ to add a new rule to the scope. The rules have several settings and are all described below:    All Content  This rule will crawl all content in the scope  Content source     Scope content type The type off the scope. Here it is Content Source   Content Source The content source were the rule applies for.   Behaviour  Decide what the rule should do with the content. There are three options:    Include   Require   Exclude         Property Query       Scope content type  The type off the scope. Here it is Property Query   Property Query Looking if the crawled content applies to a restriction. An example of a restriction is: Author = Klaas   Behaviour  Decide what the rule should do with the content. There are three options:    Include   Require   Exclude         Web Address     Scope content type  The type off the scope. Here it is Web Address   Web Address   Rule for adding external and internal content. There are three options:    Folder   Hostname   Domain or Sub domain     Behavior Decide what the rule should do with the content. There are three options:  Include   Require   Exclude        ","categories": ["Administration","SharePoint"],
        "tags": ["Search","Setting up","SharePoint 2007"],
        "url": "/2008/10/setting-up-sharepoint-search/",
        "teaser": null
      },{
        "title": "Free e-book offer SQL Server 2008 from Microsoft",
        "excerpt":" Today I received a Microsoft Newsletter to inform me that there is a new free eBook offer for SQL Server 2008.  I tough that more people can take advantage from this. So here is the link to the free e-book.  http://www.microsoft.com/learning/sql/2008/default.mspx#EBOOK  Enjoy your reading.   ","categories": ["Documentation"],
        "tags": ["Documentation","SQL","SQL Server 2008"],
        "url": "/2008/10/free-e-book-offer-sql-server-2008-from-microsoft/",
        "teaser": null
      },{
        "title": "Changing the user that runs the Application Pool",
        "excerpt":" For a project we were working on we created a web application with a wrong application pool Identity. We wanted to set this correctly and we thought we could just change the application pool Identity in IIS, but we were wrong.  What happens was that the application pool identity was changed back by SharePoint in a period off time. When we saw this happening we began to look how we could change the application pool account in SharePoint.  Navigate to ‘Central Administration’ and click on the operations tab.    Click on ‘Service Accounts’ that is highlighted in the image above. In the screen that follows change the boxes to the preferences you want. For a ‘Web Application’ Identity you have to select the radio button ‘Web Application pool’ and select the correct preferences.     ","categories": ["Administration","SharePoint"],
        "tags": ["Accounts","Application Pool","SharePoint"],
        "url": "/2008/12/changing-the-user-that-runs-the-application-pool/",
        "teaser": null
      },{
        "title": "Disabling the MySite link for all your users",
        "excerpt":" Did you ever have a problem with your my site? Well we did! The customer wanted a Citrix webpart on their my site. I did not recommend it but still they wanted it. The performance off the my site fell down the roof and we had to disable the my site so we could do a proper investigation what they didn’t do in the accept environment.  So how do you shut down the my site for all your users? What you can do is just stop the my site web application in IIS and all users will get a ‘Page Not Found’ error. This is not something a client will love so we did some research and found out that you can disable it by user or by user group.  If you navigate to ‘Central Administration’ and click on the ‘Shared Service Provider’ of you web application on the left side of your screen.  Under User Profiles and My sites you can click on ‘Personalization services permissions’.    In the screen that follows click on the user ‘NTAuthority\\Authenticated Users’. This group off users represents all authenticated users.    Deselect ‘Create personal site’ in the upcoming screen and save your changes. Now the my site link will not be available to your users.   ","categories": ["Administration","SharePoint"],
        "tags": ["Disable","MySite","Security","SharePoint"],
        "url": "/2008/12/disabling-the-mysite-link-for-all-your-users/",
        "teaser": null
      },{
        "title": "Scheduling SharePoint Backup",
        "excerpt":" Update:Version 2.0 available of the script here  For one of our clients we had to schedule the SharePoint Backup what isn’t possible trough the interface, so we went looking for some possible solutions. The solutions we came across are:    Performing the backup every time by hand (isn’t scheduling after all).   Creating a batch file to perform the backup. And schedule it in windows with scheduled tasks.   Creating a VBScript for the backup and scheduling it in windows with scheduled tasks.  We choose the third solution because we found a create article on the internet from Rabi Achrafi. In this article he shows his version of a VBScript he used. The great thing about the script was that it sends an e-mail to a specified e-mail address when the job is finished and gives back the completion status of the job. The script did not completely cover our needs so we changed the file a little bit. The main thanks still goes out to Rabi Achrafi. Besides the small things we changed we also added a method for deleting all the files that reside in the shared folder. Because those files where already taped. The file we created looked something like this:   '''''''''''''''''''''''''''''''''''''''''''''''''''''  ''SharePoint Backup Script  ''  ''This is a VBScript for scheduling the SharePoint backup.  '''''''''''''''''''''''''''''''''''''''''''''''''''''   'string E-Mail from  Const strFrom = \"toemail@smeikkie.nl\"   'string E-Mail To  Const strTo = \"fromemail@smeikkie.nl\"   'string Mail Server  Const strMailserver = \"mail.test.nl\"   'string backup directory for the sharepoint backup  Const strBackUpDir = \"\\\\MOSS2007DEV\\SharepointBackup\"   'string with the backupmethod  Const BackUpMethod = \"full\"   'Create windows shell object  Set objShell = CreateObject(\"WScript.Shell\")   'Clean the sharepoint backup folder. Old backups will be deleted!!  Call ClearFolder(strBackUpDir)   'Retrieve the 12hive from sharepoint from the registery  Dim strRegKey  strRegKey = objShell.RegRead (\"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Shared Tools\\Web Server Extensions\\12.0\\Location\")   'Add the bin folder to the 12hive folder  Dim strMOSSPath  strMOSSPath = strRegKey &amp; \"BIN\\\"   'Setting the BackUp Location  Dim strBackupLocation  strBackUpLocation = strBackUpDir   'Define the path off the sharepoint location. This will point to the file with specific information about the backup.  Dim SharPointBackupRestoreTable  SharPointBackupRestoreTable = strBackUpLocation &amp; \"\\spbrtoc.xml\"   'Ensure that we run in the sharepoint folder  objShell.CurrentDirectory = strMOSSPath   'Execute the stsadm backup command  objShell.Exec (\"stsadm -o backup -directory \" &amp; strBackupLocation &amp; \" -backupmethod \" &amp; BackUpMethod &amp; \" \")   ' This Do loop checks the status of the backup process every minute.  ' If the backup process hasn't completed within 60 minutes an email is sent to the  ' Sharepoint administrator notifying him/her about this, otherwise an email is sent  ' notifying the SharePoint Administrator of the outcome of the backup  Do  loopCounter = loopCounter + 1   If count &amp;amp;gt; 60 Then  'Send the e-mail that it took 60 minutes  Call SendEmail(\"SharePoint Backup Process\", \"SharePoint backup process has been running for over 60 minutes. Please check progress of backup. To make sure everything is going as it should be going.\")  End If  ' Wait for 1 minute  WScript.Sleep 60000   ' Check if the backup process (i.e. stsadm.exe) is currently running  strComputer = \".\"  Set objWMIService = GetObject(\"winmgmts:{impersonationLevel=impersonate}!\\\\\" &amp; strComputer &amp; \"\\root\\cimv2\")  Set sharepointProcess = objWMIService.ExecQuery (\"Select * from Win32_Process Where Name = 'stsadm.exe'\")   If (sharepointProcess.count) = 0 Then  'Backup process has ended therefore check the SharePoint Backup Restore Table to analyse the outcome of the backup  Set objXMLDoc = CreateObject(\"Microsoft.XMLDOM\")  objXMLDoc.async = False  objXMLDoc.load(SharPointBackupRestoreTable)  Set NodeList = objXMLDoc.documentElement.selectNodes(\"//SPErrorCount\")  For Each Node In NodeList  If (Node.text) &lt;&gt; \"0\" Then  ' Backup errors were generated  Call SendEmail(\"SharePoint Backup Process Failed\", \"The SharePoint Backup Process failed with errors. Please check the errors and run the backup process again.\")  Else  ' No backup errors were generated  Call SendEmail(\"SharePoint Backup Process Completed Successfully\", \"The SharePoint Backup Process completed without errors\")  End If  Next   Exit Do  End If   Loop   ''Method for cleaning out the backup folder. It will delete all off the old backups!!!  ''The String Off The Folder To Cleam  Sub ClearFolder(strfolder)   'Get file object  Set fso=CreateObject(\"Scripting.FileSystemObject\")  SharepointBackupDir = fso.GetFolder(strfolder)   'Delete all files  For Each file In fso.GetFolder(strfolder).Files  file.delete  Next   'Loop trough the subfolders and put them in an array  arrayFolders = Array()  For Each objectFolder In fso.GetFolder(strfolder).SubFolders  intCount = UBound(arrayFolders) + 1  ReDim Preserve arrayFolders(intCount)  arrayFolders(intCount) = objectFolder.Path  Next   'Loop trough the array and delete the subfolders  For n = 0 To UBound(arrayFolders)  fso.DeleteFolder arrayFolders(n), True  Next   End Sub   ''Mehtod for sending the e-mail  ''The subject off the email  ''The Body text off the e-mail  Sub SendEmail (subject, body)   'create message  Set objEmail = CreateObject(\"CDO.Message\")  objEmail.From = strFrom  objEmail.To = strTo  objEmail.Subject = subject  objEmail.Textbody = body   'Mail configuration  objEmail.Configuration.Fields.Item _  (\"http://schemas.microsoft.com/cdo/configuration/sendusing\") = 2  objEmail.Configuration.Fields.Item _  (\"http://schemas.microsoft.com/cdo/configuration/smtpserver\") = strMailserver  objEmail.Configuration.Fields.Item _  (\"http://schemas.microsoft.com/cdo/configuration/smtpserverport\") = 25  objEmail.Configuration.Fields.Update   'Send E-mail  objEmail.Send  'Write to the event log  Call WriteEvent(subject,body)  End Sub   ''Method for Logging the outcome off the backup to the application event log  ''The subject off the event  ''The Body text off the event  Sub WriteEvent(subject,body) ","categories": ["Administration"],
        "tags": ["Backup","SharePoint 2007","VBScript"],
        "url": "/2008/12/scheduling-sharepoint-backup/",
        "teaser": null
      },{
        "title": "SharePoint prompts to login multiple time when opening a document",
        "excerpt":"One of our most annoying issues we came across in one of our projects was the problem that SharePoint kept prompting us to login when we were opening a document. This problem ruined the user experience, so we began a search for possible solutions. In our environment the user could access the portal trough two options locally from their desktop or laptop or trough Citrix Application Neighbourhood. We found out that we had to take three actions to make this work:  Insert the SharePoint portal site into the trusted sites of Internet Explorer.   Set your security level of Internet Explorer to login you in Automatically for intranet applications.  How do you perform these actions I hear you thinking? The first you can perform in multiple ways because it is different for each version of Internet Explorer:  IE6: Internet Options -&gt; Security -&gt; Trusted Site -&gt; Then add the URL of the portal to the trusted sites.   IE7/IE8:Internet options -&gt; Security -&gt; Local intranet -&gt; Websites -&gt; Advanced -&gt; Then add the URL of the portal to the trusted sites.  The next action you can perform by opening Internet Options -&gt; Security -&gt; Local intranet -&gt; Custom level -&gt; Scroll all the way down and select Automatic logon with current username and password. Now there is still the third step we have to make. But what is it? The third step is changing the Group Policies off all the users. This will make sure every user off the company will receive these changes.   ","categories": ["Administration","SharePoint"],
        "tags": ["Citrix","Login","SharePoint"],
        "url": "/2008/12/sharepoint-prompts-to-login-multiple-time-when-opening-a-document/",
        "teaser": null
      },{
        "title": "Integration Reporting Services with SharePoint 2007 in Native Mode",
        "excerpt":" It is possible to integrate SQL Reporting Services with SharePoint 2007, but know the question is how you integrate it with SharePoint. The easiest why is to install the SharePoint Reporting Web Parts on your server. This is also called the Integration in Native Mode.  It isn’t more than installing the webpart and connect those webpart to an existing report that resist on you reporting server. But how do you install these webparts.  In the installation folder off SQL server there is a cab file with the two webparts. You can find this file (RSWebParts.cab) in the following folder:  C:\\Program Files\\Microsoft SQL Server\\80\\Tools\\Reporting Services\\SharePoint\\RSWebParts.cab  After you found this file you can install it on your SharePoint Farm. To do this you have to navigate to the Application server of your SharePoint farm. Open a Command Prompt window and navigate to the folder that has the Stsadm.exe tool for SharePoint. Normally this path is C:\\Program Files\\Common Files\\Microsoft Shared\\web server extensions\\12\\BIN.  Run the Stsadm.exe tool and run it with the following syntax:  stsadm -o addwppack -filename “(Folder of the cab file)\\RSWebParts.cab” -globalinstall  Adding web part and configuring the webpart   Open a Web Part page in your SharePoint Web Application   Set your page in edit mode: Site Action -&gt; Edit Page, and click ‘Add a Web Part’ in the zone where you would like to add the Web Part.   In the Add Web Parts dialog box, scroll down to Miscellaneous. Select Report Explorer or Report Viewer if you want to add both Web Parts, and then click Add.   Know modify the settings of the Report Web Part. In Report Manager URL, type your URL to a Report Manager instance. By default, a Report Manager URL has the following syntax: http:///reports. For this setting you can also set the Start Path.  The Web Parts also allows you to connect the Report Explore to the Report Viewer. This connection can be set in the edit mode of the page:   Click on the edit button of the title bar of the Report Explorer Web Part   Point to Connections and follow that up by pointing to ‘Show report in’, and then click ‘Report Viewer’   ","categories": ["Administration"],
        "tags": ["Native Mode","Reporting Services","SharePoint 2007"],
        "url": "/2008/12/integration-reporting-services-with-sharepoint-2007-in-native-mode/",
        "teaser": null
      },{
        "title": "Virtual Earth WebPart",
        "excerpt":" After reading a post on the blog of Wesley Bakker I thought by myself maybe it is fun to create a webpart in which we can load a map from Virtual Earth.  And so I did I began with creating a webpart with certain properties so that you can also add a location and a pushpin for that location.  First you have to think off a way to get the virtual earth map in the webpart. I have done this by writing the JavaScript to the page in the render method off the webpart.  [Guid(\"d2a82f7b-d32b-4154-8370-2a2bae4f7af7\")]   public class VirtualEarthWebPart : System.Web.UI.WebControls.WebParts.WebPart   {          public VirtualEarthWebPart()          {          }           protected override void Render(HtmlTextWriter writer)          {              string javascript = null;               if (String.IsNullOrEmpty(strLocation))              {                  javascript = \"This query has returned no items. To configure the query for this Web Part, &lt;a href=\\\"javascript:MSOTlPn_ShowToolPane2Wrapper('Edit', this, '\" + this.ID + \"')\\\"&gt;open tool pane&lt;/a&gt;.\";              }              else              {                  //Create Java Script String                  javascript = CreateJavaScriptString();              }               writer.Write(javascript);          }           public string CreateJavaScriptString()          {              StringBuilder htmltext = new StringBuilder();               htmltext.Append(\"&lt;script src=\\\"http://dev.virtualearth.net/mapcontrol/mapcontrol.ashx?v=6.2\\\"&gt;&lt;/script&gt;\");              htmltext.Append(\"&lt;script type=\\\"text/javascript\\\"&gt;\");               htmltext.Append(\" var map = null; \");              htmltext.Append(\" var points = null; \");              //Load Virtual Earth Map              htmltext.Append(\"  function GetMap() \");              htmltext.Append(\"  { \");              htmltext.Append(\"    map = new VEMap('myMap'); \");              htmltext.Append(\"    map.LoadMap(); \");              htmltext.Append(\"    map.Find(null, '\" + strLocation + \"', null, null, null, null, false, false, true, true, findAddressCallBack); \");              htmltext.Append(\"  }\");               htmltext.Append(\" function findAddressCallBack(thelayer, resultsArray, places, hasMore, veErrorMessage) \");              htmltext.Append(\" { \");              htmltext.Append(\"   if(places != null &amp;&amp; places.length &gt;0) \");              htmltext.Append(\"   { \");              htmltext.Append(\"     var latitude = places[0].LatLong.Latitude; \");              htmltext.Append(\"     var longitude = places[0].LatLong.Longitude; \");              htmltext.Append(\"     points = new VELatLong(latitude, longitude); \");              htmltext.Append(\"     try \");              htmltext.Append(\"     { \");              htmltext.Append(\"       var shape = new VEShape(VEShapeType.Pushpin, points); \");               //Set Custom Image URL              if (!String.IsNullOrEmpty(customIconUrl))              {                  htmltext.Append(\" shape.SetCustomIcon('\" + customIconUrl + \"'); \");              }               //Set Location Title              if (!String.IsNullOrEmpty(titleLocation))              {                  htmltext.Append(\" shape.SetTitle('\" + titleLocation + \"'); \");              }               if (!String.IsNullOrEmpty(descriptionLocation))              {                  htmltext.Append(\" shape.SetDescription('\" + descriptionLocation + \"'); \");              }               if (!String.IsNullOrEmpty(photoLocation))              {                  htmltext.Append(\" shape.SetPhotoURL('\" + photoLocation + \"');    \");              }               if (!String.IsNullOrEmpty(moreLocation))              {                  htmltext.Append(\" shape.SetMoreInfoURL('\" + moreLocation + \"'); \");              }               if (zoomLevel != 0 &amp;&amp; zoomLevel &lt;= 19)              {                  htmltext.Append(\"  map.SetZoomLevel(\" + zoomLevel + \"); \");              }               if (modeEnum == MapModeEnum.TwoDimensional)              {                  htmltext.Append(\"       map.SetMapMode(VEMapMode.Mode2D); \");              }              else              {                  htmltext.Append(\"       map.SetMapMode(VEMapMode.Mode3D); \");              }              htmltext.Append(\"       map.AddShape(shape); \");              htmltext.Append(\"      map.SetMapStyle(VEMapStyle.\" + mapEnum.ToString() + \"); \");              htmltext.Append(\"     } \");              htmltext.Append(\"     catch(e) \");              htmltext.Append(\"     { \");              htmltext.Append(\"       alert(e.message); \");              htmltext.Append(\"     } \");              htmltext.Append(\"   } \");              htmltext.Append(\" } \");               //Add Function GetMap to the LoadEvent              htmltext.Append(\"  function addLoadEvent(func) \");              htmltext.Append(\"  { \");              htmltext.Append(\"    var oldonload = window.onload; \");              htmltext.Append(\"    if(typeof window.onload != 'function') \");              htmltext.Append(\"    { \");              htmltext.Append(\"      window.onload = func; \");              htmltext.Append(\"    } \");              htmltext.Append(\"    else \");              htmltext.Append(\"    { \");              htmltext.Append(\"      window.onload = function()\");              htmltext.Append(\"        { \");              htmltext.Append(\"          func(); \");              htmltext.Append(\"        } \");              htmltext.Append(\"    } \");              htmltext.Append(\"  } \");               htmltext.Append(\"  addLoadEvent(GetMap); \");               htmltext.Append(\"&lt;/script&gt; \");              htmltext.Append(\"&lt;div id='myMap' style=\\\"position:relative;\\\"&gt;&lt;/div&gt;\");               return htmltext.ToString();          }       }  In the code displayed above I created a StringBuilder with the complete JavaScript and even a html section (div) for displaying the map. In the code above you can also see multiple checks for empty strings. This I have done for certain properties I have added to the webpart.  The properties I have added to the webpart are written down at the end of this post. In the code section below I have added two examples off properties I created for the webpart.   public enum MapStyleEnum  {        Road, Shaded, Aerial, Hybrid, Oblique, Birdseye, BirdseyeHybrid  };   protected MapStyleEnum mapEnum;   /// &lt;summary&gt;  /// property for the Map Style  /// &lt;/summary&gt;  [WebBrowsable(true),  Personalizable(true),  Category(\"Virtual Earth Map Settings\"),  DisplayName(\"Map Style\"),  WebDisplayName(\"Map Style\"),  Description(\"Please choose a Map Style.\")]  public MapStyleEnum MyEnum  {       get       {          return mapEnum;       }       set       {           mapEnum = value;       }  }   //Custom icon  private string customIconUrl;   /// &lt;summary&gt;  /// property for the location title  /// &lt;/summary&gt;  [WebBrowsable(true),  Personalizable(true),  Category(\"Virtual Earth Location Settings\"),  DisplayName(\"Custom Icon Url\"),  WebDisplayName(\"Custom Icon Url\"),  Description(\"Please enter the url off the custom icon.\")]  public string CustomIconUrl  {    get    {        return customIconUrl;    }    set    {      customIconUrl = value;     }   } ","categories": ["Development"],
        "tags": ["Javascript","Virtual Earth"],
        "url": "/2009/01/virtual-earth-webpart/",
        "teaser": null
      },{
        "title": "Disposing of SharePoint Objects (SPDisposeCheck tool Finally Available!)",
        "excerpt":" Update: A new version of the tool is available as you can read here  On the internet you can find several excellent articles on the various design patterns for disposing of SharePoint or WSS objects.  Some of these objects can cause serious memory leaks if you don’t use them correctly. Normally you have to take care of three different objects: SPSite, SPWeb and Publishing.  There are a couple of White Papers available on MSDN that you can read about this subject:  Best Practices: Using Disposable Windows SharePoint Services Objects   Best Practices: Common Coding Issues When Using the SharePoint Object Model.  Yesterday a tool was released for checking if you dispose of the objects in a correct way. The tool is called SPDisposeCheck. SPDisposeCheck is a tool to help you check your assemblies that use the SharePoint API so that you can build better code. The tool may not show all memory leaks and further investigation is advised if you continue to experience issues.    You can download SPDisposeCheck here.  I checked the tool out by applying it to one of our project and we found 21 problems. We fixed those problems and we certainly saw an improvement. So try this tool out and let it help you with building better code.   ","categories": ["Development","SharePoint","Tools"],
        "tags": ["Disposing","SPDisposeCheck","Tool"],
        "url": "/2009/01/disposing-of-sharepoint-objects-spdisposecheck-tool-finally-available/",
        "teaser": null
      },{
        "title": "Content Editor WebPart ControlAdapter",
        "excerpt":" One of our clients had a internet facing website. With this website we had a number of problems because it was deployed to our Internet facing farm with Content Deployment. The problem we had was that the Content Editor Web Part (for the rest of this article I will name it CEWP) does not allow relative URLs but saves the complete URL.   So the Internet site had many URLs in the content editor part the referenced to the internal address that you could not access from the internet. Till a few weeks ago we did not have a solution to this problem till i read a blog post (here). They created a Control Adapter that is run when the content editor web part is on a certain page. I have chosen to write out my complete solution because I found that the other post could have been written with more details on how to create it and how to implement it.   The Control Adapter replaces the URLs of the internal site in the CEWP with \"/\"&nbsp; so that it displays a relative URL so that you can use the link on the internet. So what do you need for this solution:   A feature for adding a link to Central Administration and also places a browser file in the App_Browser Directory for a certain web application.   A application page for saving the URL that has to be saved in the web application properties.   A browser file to set the Control Adapter.   First lets make the browser file to set the Control Adapter to the CEWP:   &lt;browsers&gt;     &lt;browser refID=\"Default\"&gt;         &lt;controlAdapters&gt;             &lt;adapter controlType=\"Microsoft.SharePoint.WebPartPages.ContentEditorWebPart\" adapterType=\"CEWPFIX.SharePoint.CEWPFIXAdapter, CEWPFIX.SharePoint, Version=1.0.0.0, Culture=neutral, PublicKeyToken=1e8dcda25bf26f8e\" /&gt;         &lt;/controlAdapters&gt;     &lt;/browser&gt; &lt;/browsers&gt; What you do in the browser file is that you assign a Adapter to the Microsoft.SharePoint.WebPartPages.ContentEditorWebPart and that is the CEWPFIXAdapter in the example above.   The next step is to make the Control Adapter. So we created a class file and inherit from the System.Web.UI.Adapters.   using System.Web; using System.Web.UI; using System.Web.UI.WebControls; using System.Web.UI.Adapters; using System.IO; using System.Text.RegularExpressions; using Microsoft.SharePoint.Administration; using System.Web.Caching; using Microsoft.SharePoint; namespace CEWPFIX.SharePoint {     /// &lt;summary&gt;      /// ControlAdapter for the Content Editor WebPart      /// &lt;/summary&gt;   ControlAdapter     {         /// &lt;summary&gt;          /// Constant string for the creation of the cache object          /// &lt;/summary&gt;          private const string AlternateUrls = \"AlternateUrls\";         /// &lt;summary&gt;          /// Key for the webapplication property          /// &lt;/summary&gt;          private const string FilterUrl = \"FilterUrls\";         /// &lt;summary&gt;          /// Override the render function off the CEWP          /// &lt;/summary&gt;          /// &lt;param name=\"writer\"&gt;HtmlTextWriter write&lt;/param&gt;          protected override void Render(HtmlTextWriter writer)         {             //Create a new stringbuilder              StringBuilder sb = new StringBuilder();             //create a HtmlTextWriter              HtmlTextWriter htw = new HtmlTextWriter(new StringWriter(sb));             //Preform the base Render Method              base.Render(htw);             //Get the output string              string output = sb.ToString();             //Create a list with the alternate access urls              List&lt;Uri&gt; alternateUrls = GetAlternateUrls();             //Read out the list with alternate access urls              foreach (Uri alternateUrl in alternateUrls)             {                 //Replace the url's with /                  output = output.Replace(alternateUrl.ToString(), \"/\");             }             //write the output to the page              writer.Write(output);         }         /// &lt;summary&gt;          /// Method for retrieving the Alternate Access Urls of retrieving them from the Cache          /// &lt;/summary&gt;          /// &lt;returns&gt;&lt;/returns&gt;          private List&lt;Uri&gt; GetAlternateUrls()         {             //Try to retrieve the list from the cache              List&lt;Uri&gt; alternateUrls = (List&lt;Uri&gt;)HttpContext.Current.Cache[AlternateUrls];             //Make sure anonymous users can access the web application properties              SPSecurity.RunWithElevatedPrivileges(delegate()             {                 //If the list is null recreate the list en put it in the chache                  if (alternateUrls == null)                 {                     //Create a new list object                      alternateUrls = new List&lt;Uri&gt;();                     //Retrieve the WebApplication object                      SPWebApplication webApp = SPWebApplication.Lookup(HttpContext.Current.Request.Url);                     if (webApp.Properties.ContainsKey(FilterUrl))                     {                         string[] urls = webApp.Properties[FilterUrl].ToString().Split(new char[] { ';' });                         foreach (string str in urls)                         {                             try                             {                                 Uri url = new Uri(str);                                 alternateUrls.Add(url);                             }                             catch (Exception ex)                             {                                 new ExceptionHandler(\"Something went wrong while creating a uri in the CEWPFIXAdapter\", ex);                             }                         }                     }                     //Add the list to the cache                      HttpContext.Current.Cache.Add(AlternateUrls, alternateUrls, null, DateTime.Now.AddHours(12), Cache.NoSlidingExpiration, CacheItemPriority.Normal, null);                 }             });             //return the list with the urls              return alternateUrls;         }     } } This code will make sure the URLs will get the right format.   ","categories": ["Development","SharePoint"],
        "tags": ["Content Editor","ControlAdapter"],
        "url": "/2009/02/content-editor-webpart-controladapter/",
        "teaser": null
      },{
        "title": "Scheduling SharePoint Backup : Part 2",
        "excerpt":"A while ago I posted a SharePoint Backup script so that it is possible to schedule a SharePoint Backup. I had to make some adjustments to the script because it contains small errors. The adjustments I had to make were:  When you perform multiple differential backups in a week you will receive multiple e-mails for one backup.   I have created more properties so that you can alter the script from the top.   You do not have to delete a method when you choose the differential backup method   I cleaned the script up a little bit.   ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''  ''SharePoint Backup Script  ''  ''This is a VBScript for scheduling the SharePoint backup.  ''The script has the ability to be scheduled as an differential  ''backup or a full backup. This script will also send an email  ''when the job is finished.  ''  ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''   ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''  ''declaration off constants that are needed to perform the backup  ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''  'email address from  Const strFrom = \"\"   'email address to  Const strTo = \"\"   'email subject timeout  Const strSubjectTimeOut = \"Sharepoint Backup Process - Time Out\"   'message for the time out  Const strTimeOutMessage = \"The SharePoint backup process has been running for over 60 minutes. Please check the progress of the backup. To make sure everything is going as it should be going.\"   'email subject failed  Const strSubjectFailes = \"Sharepoint Backup Process - Failed\"   'email message failed  Const strFailedMessage = \"The SharePoint Backup Process failed with errors. Please check the errors and run the backup process again.\"   'email subject success  Const strSubjectSucceeded = \"Sharepoint Backup Process - Succeeded\"   'email message success  Const strSucceededMessage = \"The SharePoint Backup Process completed without errors.\"   'backup enviroment  Const strEnviroment = \"Intranet\"   'url off the mail server  Const strMailserver = \"\"   'backup location, this has to be a shared drive  Const strBackUpDir = \"\"   'backupmethod full or differential  Const strBackUpMethod = \"differential\"   ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''  ''Beginning the backup process  ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''   'object for checking succeeded update  bool = \"false\"   'create windows shell object  Set objShell = CreateObject(\"WScript.Shell\")   if strBackUpMethod = \"full\" Then      Call ClearFolder(strBackUpDir)  End If   'retrieve the 12hive from sharepoint from the registery  strMossPath = objShell.RegRead (\"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Shared Tools\\Web Server Extensions\\12.0\\Location\") &amp; \"BIN\\\"   'Define the path off the sharepoint location. This will point to the file with specific information about the backup.  SharPointBackupRestoreTable = strBackUpDir &amp; \"\\spbrtoc.xml\"   'Ensure that we run in the sharepoint folder  objShell.CurrentDirectory = strMossPath   'Execute the stsadm backup command  objShell.Exec (\"stsadm -o backup -directory \" &amp; strBackUpDir &amp; \" -backupmethod \" &amp; strBackUpMethod &amp; \" \")   ' This Do loop checks the status of the backup process every minute.  ' If the backup process hasn't completed within 60 minutes an email is sent to the  ' Sharepoint administrator notifying him/her about this, otherwise an email is sent  ' notifying the SharePoint Administrator of the outcome of the backup  Do   loopCounter = loopCounter + 1   If count &gt; 60 Then  'Send the e-mail that it took 60 minutes  Call SendEmail(strSubject, strTimeOutMessage, strEnviroment)  End If  ' Wait for 1 minute  WScript.Sleep 60000   ' Check if the backup process (i.e. stsadm.exe) is currently running  strComputer = \".\"  Set objWMIService = GetObject(\"winmgmts:{impersonationLevel=impersonate}!\\\\\" &amp; strComputer &amp; \"\\root\\cimv2\")  Set sharepointProcess = objWMIService.ExecQuery (\"Select * from Win32_Process Where Name = 'stsadm.exe'\")   If (sharepointProcess.count) = 0 Then  'Backup process has ended therefore check the SharePoint Backup Restore Table to analyse the outcome of the backup  Set objXMLDoc = CreateObject(\"Microsoft.XMLDOM\")  objXMLDoc.async = False  objXMLDoc.load(SharPointBackupRestoreTable)  Set NodeList = objXMLDoc.documentElement.selectNodes(\"//SPErrorCount\")  'check each node  For Each Node In NodeList  If (Node.text) &lt;&gt; \"0\" Then  bool = \"false\"  Else  bool = \"true\"  End If  Next   'send message  If bool=\"false\" Then  ' Backup errors were generated  Call SendEmail(strSubjectFailed, strFailedMessage, strEnviroment)  Else  ' No backup errors were generated  Call SendEmail(strSubjectSucceeded, strSucceededMessage, strEnviroment)  End If  Exit Do  End If   Loop   ''Method for cleaning out the backup folder. It will delete all the old backups  ''&lt;strfolder&gt;The String Off The Folder To Cleam&lt;/strfolder&gt;  Sub ClearFolder(strfolder)   'Get file object  Set fso=CreateObject(\"Scripting.FileSystemObject\")  CleanPath = fso.GetFolder(strfolder)   'Delete all files  For Each file In fso.GetFolder(CleanPath).Files  file.delete  Next   'Loop trough the subfolders and put them in an array  arrFolders = Array()  For Each oFolder In fso.GetFolder(strfolder).SubFolders  intCount = UBound(arrFolders) + 1  ReDim Preserve arrFolders(intCount)  arrFolders(intCount) = oFolder.Path  Next   'Loop trough the array and delete the subfolders  For n = 0 To UBound(arrFolders)  fso.DeleteFolder arrFolders(n), True  Next   End Sub   ''Mehtod for sending the e-mail  ''&lt;subject&gt;The subject off the email&lt;/subject&gt;  ''&lt;body&gt;The Body text off the e-mail&lt;/body&gt;  Sub SendEmail (subject, body, enviroment)   'create message  Set objEmail = CreateObject(\"CDO.Message\")  objEmail.From = strFrom  objEmail.To = strTo  objEmail.Subject = subject &amp; \" (\" &amp; strBackUpMethod &amp; \") \" &amp; \"- \" &amp; enviroment  objEmail.Textbody = body   'Mail configuration  objEmail.Configuration.Fields.Item _  (\"http://schemas.microsoft.com/cdo/configuration/sendusing\") = 2  objEmail.Configuration.Fields.Item _  (\"http://schemas.microsoft.com/cdo/configuration/smtpserver\") = strMailserver  objEmail.Configuration.Fields.Item _  (\"http://schemas.microsoft.com/cdo/configuration/smtpserverport\") = 25  objEmail.Configuration.Fields.Update   'Send E-mail  objEmail.Send  'Write to the event log  Call WriteEvent(subject,body)  End Sub  ","categories": ["Administration"],
        "tags": ["Backup","Schedule","SharePoint 2007","VBScript"],
        "url": "/2009/02/scheduling-sharepoint-backup-part-2/",
        "teaser": null
      },{
        "title": "Using SPSecurity.RunWithElevatedPrivileges()",
        "excerpt":" The last time I have been receiving questions about using SPSecurity.RunWithElevatedPrivileges in a SharePoint webpart. This made me think and I thought by myself lets write a short post with some examples about this subject.  When you want to build a webpart that will be used by anonymous or read only users you often have to use RunWithElevatedPriviledges because a lot of objects in a SharePoint site require more rights. If you use RunWithElevatedPriviledges you will run a particular action under a account that has more rights in SharePoint.  I hear you asking under what account do we run this particular code then? The answer is the application pool account of your web application.  Now we know that we can write some examples to see what will work. Let’s begin with an example how you should not use RunWithElevatedPriviledges.   SPWeb web = SPContext.Current.Web;   SPSecurity.RunWithElevatedPrivileges(delegate()  {     SPList list = web.Lists[\"Test\"];  }); In this example we get the current web object from the SPContext. This means we have got a web object that runs under the account that we are logged in with. After we have got the web object we use RunWithElevatedPriviledges because read only users can not access the list.  If we would run this code in a webpart we will see an access denied message, all because we use a web object that is retrieved with our current context.  Let’s take a look at an example how you could do it the right way (this is the way I mostly use RunWithElevatedPriviledges):  Guid webID = SPContext.Current.Web.ID;  Guid siteID = SPContext.Current.Site.ID;   SPSecurity.RunWithElevatedPrivileges(delegate()  {     using (SPSite site = new SPSite(siteID))     {        using(SPWeb web = site.AllWebs[webID])        {           SPList list = web.Lists[\"Test\"];        }     }  }); In this example you can see that we first retrieve the ids of the site collection and the web so that we know which site object and web object we want to work with.  We then use RunWithElevatedPriviledges and retrieve the SPSite and SPWeb within the RunWithElevatedPriviledges. These objects will then run under the account of the application pool.  I hope this post will you guys out there and please fill free to leave comments.  ","categories": ["Development","SharePoint"],
        "tags": ["Elevated","Priviledges","SPSecurity"],
        "url": "/2009/02/using-spsecurity-runwithelevatedprivileges/",
        "teaser": null
      },{
        "title": "ASP:Menu in IE8",
        "excerpt":" Since a few days I have installed Internet Explorer 8 on my computer and I was curious how it would display certain sites. Till a few hours ago everything looked nice.   When navigating to an internet site we had just developed for one of our clients I noticed a strange thing. On the default page we added a custom control that inherits from the ASP Menu. The hover of that menu wasn’t working anymore in IE8.  After searching on Google I found an solution for this problem ( here ) and I also found another possible solution (here tanks to Mark Hildreth’s). The problem is caused by IE8 because it does not handle the z-index off the page right. You can fix this problem by doing the following:  Create a new class in your CSS file or place it in your web page:   .aspmenufix  {      z-index: 1;  } In the ASP menu control you have to add the following:  &lt;asp:Menu ID=\"Menu1\" runat=\"server\"&gt;     &lt;DynamicMenuStyle CssClass=\"aspmenufix\" /&gt; &lt;/asp:Menu&gt; I hope this will help everyone who uses the ASP Menu in his website!   ","categories": ["Development"],
        "tags": ["ASP.net","IE8","Menu"],
        "url": "/2009/02/aspmenu-in-ie8/",
        "teaser": null
      },{
        "title": "Problem with importing User profiles (0x800706D9)",
        "excerpt":" For one of our clients we have got an internet facing SharePoint site. For this website SharePoint user profiles need to be available. When we configured the user profile import we came across a very strange error:  There are no more endpoints available from the endpoint mapper. (Exception from HRESULT: 0×800706D9)  After some searching we figured out that there was something wrong with the firewall settings between de AD server and the SharePoint Farm. If you have the same problem take a look at the following kb articles from Microsoft:   How to configure a firewall for domains and trusts:  http://support.microsoft.com/kb/179442/   How to configure RPC dynamic port allocation to work with firewalls  http://support.microsoft.com/kb/154596/    ","categories": ["SharePoint"],
        "tags": ["User Profiles"],
        "url": "/2009/02/problem-with-importing-user-profiles-0x800706d9/",
        "teaser": null
      },{
        "title": "February Cumulative Update",
        "excerpt":" The new cumulative update for Microsoft Office SharePoint Server 2007 and Windows SharePoint Services is available for you to apply to you server farm.  The Microsoft SharePoint Team blog states:  “However, unless customers are affected by the problem described in the KB articles, and already have Dec CU applied, we do not suggest them to apply these new update packages.   The “Uber” package for Feb CU is scheduled to be released in the coming weeks. The update packages released now are a subset of the whole Uber package. These update packages do not include DLC (Document Lifecycle), IFS(InfoPath Forms Services) and XLSRV(Excel Calculation Services) updates. So, if customers apply them directly on a SharePoint installation which does not have Dec CU applied, they will miss the updates of those mentioned components. “  You can download the cumulative updates from here:   Windows SharePoint Services 3.0 – 961750 Global http://support.microsoft.com/kb/961750   Windows SharePoint Services 3.0 – 967703 Language specific http://support.microsoft.com/kb/967703   Microsoft Office SharePoint Server 2007 &amp; Microsoft Search Server 2008 – 961749 Global http://support.microsoft.com/kb/961749   Microsoft Office SharePoint Server 2007 &amp; Microsoft Search Server 2008 – 91754 Language specific http://support.microsoft.com/kb/961754  If you would like to install the updates on your farm I would recommend the following sequence:   961750   967703   961749   961754  Just like the Microsoft SharePoint Team blog stats it is better to wait for the uber package especially when you do not have the December Cumulative update installed.   ","categories": ["Administration"],
        "tags": ["Cumulative update","SharePoint 2007","Update"],
        "url": "/2009/03/february-cumulative-update/",
        "teaser": null
      },{
        "title": "Removing the Contextual Search items from the Search Drop Down Box",
        "excerpt":" For one of our clients we were looking for a solution to remove or replace the search result page that is used by the contextual searches from the Search DropDownBox. This because the contextual search let to a page “/_layouts/osssearchresults.aspx” or “/_layouts/searchresults.aspx” depending on whether you use WSS or MOSS.   Because this page has another layout then the search center they thought people would get confused. So they wanted to remove the contextual search or let them go to the custom search center.  When I was searching over the internet for a solution I found this post from Mark Arend. What he does in his post is change the out of the box page and let it redirect to his custom Search Center. This looks like a great solution but I won’t recommend you to edit out of the box pages from SharePoint so we went looking for another solution.  We thought that there would be a certain property for disabling the searches. But there isn’t, you have to create a custom feature were in you define a new search area where in you want to disable the contextual search.  You can define it like this:  feature.xml   &lt;?xml version=\"1.0\" encoding=\"utf-8\" ?&gt;  &lt;Feature  Id=\"89FC9564-0E82-490f-AD47-8678A619AB75A\"            Title=\"CustomSearchBox\"            Description=\"$Resources:EnhancedSearch_Feature_Description;\"            DefaultResourceFile=\"spscore\"            Version=\"12.0.0.0\"            Scope=\"WebApplication\"            xmlns=\"http://schemas.microsoft.com/sharepoint/\"&gt;      &lt;ElementManifests&gt;          &lt;ElementManifest Location=\"searcharea.xml\"/&gt;      &lt;/ElementManifests&gt;  &lt;/Feature&gt;  This is the code you need for the feature. In de searcharea.xml the xml defines the search area:  SearchArea.xml   &lt;?xml version=\"1.0\" encoding=\"utf-8\" ?&gt;  &lt;Elements xmlns=\"http://schemas.microsoft.com/sharepoint/\"&gt;      &lt;Control          Id=\"SmallSearchInputBox\"          Sequence=\"5\"          ControlClass=\"Microsoft.SharePoint.Portal.WebControls.SearchBoxEx\" ControlAssembly=\"Microsoft.SharePoint.Portal, Version=12.0.0.0, Culture=neutral, PublicKeyToken=71e9bce111e9429c\"&gt;          &lt;Property Name=\"GoImageUrl\"&gt;/_layouts/images/gosearch.gif&lt;/Property&gt;          &lt;Property Name=\"GoImageUrlRTL\"&gt;/_layouts/images/goRTL.gif&lt;/Property&gt;          &lt;Property Name=\"GoImageActiveUrl\"&gt;/_layouts/images/gosearch.gif&lt;/Property&gt;          &lt;Property Name=\"GoImageActiveUrlRTL\"&gt;/_layouts/images/goRTL.gif&lt;/Property&gt;                            &lt;Property Name=\"UseSiteDefaults\"&gt;true&lt;/Property&gt;          &lt;Property Name=\"DropDownMode\"&gt;ShowDD_NoContextual&lt;/Property&gt;          &lt;Property Name=\"ScopeDisplayGroupName\"&gt;Facilicom&lt;/Property&gt;          &lt;Property Name=\"FrameType\"&gt;None&lt;/Property&gt;                           &lt;Property Name=\"ShowAdvancedSearch\"&gt;false&lt;/Property&gt;      &lt;/Control&gt;  &lt;/Elements&gt;  In the above example you can see that there are many properties available for the search area. The most properties found this page.  For not showing the Contextual search you have to set the DropDownMode to: ShowDD_NoContextual.   Tanks to Mortens   ","categories": ["Development","SharePoint"],
        "tags": ["Contextual","Search Drop Down"],
        "url": "/2009/03/removing-the-contextual-search-items-from-the-search-drop-down-box/",
        "teaser": null
      },{
        "title": "ASP.Net Server Controls",
        "excerpt":" In January of this year I wrote a post about creating a virtual earth web part (You can read about it here). I had done this by adding JavaScript to the page.  When I was talking to one of my colleges (blog) a week ago after a team meeting he told me that there is also an ASP.Net Server Control made available by Microsoft. I was curious and I went looking on the internet and found the site where you can download the control and some other great server controls (here).  The controls that Microsoft made available is a package called “Windows Live Tools for Microsoft Visual Studio”. This package contains the following controls:   Contacts: Provides Windows Live Contacts functionality on a Web page.   IDLoginStatus: Provides Windows Live ID authentication on a Web page.   IDLoginView: Provides Windows Live ID authentication, including support for associating a Windows Live ID with an ASP.NET membership profile, on a Web page.   Map: Provides Virtual Earth Map functionality on a Web page.   MessengerChat: Provides Windows Live Messenger access on a Web page.   SilverlightStreamingMediaPlayer: Provides Silverlight™ Streaming media content on a Web page.  So take a look at the tools site (http://dev.live.com/tools/) from Microsoft and use the controls in your web application when they come in handy.   ","categories": ["Development"],
        "tags": ["ASP.net","Server Controls"],
        "url": "/2009/03/asp-net-server-controls/",
        "teaser": null
      },{
        "title": "February Cumulative Update Uber Package",
        "excerpt":" In my post a week ago I announced that the February Cumulative update was available for download but it could only be used by users who had the December Cumulative update applied. Microsoft said that they would create an uber package and it is available know:  February Cumulative Update Uber Package for Windows SharePoint Services 3.0 (Version: 12.0.6341.5000)  http://support.microsoft.com/hotfix/KBHotfix.aspx?kbnum=961755  February Cumulative Update Uber Package for Microsoft Office SharePoint Server 2007 (Version: 12.0.6341.5002)  http://support.microsoft.com/hotfix/KBHotfix.aspx?kbnum=961756  Can’t see the version you want on download page? Check out download instruction from James Blackwell:  http://blogs.msdn.com/jb/archive/2009/03/09/downloading-hotfixes-for-multiple-platforms.aspx   Note: You have to have Service Pack 1 Installed before you install this version.   ","categories": ["Administration"],
        "tags": ["Cumulative update","Release","SharePoint 2007"],
        "url": "/2009/03/february-cumulative-update-uber-package/",
        "teaser": null
      },{
        "title": "Microsoft SharePoint Server 2010",
        "excerpt":" For some time now I have been following information about SharePoint 14 that finds its way to blogs or news sites. In that time I have found a large amount of information that I would like to share with you.  A few weeks ago Microsoft announced that the new version of SharePoint will be known as SharePoint 2010.  Microsoft also announced that SharePoint also known as “Microsoft Office SharePoint Server” will not be called “MOSS” anymore because they cut off the “Office” part so it will be “Microsoft SharePoint Server 2010”.  Requirements  Preliminary system requirements for SharePoint Server 2010 have been made available by Microsoft. These requirements are:  · SharePoint Server 2010 will be 64-bit only.   · SharePoint Server 2010 will require 64-bit Windows Server 2008 or 64-bit Windows Server 2008 R2.   · SharePoint Server 2010 will require 64-bit SQL Server 2008 or 64-bit SQL Server 2005  When you install Service Pack 2 of SharePoint 2007 you can check whether your Farm is ready to upgrade to the next version when it comes available. You can do this by using the Upgrade Checker.  Microsoft also announced that it will not support Internet Explorer 6.   “From the SharePoint Team blog: SharePoint 2010 will be “targeting standards based browsers (XHTML 1.0 compliant) including Internet Explorer 7, Internet Explorer 8 and Firefox 3.x. running on Windows Operating Systems. In addition we’re planning on an increased level of compatibility with Firefox 3.x and Safari 3.x on non-Windows Operating Systems,” according to the SharePoint Team Blog.”  The architecture will not change as it did between SharePoint Server 2003 to Microsoft Office SharePoint Server 2007. This for insuring that there will be less compatibility issues and a smoother upgrade path (in theory).  Features  Not many features are announced for SharePoint 2010. Some features are announced or are simply speculations. Below you can find a list of features that have been announced or are speculations.  · Claims-based authentication, authentication that is based on other authentication systems like LiveID.  · Web based functionalities for InfoPath.  · SQL Table like behavior for SharePoint lists.  · Filters for indexing of PDF and ODF Files.  · Integration with Jquery.  · Support for OpenXML  · CMIS support will allow interoperability between SharePoint 2010 and other content management systems  · SharePoint 2010 will feature a “Web-enabled Ribbon control” and support greater use of Silverlight controls  · SharePoint as archive for Exchange server  According to Microsoft officials Office 2010 will become available to a wider cross-section testers starting this July. This will be the Community Technology Preview (CTP) test build of Office 2010. This test build will not include SharePoint Server. In July the SharePoint 2010 beta/technology preview will be invitation-only and will focus on a number of its enterprise customers and target specific enterprise deployment scenarios.&nbsp;   Later this year, a public beta will launch for both SharePoint Server 2010 and Project Server 2010.  When the CTP of SharePoint 2010 is available we will learn much more about SharePoint 2010. If some of you know more information about SharePoint 2010 let me know by leaving a comment or sending me an e-mail. When the information is useful I will update this post.  Resources:  · http://blogs.msdn.com/sharepoint/default.aspx  · http://blogs.zdnet.com/microsoft/  · http://www.sharepointbuzz.com/archive/tags/Microsoft%20SharePoint%20Server%202010/default.aspx  Updated: 18-05-2009   ","categories": ["SharePoint"],
        "tags": ["Release","SharePoint 2010"],
        "url": "/2009/03/microsoft-sharepoint-server-2010/",
        "teaser": null
      },{
        "title": "Bulk deletion of SPListItems (SPListItemCollection)",
        "excerpt":" A few days ago I had to develop a TimerJob that deletes a large number of items from a SharePoint list. My first idea was to iterate trough the list items and then call the delete method of the items that needed to be deleted.  Doing this was a major performance set back because it took too long to perform this action.   I went searching for another solution and I found one on the blog of The kid.  A SPWeb object has a method called: ProcessBatchData() that processes a specified batch string of commands for sending multiple requests to the server per transaction.  You can build a batch string to delete all of the items from a SharePoint list like this:   //create new StringBuilder  StringBuilder batchString= new StringBuilder();   //add the main text to the stringbuilder  batchString.Append(&quot;&lt;?xml version=\\&quot;1.0\\&quot; encoding=\\&quot;UTF-8\\&quot;?&gt;&lt;Batch&gt;&quot;);   //add each item to the batch string and give it a command Delete  foreach (SPListItem item in itemCollection)  {     //create a new method section     batchString.Append(&quot;&lt;Method&gt;&quot;);     //insert the listid to know where to delete from     batchString.Append(&quot;&lt;SetList Scope=\\&quot;Request\\&quot;&gt;&quot; + Convert.ToString(item.ParentList.ID) + &quot;&lt;/SetList&gt;&quot;);     //the item to delete     batchString.Append(&quot;&lt;SetVar Name=\\&quot;ID\\&quot;&gt;&quot; + Convert.ToString(item.ID) + &quot;&lt;/SetVar&gt;&quot;);     //set the action you would like to preform      batchString.Append(&quot;&lt;SetVar Name=\\&quot;Cmd\\&quot;&gt;Delete&lt;/SetVar&gt;&quot;);     //close the method section     batchString.Append(&quot;&lt;/Method&gt;&quot;);  }   //close the batch section  batchString.Append(&quot;&lt;/Batch&gt;&quot;);   //preform the batch  SPContext.Current.Web.ProcessBatchData(batchString.ToString());  The only disadvantage that I can think of right know is that all the items you delete will be put in the recycle bin. So if you perform the deletion more than ones you have to be careful with your storage quota.   You can get around that disadvantage by adding the following lines of code. The first section before the batch deletion and the other section after the deletion.  //get the recyclebinenabled status  bool IsRecycleBinEnabled = SPContext.Current.Web.Site.WebApplication.RecycleBinEnabled;   if(IsRecycleBinEnabled)  {     //set the use off the recyclebin to false     SPContext.Current.Web.Site.WebApplication.RecycleBinEnabled = false;  }   //preform batch deletion   //set the use off the recyclebin to true ","categories": ["Development","SharePoint"],
        "tags": ["Delete","SharePoint","TimerJob"],
        "url": "/2009/03/bulk-deletion-of-splistitems-splistitemcollection/",
        "teaser": null
      },{
        "title": "TimerJob status stays Initialized 0%",
        "excerpt":" Last week I was developing a custom TimerJob for SharePoint. For the development of the TimerJob I used the MSDN documentation that is written by Andrew Connell:   http://msdn.microsoft.com/en-us/library/cc406686.aspx  This article really points out everything when you are developing your first TimerJob. I followed the article and I almost read all the code sections so that I knew what to do and added my personal code so that the feature did what I wanted.  The only thing that I had changed was the scope of the feature because I wanted a WebApplication scoped feature. When I deployed the feature and activated it trough the GUI the feature added the TimerJob to SharePoint. The TimerJob also started but it stayed on the status Initialized with 0% complete.  I found this really strange. Because I read the article from Andrew Connell real quick I went back and read it again. I know many developers and most of the time everything has to be done quickly and they forget to read the whole article (just like me) that’s why I wrote this post to point out a section of the article. Because I could not find a solution with Google because Andrew Connell just points it out in the article.  “The Feature that handles the installation and uninstallation of a timer job should be a hidden Feature so that activation is only possible by using Stsadm.exe through the console. This is because when Features are activated through the Web interface, the application pool’s identity is used to execute the code in the Feature receiver. This account typically does not have permissions to install the timer job. Instead, use an account that is part of the farm administrators group to activate the Feature using Stsadm.exe.”   ","categories": ["Development","SharePoint"],
        "tags": ["TimerJob"],
        "url": "/2009/03/timerjob-status-stays-initialized-0/",
        "teaser": null
      },{
        "title": "Sitemap Protocol",
        "excerpt":" When you are developing an internet facing SharePoint site you want search engines too completely and correctly crawl your site. A sitemap protocol (xml sitemap) can help you with that problem. A Sitemap Protocol allows you to inform a search engine about the available urls (pages) on your website that need to be crawled. In simple words it is an xml file with all the urls off your website. When you would like to implement this you have to make the file available in the root off your website. I hear you asking how a sitemap.xml looks like, so here is an example:    &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;  &lt;urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"&gt;     &lt;url&gt;      &lt;loc&gt;http://www.example.com/&lt;/loc&gt;     &lt;lastmod&gt;2005-01-01&lt;/lastmod&gt;      &lt;changefreq&gt;monthly&lt;/changefreq&gt;      &lt;priority&gt;0.8&lt;/priority&gt;     &lt;/url&gt;  &lt;/urlset&gt;  More information about a sitemap.xml file can be found here:    Sitemap.org  But how can you implement this in SharePoint. I think that there are two integration possibilities (If you know more please let me know):    HttpHandler:That generates the xml file on request. An example of this implementation can be found here: Solution. With this solution you can have a sitemap.xml for each site collection you have in a web application  TimerJob:A custom timer job that generates a new sitemap.xml in the root folder of your web application.  In a few weeks I will write a post, were in I will explain how to create a custom timerjob. So let me know if you are interested in that post.   ","categories": ["Development"],
        "tags": ["SEO"],
        "url": "/2009/03/sitemap-protocol/",
        "teaser": null
      },{
        "title": "Problems with Re-installing Windows SharePoint Services",
        "excerpt":" Problems can occur when you are trying to re-install Windows SharePoint Services. The problem occurred when I was trying to install the Dutch version off WSS after I uninstalled the English version.  The Installation of WSS went ok, but when I tried to configure WSS it did not worked correctly.  After some searching I found out that WSS installs his own SQL Server instance but does not uninstall the instance when you remove WSS.  But how do you remove this instance? You cannot uninstall this instance with the normal tools of SQL Server 2005 so you have to do this another way.  1) Using regedit to locate the following registry key:  HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall  2) Under this registry key search for “Embedded”. On the left side of the screen, click the GUID that is found with the search. Copy the value under the key named: “UninstallString”.  3) Go to the command prompt and then paste the string. Now, write the following after the string and press enter:  CALLERID=OCSETUP.EXE  4) An Uninstall program will start.  Know you have uninstalled the instance and you can safely install your version off WSS.   ","categories": ["Administration"],
        "tags": ["Installation","WSS 3.0"],
        "url": "/2009/03/problems-with-re-installing-windows-sharepoint-services/",
        "teaser": null
      },{
        "title": "System.Web.UI.WebControls.WebParts.WebPart versus Microsoft.SharePoint.WebPartPages.WebPart",
        "excerpt":" When I started developing for SharePoint two years ago I was thrown in a big black hole and started to develop webparts by hands on experience.  In the first couple of weeks I found out that your webpart could inherit from System.Web.UI.WebControls.WebParts.WebPart or Microsoft.SharePoint.WebPartPages.WebPart.  I never found out the big difference between those two, also never found something on the internet about it besides that they have different serialization formats, besides that it also uses different attributes to give the developer some influence over how properties get displayed and how they are stored.  WSS Backward-Compatibility Comparison Table     ASP.NET Web Parts SharePoint Backwards Compatibility   WebBrowsableAttribute BrowsableAttribute   WebDisplayName FriendlyName   WebDescription Description   Personalizable WebPartStorage   PersonalizationScope Storage   EditorPart ToolPart   EditorPartCollection ToolPart[]   CreateEditorParts() GetToolParts()   RenderContents() RenderWebPart()   SetPersonalizationDirty() SaveProperties   Category Category    &nbsp;  The question still is: When do we use System.Web.UI.WebControls.WebParts.WebPart or Microsoft.SharePoint.WebPartPages.WebPart?  Microsoft.SharePoint.WebPartPages.WebPart derives from the ASP.NET WebPart class and includes multiple compatibility layers to simplify code migration. This means you should use Microsoft.SharePoint.WebPartPages.WebPart when you are migrating existing code from WSS 2.0. When you are developing new webparts you should use System.Web.UI.WebControls.WebParts.WebPart.  I could recommend you guys to read Inside Microsoft Windows SharePoint Services 3.0 because this information is coming from that book and it is really great book.   ","categories": ["Development"],
        "tags": ["ASP.net","SharePoint","Web Part"],
        "url": "/2009/04/system-web-ui-webcontrols-webparts-webpart-versus-microsoft-sharepoint-webpartpages-webpart/",
        "teaser": null
      },{
        "title": "Access denied while crawling sites",
        "excerpt":" Today I received the strangest error when I was configuring my SharePoint Virtual PC to crawl my websites. The crawler inserted a error for each web application containing the following text: Access Denied.   Since this was a new environment and I certainly know that the accounts where set correctly I was a bit amassed about the fact it would give this error. Checking the default content access account rights in the “Policy for Web Application” in Central Administration I saw that is was set correctly to “Full Read”.  I consulted one of my colleges but he also did not have an answer for this. The strange thing about all of this was that was a web application that was crawled. The difference between the application that was crawled and the other applications was that for the other applications I created a host name. The application that was crawled was using “Computername:Port number”.  After looking in de log files off SharePoint I found the following entry:  Couldn’t retrieve server http://dev.motion10-dev.local policy, hr = 80041205 – File:d:\\office\\source\\search\\search\\gather\\protocols\\sts3\\sts3util.cxx Line:543  Searching on this error with Google let me to a support page from Microsoft. The page states that when you are using Windows server 2003 SP1 or Windows XP SP2 authentication fails because Windows XP SP2 and Windows Server 2003 SP1 include a loopback check security feature that is designed to help prevent reflection attacks on your computer. Therefore, authentication fails if the FQDN or the custom host header that you use does not match the local computer name.  Poorly Microsoft states that this only occurs when you are using Windows Server 2003 SP1 or Windows XP SP2. As I found out it also occurs when you are working with Windows Server 2008!  You can solve this problem by following one of the two solutions displayed below. I followed the second solution because I had this problem on a development machine. In a production environment I would have followed the first solution just like Microsoft recommends:  Solution 1:  1. Click Start, click Run, type regedit, and then click OK.  2. In Registry Editor, locate and then click the following registry key:  HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Lsa\\MSV1_0  3. Right-click MSV1_0, point to New, and then click Multi-String Value.  4. Type BackConnectionHostNames, and then press ENTER.  5. Right-click BackConnectionHostNames, and then click Modify.  6. In the Value data box, type the host name or the host names for the sites that are on the local computer, and then click OK. You do not have to use a separation character just separate then by pressing enter.  7. Quit Registry Editor, and then restart the IISAdmin service.  Solution 2:  1. Click Start, click Run, type regedit, and then click OK.  2. In Registry Editor, locate and then click the following registry key:  3. HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Lsa  4. Right-click Lsa, point to New, and then click DWORD Value.  5. Type DisableLoopbackCheck, and then press ENTER.  6. Right-click DisableLoopbackCheck, and then click Modify.  7. In the Value data box, type 1, and then click OK.  8. Quit Registry Editor, and then restart your computer.  You can read the complete Microsoft Support Page here: http://support.microsoft.com/kb/896861   ","categories": ["Administration","SharePoint","Troubleshooting"],
        "tags": ["DisableLoopBackCheck","KB","SharePoint","Windows Server"],
        "url": "/2009/04/access-denied-while-crawling-sites/",
        "teaser": null
      },{
        "title": "Office Service Pack will be available on 28 April",
        "excerpt":" The Microsoft Update Product Team Blog announced today that Office Service Pack 2 will be released on 28 April this year.  The formal changes you can aspect for the server products are:  Windows SharePoint Services 3.0 SP2 and Microsoft Office SharePoint Server SP2 include fixes and enhancements designed to improve performance, availability, and stability in your server farms. SP2 provides the groundwork for future major releases of SharePoint Products and Technologies.   An STSADM command line that scans your server farm to establish whether it is ready for upgrade to the next version of SharePoint and provides feedback and best practice recommendations on your current environment.   SP2 offers support for standards-based documents formats and compatibility with a broader range of Web browsers.   Substantial improvements to Forms-based authentication.  Windows Server 2008 SP2 and Windows Server R2 will be supported on their release.  Enterprise Content Management (ECM)   The performance and stability of content deployment and variations feature has been improved.   A new tool has been added to the STSADM command-line utility that enables a SharePoint administrator to scan sites that use the variations feature for errors.  Excel and Access Services   SP2 makes it easier to configure Excel Web Access Web parts on new sites. Several rendering, calculation, and security issues have been resolved.   Some display issues have been addressed.   Improved compatibility with Mozilla Firefox browsers.  Groove Server   Improved synchronization reliability.   Groove Server 2007 Manager will install and run with SQL 2008.   Groove’s LDAP connectivity and auto-activation functionality have been improved.   Error reporting in the Groove Relay Server has improved significantly.   Groove Relay Server has improved robustness.  Forms Server   Memory requirements and the page load times for large browser-rendered forms have been reduced.   Browser rendering of various controls, such as the ‘cannot be blank’ asterisk and the rich text field has been improved.  Project Server   Better memory management in the queue service.   Performance to certain database table indexes is improved   Resource plans, build team, cost resources, and the server scheduling engine have improved.  Search Server   Improvements to the reliability and stability of very large corpus crawls.   Backup-restore has been improved.   A new command has been introduced to the stsadm.exe tool that lets a SharePoint Administrator to tune the Query processor multiplier parameter.   Improved accuracy in searches involving numbers.   ","categories": ["Administration","Microsoft Office","SharePoint"],
        "tags": ["Office","Service Pack","SharePoint"],
        "url": "/2009/04/office-service-pack-will-be-available-on-28-april/",
        "teaser": null
      },{
        "title": "Access SharePoint Data from the Database",
        "excerpt":" When you are developing web parts or other kind of solutions for SharePoint it is possible you work yourself in a situation where you use the SharePoint API for your solutions but you do not want to use it because it is insufficient.  When you are in a situation like this you can choose to connect to the content database of your site collection. When you choose to connect to the database you have to remember you should not alter the database directly but use the API for changing properties.  A few days ago I was in a situation like this. One of my colleges asked me if there was a web part in SharePoint were in he could see all the alerts you are subscripted to. The answer was no so I started looking if I could develop a web part.  I started with retrieving the alerts for the current user like this:  SPAlertCollection alertCollection = SPContext.Current.Web.CurrentUser.Alerts;  I thought I would get all off the alerts off the current user for the entire farm but that wasn’t it. The CurrentUser.Alerts only retrieves the alerts for the user within the context you are in, so I received all off the user alerts that were made in that particular SPWeb.  So I thought lets iterate trough all the site collections and then retrieve the alert like this:   List&lt;SPAlert&gt; Alerts = new List&lt;SPAlert&gt;();   SPWebApplication webApp = SPContext.Current.Site.WebApplication;   foreach (SPSite site in webApp.Sites)  {     using (site)     {        foreach (SPWeb web in site.AllWebs)        {           using (web)           {              SPAlertCollection alerts = web.CurrentUser.Alerts;               foreach (SPAlert alert in alerts)              {                 Alerts.Add(alert);              }           }        }     }  } The solution works but the problem with this solution is when you get to a site you don’t have access to you will get a access denied and besides that if you have a large collection of SPWebs or SPSites it will take forever to perform this piece of code.  So my next solution was to retrieve the alerts from the database. You can do this by retrieving the content databases from your web application and connect to them. You can use a query like below to retrieve all the alerts for the user you want.  SELECT SchedSubscriptions.Id, SchedSubscriptions.NotifyFreq, SchedSubscriptions.WebId, SchedSubscriptions.SiteId, (SchedSubscriptions.SiteUrl+SchedSubscriptions.WebUrl) AS URL, SchedSubscriptions.ListTitle, SchedSubscriptions.AlertTitle, SchedSubscriptions.AlertType, SchedSubscriptions.Status  FROM  SchedSubscriptions  WITH  (NOLOCK)  Where  SchedSubscriptions.UserId = 1 AND  SchedSubscriptions.Deleted = 0  UNION   SELECT  ImmedSubscriptions.Id, NotifyFreq = 0, ImmedSubscriptions.WebId, ImmedSubscriptions.SiteId, (ImmedSubscriptions.SiteUrl+ImmedSubscriptions.WebUrl) AS  URL, ImmedSubscriptions.ListTitle, ImmedSubscriptions.AlertTitle, ImmedSubscriptions.AlertType, ImmedSubscriptions.Status  FROM  ImmedSubscriptions  WITH  (NOLOCK)  Where  ImmedSubscriptions.UserId = 1 AND  ImmedSubscriptions.Deleted = 0 This is a simple query that queries the content database for all the alerts. The alerts are saved in two tables:   ImmedSubscriptions  SchedSubscriptions  The WITH (NOLOCK) we use in the query is for making sure the tables will not get locked will retrieving the values.  My code for retrieving all off the alerts for a user within a web application looked something like this:   private const string getAlertCommand = \"SELECT SchedSubscriptions.Id, SchedSubscriptions.NotifyFreq, SchedSubscriptions.WebId, SchedSubscriptions.SiteId, (SchedSubscriptions.SiteUrl+SchedSubscriptions.WebUrl) AS URL, SchedSubscriptions.ListTitle, SchedSubscriptions.AlertTitle, SchedSubscriptions.AlertType, SchedSubscriptions.Status FROM SchedSubscriptions WITH (NOLOCK) Where SchedSubscriptions.UserId = {0} AND SchedSubscriptions.Deleted = 0 UNION SELECT ImmedSubscriptions.Id, NotifyFreq = 0, ImmedSubscriptions.WebId, ImmedSubscriptions.SiteId, (ImmedSubscriptions.SiteUrl+ImmedSubscriptions.WebUrl) AS URL, ImmedSubscriptions.ListTitle, ImmedSubscriptions.AlertTitle, ImmedSubscriptions.AlertType, ImmedSubscriptions.Status FROM ImmedSubscriptions WITH (NOLOCK) Where ImmedSubscriptions.UserId = {0} AND ImmedSubscriptions.Deleted = 0\";   protected override void CreateChildControls()  {     base.CreateChildControls();      List&lt;SPContentDatabase&gt; contentdatabases = new List&lt;SPContentDatabase&gt;();      Guid siteID = SPContext.Current.Site.ID;     int UserID = SPContext.Current.Web.CurrentUser.ID;      SPSecurity.RunWithElevatedPrivileges(delegate()     {        SPWebApplication webAPP = null;         using (SPSite site = new SPSite(siteID))        {           webAPP = site.WebApplication;            if (webAPP != null)           {              contentdatabases = RetrieveContentDatabases(webAPP);               foreach (SPContentDatabase database in contentdatabases)              {                  //use the sql connection so that it will be disposed when we are done                 using (SqlConnection con = new SqlConnection(database.DatabaseConnectionString))                 {                    //open the database connection                    con.Open();                     //Use the command. It will be disposed when we are done using it                    using (SqlCommand com = con.CreateCommand())                    {                       com.CommandText = string.Format(getAlertCommand, UserID.ToString());                        //read the informtion from the from the database                                       using (SqlDataReader reader = com.ExecuteReader())                       {                           while (reader.Read())                           {                              //do what you like to do with the information you retrieved                           }                        }                     }                  }               }            }         }      });  }  ","categories": ["Development"],
        "tags": ["Database","SharePoint Data"],
        "url": "/2009/04/access-sharepoint-data-from-the-database/",
        "teaser": null
      },{
        "title": "Search Results on a Internet Facing SharePoint Site",
        "excerpt":"When you crawled a SharePoint site everything on that site will become visible on the search results page of your search center. If you have an internet facing SharePoint site you do not want to see list views or other background pages in the search results. These pages can be filtered out from your search results by making crawl rules in the Shared Service Provider. You can create crawl rules in the following steps: 1: Navigating to your Shared Service Provider. 2: Click search settings under Search. 3: Then click Crawl Rules. In this screen you can add new Crawl rules by selecting new crawl rule. If you want to exclude background pages from the search results you can insert some of the following crawl rules:  *://*webfldr.aspx – This will exclude all explorer view pages if you choice it as a exclude rule.   *://*mod-view.aspx* – This will exclude the Moderation view page if you choice it as a exclude rule.   *://*my-sub.aspx* – This will exclude the page with your items if you choice it as a exlude rule.   *://*allitems.aspx* – This will exclude the allitems page from the search results if you choice it as a exclude rule.   *://*allforms.aspx* – This will exclude the all forms page from the search results if you choice it as a exclude rule.   *://*/lists/* – This will exclude the list from the search results if you choice it as a exclude rule.   *://*DispForm.aspx* – This will exclude the list display form from the search results if you choice it as a exclude rule.  If you have entered these crawl rules in you shared service provider and you run a full crawl again you will notice that you have less items indexed.  ","categories": ["Administration","SharePoint"],
        "tags": ["Internet site","Search results"],
        "url": "/2009/04/search-results-on-a-internet-facing-sharepoint-site/",
        "teaser": null
      },{
        "title": "Handle access denied for application pages",
        "excerpt":" Within SharePoint you have the ability to create custom application pages. Application pages remain in the layouts folder of SharePoint. Each user can access these pages by typing in the URL in the explorer bar.  When you would navigate to one of these pages and you do not have enough rights, you would expect that you would be redirected to the access denied page. This is does not happen.   You can try this with a user that has minimal rights and navigate to the following page /_layouts/srchvis.aspx (An out of the box application page for settings the search visibility). You will see that the page gets rendered.  When you create a custom application page you can work around by doing the following:    protected override void OnLoad(EventArgs e) {   base.OnLoad(e);    if (SPContext.Current.Web.UserIsWebAdmin) {       if (!Page.IsPostBack) {          //perform your actions       }   }   else {        SPUtility.HandleAccessDenied(new Exception(\"You do not have access to this page.\"));     }   }  In the OnLoad of your page you can check whether the user has sufficient rights. In the example I perform this action by checking if the user is a site admin.  When the user hasn’t sufficient rights you can redirect him to the access denied page of SharePoint by using the HandleAccessDenied() method of the SPUtility class.   ","categories": ["Development","SharePoint"],
        "tags": ["Acces denied","Application pages","Security"],
        "url": "/2009/04/handle-access-denied-for-application-pages/",
        "teaser": null
      },{
        "title": "SharePoint 2007 Service Pack 2",
        "excerpt":" Yesterday Service Pack 2 was released for SharePoint. In my post “Office Service Pack will be available on 28 April” you can read about some changes that are included in the service pack.  More information about the Service Pack can be found in the KB articles:   Description of Windows SharePoint Services 3.0 SP2 and of Windows SharePoint Services 3.0 Language Pack SP2  http://support.microsoft.com/kb/953338   Description of 2007 Microsoft Office servers Service Pack 2 (SP2) and of 2007 Microsoft Office servers Language Pack Service Pack 2 (SP2)  http://support.microsoft.com/kb/953334  The Service Packs can be downloaded from the Microsoft download center:   Service Pack 2 for Windows SharePoint Services 3.0, x86 &amp; x64  http://www.microsoft.com/downloads/details.aspx?FamilyId=79BADA82-C13F-44C1-BDC1-D0447337051B&amp;displaylang=en   Service Pack 2 for Office SharePoint Server 2007, x86 &amp; x64  http://www.microsoft.com/downloads/details.aspx?FamilyId=B7816D90-5FC6-4347-89B0-A80DEB27A082&amp;displaylang=en   ","categories": ["Administration","SharePoint"],
        "tags": ["Service Pack","SharePoint 2007"],
        "url": "/2009/04/sharepoint-2007-service-pack-2/",
        "teaser": null
      },{
        "title": "Branding a SharePoint site &ndash; Part 1",
        "excerpt":" Within SharePoint and WSS you have the possibility to brand your site. You can do this on several ways:   A custom style sheet.   A custom theme.  I think the best way to brand your SharePoint site is to create a custom theme.   You can create a theme by copying one of the themes folders in the “C:\\Program Files\\Common Files\\microsoft shared\\Web Server Extensions\\12\\TEMPLATE\\THEMES” directory. When you have a copy of a directory you have to rename it to whatever you like.  In this article we will use as example a copy of the “Simple” theme and rename it to “motion10”.  In the directory you have three files that are necessary to create a theme. The images can be deleted.  INF file  In this file you can find information about your theme. The file contains two sections the [info] section and the [titles] section.  In the [info] section information is given about the theme. In this section we have to change the title from “simple” to “motion10”.  In the [titles] section the title of the theme is given for each language code besides the title for the language code 1033. This title is saved into SPThemes.xml file (This file will be discussed later in this article).  After we made the changes to the file we save and rename it. You have to give it the same name as your folder. We will rename it from “Simple.inf” to “motion10.inf”.  Theme.css  The theme.css file is the style sheet file off the theme. In this file you will have to define your own styling.  mossExtension.css  The mossExtension.css is an extension file for the Theme.css file. When the Theme.css file is loaded it will be extended with the content of this file.  When you want to create a custom theme you can use several tools to help you edit the theme.css file. One of my favorites is the Web Developers tool in Internet Explorer 8. You can use this tool by opening your SharePoint site and selecting: “Tools – Developer Tools” or just press F12.  This will open a window like below. If you select the arrow at the top of the page you are able to select a section of your site and see the css styling of it.    On my SharePoint site I have selected the site title. On the right side of the developer tool you can see the styling of the site title. Every item that is stripped away will not be used so you can see that the colour of the title is changes to #0066a4 in moti1011-65001.css. This file is the theme.css file in our motion10 theme folder.   When you are done with the creation of the theme you have to make it available within the SharePoint farm. The theme can be added by updating the “SPThemes.xml” file within the following directory:  C:\\Program Files\\Common Files\\microsoft shared\\Web Server Extensions\\12\\TEMPLATE\\LAYOUTS\\1033  If you examine the file you can see that every theme has its own section. You have to add a section for your theme. You can do this by copying one of the Templates nodes and pasting it into the file and change the following properties:   TemplateID: The ID of your template. This has be the same name as your folder. In our example it will be “motion10”.   DisplayName: This is the display name for your theme.   Description: This is the description of your theme.   Thumbnail: This is the URL to a picture that represents your theme. This will be shown on the themes page. Example: “images/motion10/theme/theme.jpg”.   Preview: This is the URL to a picture that represents your theme. Example: “images/motion10/theme/theme.jpg”.  When you save the file the theme will be available in the SharePoint Farm.   “Important: When you use multiple servers in the SharePoint farm you will have to change the SPThemes file on every server. You also have to create the themes folder on every server.”  Manually editing of files in the farm is not desired and support. To solve this you have to create a solution using WSP Builder and add your Theme folder to the right path in your solution. You also have to make changes to the “SPThemes.xml” so you also add this file to your solution. You will have a problem with this because you have the complete file and extend it with you section. When you deploy your solution the out of the box file will be overwritten. But what happens when another SharePoint developer also makes a theme en deploys it to the server. He will also overwrite the file with the result that your theme will be deleted.  To work around this problem you have to create a farm feature that will kick off a timer job to alter the SPThemes file.  First we have to create a farm scoped feature that has a feature receiver.   &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;  &lt;Feature  Id=\"e0c634c2-5378-4fe6-927b-03aa895b5d8d\"                Title=\"motion10 Theme Installer : motion10\"                Description=\"Feature for adding the motion10 theme to the SharePoint Farm.\"                Version=\"12.0.0.0\"                Hidden=\"FALSE\"                Scope=\"Farm\"                DefaultResourceFile=\"core\"                ReceiverAssembly=\"Motion10.SharePoint.Theme, Version=1.0.0.0, Culture=neutral, PublicKeyToken=09e87b786333535e\"                ReceiverClass=\"Motion10.SharePoint.Theme.ThemeInstaller\"                Creator=\"Maik van der Gaag\"                ImageUrl=\"motion10/FeaturesIcon.png\"                ImageUrlAltText=\"http://www.motion10.com\"                xmlns=\"http://schemas.microsoft.com/sharepoint/\"&gt;    &lt;Properties&gt;      &lt;Property Key=\"Theme:TemplateID\" Value=\"motion10\" /&gt;      &lt;Property Key=\"Theme:DisplayName\" Value=\"motion10\" /&gt;      &lt;Property Key=\"Theme:Description\" Value=\"Theme for enabling the motion10 styling.\" /&gt;      &lt;Property Key=\"Theme:Thumbnail\" Value=\"images/motion10/theme/theme.jpg\" /&gt;      &lt;Property Key=\"Theme:Preview\" Value=\"images/motion10/theme/theme.jpg\" /&gt;    &lt;/Properties&gt;  &lt;/Feature&gt;   In the properties section of the feature we have defined 5 properties. These properties represent the values that you have to fill into the SPThemes.xml. In the SPFeatureReceiver we will start the timer job to insert the values into the file.  public override void FeatureActivated(SPFeatureReceiverProperties properties) {          RunJobNow(false, properties);  }   public override void FeatureDeactivating(SPFeatureReceiverProperties properties) {          RunJobNow(true, properties);  }   public override void FeatureInstalled(SPFeatureReceiverProperties properties) {  }   public override void FeatureUninstalling(SPFeatureReceiverProperties properties) {  }  In the FeatureActivated and FeatureDeactivating event we call a method called “RunJobNow” with two properties. These properties represent a boolean if the section must be deleted and the property bag of the feature for retrieving the property values.  public void RunJobNow(bool delete, SPFeatureReceiverProperties prop) {      SPFarm farm = prop.Definition.Farm;      InstallThemeTimerJob newJob = null;      foreach (SPService service in farm.Services) {        if (service.Name == serviceName) {            newJob = new InstallThemeTimerJob(service, prop.Definition.Id, delete);            foreach (SPJobDefinition def in service.JobDefinitions) {              if (def.Name == newJob.Name) {                 def.Delete();              }           }          break;        }     }      newJob.Schedule = new SPOneTimeSchedule(DateTime.Now.AddHours(-2.0));     newJob.Title = string.Format(\"{0} Theme {1} for Feature {2}\", delete ? \"Delete\" : \"Install\", prop.Definition.Properties[\"Theme:TemplateID\"], prop.Definition.Id);     newJob.Update();  } The RunJobNow method will find a service called “WSSAdministration” on the farm. We need to find a service because the timer job needs to be attached to a service. By creating a new InstallThemeTimerJob object you create a new timer job in SharePoint. By adding the Schedule to the InstallThemeTimerJob object you can ensure that the job will run immediately.  When we create a new InstallThemeTimerJob object you can see that we have to define three properties. These properties must be defined to let the timer job know what to do.  When we have finished the feature it is time to create the timer job. In the SPJobDefinition we have four read only strings that define some values we need:    filePath: The path to the SPThemes.xml file : “TEMPLATE\\\\LAYOUTS\\\\1033\\\\SPTHEMES.XML”  deleteKey: The key of the property in the property bag.  featureKey: The key of the property in the property bag.  documentNameSpace: The namespace of the xml document.  Within the timer job we have five properties to retrieve the properties from the feature definition. These properties need to be retrieved for adding our Templates section to the SPThemes file.   public string TemplateID {     get {        SPFeatureDefinition def = Farm.FeatureDefinitions[this._featureID];         SPFeatureProperty prop = def.Properties[\"Theme:TemplateID\"];         return prop.Value;     }  } For the initiation of our timer job we will have to insert two properties in the property bag. These properties are “_delete” and “_featureid”.  private readonly string deleteKey = \"C11B44D604004a1d9CD9D0CAB326373F_Delete\";   private bool _delete {     get {        if (this.Properties.ContainsKey(deleteKey)) {           return Convert.ToBoolean(this.Properties[deleteKey]);        } else {           return false;        }     }     set {        if (this.Properties.ContainsKey(deleteKey)) {           this.Properties[deleteKey] = value.ToString();        } else {           this.Properties.Add(deleteKey, value.ToString());        }     }  }  For the creation of a timer job you also need to define a LockType. This LockType is very important because the timer job has to be run on every server in the farm. For accomplishing this we will have to use SPLockType.None ( http://www.shillier.com/archive/2009/05/01/where-is-my-timer-job.aspx)   public InstallThemeTimerJob() {  }   public InstallThemeTimerJob(SPService service, Guid featureID, bool delete)     : base(\"Themes Installer TimerJob\", service, null, SPJobLockType.None) {      //save the properties     this._delete = delete;     this._featureID = featureID;  } On the execution of the timer job we will execute a method called ChangeThemesFile(). Within this method we will add or delete a section in the SPThemes file that is defined in the properties of the feature definition.  private readonly string filePath = \"TEMPLATE\\\\LAYOUTS\\\\1033\\\\SPTHEMES.XML\";  private readonly string documentNameSpace = \"http://tempuri.org/SPThemes.xsd\";   public override void Execute(Guid targetInstanceId) {        base.Execute(targetInstanceId);         this.ChangeThemesFile();  }    private void ChangeThemesFile() {      XNamespace ns = documentNameSpace;     string spThemesContent = string.Empty;      XDocument doc = XDocument.Load(SPThemesFile);      var element = from b in doc.Element(ns + \"SPThemes\").Elements(ns + \"Templates\")                        where b.Element(ns + \"TemplateID\").Value == this.TemplateID                        select b;  ","categories": ["Design"],
        "tags": ["Branding","CSS","Development"],
        "url": "/2009/05/branding-a-sharepoint-site-part-1/",
        "teaser": null
      },{
        "title": "Branding a SharePoint site &ndash; Part 2",
        "excerpt":" Last week I wrote the first article in a series of articles about branding your SharePoint site. In the first article we discussed how to brand your site and make it available within SharePoint with a Timer Job. In the following article we will make our theme the default theme by using Feature stapling and we will discuss a way to replace all the out of the box search images without replacing the default one in the 12 hive. To make our theme the default theme we will have to create two features. One feature (feature stapling) will activate a feature that activates our theme when a site is created. First we will create a feature to activate our theme. The feature.xml will look like this:   &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;  &lt;Feature  Id=\"b84c13fd-b1ab-4d90-a2c1-9701c732cb5e\"            Title=\"motion10 Theme Activator : motion10\"            Description=\"Feature that will activate the motion10 theme for the web. It will also replace the out of the box search images if you have jQuery enabled.\"            Version=\"12.0.0.0\"            Hidden=\"FALSE\"            Scope=\"Web\"            DefaultResourceFile=\"core\"            ReceiverAssembly=\"Motion10.SharePoint.Theme, Version=1.0.0.0, Culture=neutral, PublicKeyToken=09e87b786333535e\"            ReceiverClass=\"Motion10.SharePoint.Theme.ThemeActivator\"            Creator=\"Maik van der Gaag\"            ImageUrl=\"motion10/FeaturesIcon.png\"            ImageUrlAltText=\"http://www.motion10.com\"            xmlns=\"http://schemas.microsoft.com/sharepoint/\"&gt;    &lt;Properties&gt;      &lt;Property Key=\"Theme:TemplateID\" Value=\"motion10\"/&gt;    &lt;/Properties&gt;  &lt;/Feature&gt; The feature has a feature receiver that activates the theme that is defined in the feature property called “Theme:TemplateID”. The receiver will activate the theme on the web wherefore the feature is activated. On the deactivation of the feature it will deactivate our theme and activate the default theme.  public static readonly string keyTheme = \"Theme:TemplateID\";  public override void FeatureActivated(SPFeatureReceiverProperties properties) {     try {          SPWeb web = (SPWeb)properties.Feature.Parent;          string theme = properties.Definition.Properties[keyTheme].Value;           if (web.Theme != theme) {              web.ApplyTheme(theme);              web.Update();          }     } catch {           throw;     }  }   public override void FeatureDeactivating(SPFeatureReceiverProperties properties) {      try {           SPWeb web = (SPWeb)properties.Feature.Parent;           string theme = properties.Definition.Properties[keyTheme].Value;            if (web.Theme == theme) {               web.ApplyTheme(\"none\");               web.Update();           }      } catch {         throw;      }  } As you can see in the code the theme is activated with the ApplyTheme() method on the Web object that we retrieve from the feature properties. When we deactivate the feature we check if the current theme is our theme because we do not want to change the theme if the current theme is not the one we created. When we have created the feature it is time to create a feature with feature stapling. Feature stapling is a way to activate a feature when a site is created with a certain site template. First we create a feature.xml file with a scope of web application.   &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;  &lt;Feature  Id=\"b84r13fd-b1ab-4d90-a2c1-9701g732cb5e\"            Title=\"motion10 Theme Activator by defailt : motion10\"            Description=\"Feature that will activate the motion10 theme by default when a site is created.\"            Version=\"12.0.0.0\"            Hidden=\"FALSE\"            Scope=\"WebApplication\"            Creator=\"Maik van der Gaag\"            ImageUrl=\"motion10/FeaturesIcon.png\"            ImageUrlAltText=\"http://www.motion10.com\"            xmlns=\"http://schemas.microsoft.com/sharepoint/\"&gt;    &lt;ElementManifests&gt;      &lt;ElementManifest Location=\"elements.xml\" /&gt;     &lt;/ElementManifests&gt;  &lt;/Feature&gt; In the feature you can see that we defined an elements manifest. In the elements manifest we will define the feature stapling like this:   &lt;Elements xmlns=\"http://schemas.microsoft.com/sharepoint/\"&gt;     &lt;FeatureSiteTemplateAssociation Id=\"b84c13fd-b1ab-4d90-a2c1-9701c732cb5e\" TemplateName=\"STS#0\" /&gt;     &lt;FeatureSiteTemplateAssociation Id=\"b84c13fd-b1ab-4d90-a2c1-9701c732cb5e\" TemplateName=\"STS#1\" /&gt;     &lt;FeatureSiteTemplateAssociation Id=\"b84c13fd-b1ab-4d90-a2c1-9701c732cb5e\" TemplateName=\"STS#2\" /&gt;  &lt;/Elements&gt; The Id represents the feature Id of the feature that activates our theme. If you would like to make your theme the default theme for every site that is created within your web application you will have to insert a FeatureSiteTemplateAssociation with the template name “GLOBAL”:   &lt;Elements xmlns=\"http://schemas.microsoft.com/sharepoint/\"&gt;     &lt;FeatureSiteTemplateAssociation Id=\"b84c13fd-b1ab-4d90-a2c1-9701c732cb5e\" TemplateName=\"GLOBAL\" /&gt;  &lt;/Elements&gt; Note: If you use GLOBAL make sure you explicitly make a FeatureSiteTemplateAssociation for STS#1 because the blank site has an attribute “AllowGlobalFeatureAssociations” which is set to false.   Now we have discussed a way to make the theme the default theme we want the search images to be replaced with our own images without replacing the search image in the 12 hive. To accomplish this we will create a jQuery method that replaces the standard gosearch.gif with our own search image. For adding the jQuery method to all of the pages we will use the delegate control AdditionalPageHead to load our jQuery that we add trough a user control. All these components have to be packed into a feature, because we want a minimal count of features we extend our theme activation feature with an elements manifest.   &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;  &lt;Feature  Id=\"b84c13fd-b1ab-4d90-a2c1-9701c732cb5e\"            Title=\"motion10 Theme Activator : motion10\"            Description=\"Feature that will activate the motion10 theme for the web. It will also replace the out of the box search images if you have jQuery enabled.\"            Version=\"12.0.0.0\"            Hidden=\"FALSE\"            Scope=\"Web\"            DefaultResourceFile=\"core\"            ReceiverAssembly=\"Motion10.SharePoint.Theme, Version=1.0.0.0, Culture=neutral, PublicKeyToken=09e87b786333535e\"            ReceiverClass=\"Motion10.SharePoint.Theme.ThemeActivator\"            Creator=\"Maik van der Gaag\"            ImageUrl=\"motion10/FeaturesIcon.png\"            ImageUrlAltText=\"http://www.motion10.com\"            xmlns=\"http://schemas.microsoft.com/sharepoint/\"&gt;    &lt;ElementManifests&gt;      &lt;ElementManifest Location=\"elements.xml\"/&gt;    &lt;/ElementManifests&gt;    &lt;Properties&gt;      &lt;Property Key=\"Theme:TemplateID\" Value=\"motion10\"/&gt;    &lt;/Properties&gt;   &lt;/Feature&gt; In the elements file we insert our delegate control.  &lt;?xml version=\"1.0\" encoding=\"utf-8\" ?&gt;  &lt;Elements xmlns=\"http://schemas.microsoft.com/sharepoint/\"&gt;    &lt;Control Id=\"AdditionalPageHead\"             ControlClass=\"Motion10.SharePoint.Theme.SearchImageReplacer\"             ControlAssembly=\"Motion10.SharePoint.Theme, Version=1.0.0.0, Culture=neutral, PublicKeyToken=09e87b786333535e\"&gt;    &lt;/Control&gt;  &lt;/Elements&gt; In the control we place the library off jQuery on the page and also insert our own method. It will search for tags with the attribute src that end with gosearch.gif (default image) and replace the src attribute with our own ImageUrl. Besides that we remove the onmouseover and onmouseout attribute.  public class SearchImageReplacer : UserControl {           public static readonly string ImageUrl = \"/_layouts/images/motion10/Search/gosearch.gif\";          public static readonly string DefaultSearchImageUrl = \"/_layouts/images/gosearch.gif\";           protected override void OnPreRender(EventArgs e) {              base.OnPreRender(e);               StringBuilder javascript = new StringBuilder();               javascript.Append(\"if(typeof jQuery != \\\"undefined\\\") {\");              javascript.Append(\"$(function(){\");              javascript.Append(\"$(\\\"[src$='gosearch.gif']\\\").attr(\\\"src\\\", \\\" \" + ImageUrl + \" \\\").removeAttr(\\\"onmouseover\\\").removeAttr(\\\"onmouseout\\\")\");              javascript.Append(\"});\");              javascript.Append(\"}\"); ","categories": ["Design"],
        "tags": ["Branding","CSS","Features"],
        "url": "/2009/05/branding-a-sharepoint-site-part-2/",
        "teaser": null
      },{
        "title": "Starting a workflow through code",
        "excerpt":" Within SharePoint there can be situation in which you would like to start a workflow through code, like in an event receiver. But how do you start a workflow trough code? In the following minutes I will explain how you can accomplish this.  Before you can start a workflow you will have to have a SPListItem for which you want to start the workflow and something to retrieve the SPWorkflowAssociation with. In this example we will use the BaseID of the workflow. The BaseID of a workflow is defined in the elements manifest of the workflow feature.  If you have the two properties you ready to think about the restrictions you have when you start a workflow. The restriction is that a workflow can only be started once on an item. If it is already running and you still try to activate it you will get an exception (Exception from HRESULT: 0×8102009B).  This means that before we can start the workflow we have to check if the workflow isn’t running. In the code block below you see a method for this.  Important: In this method is written with an extension for the .Net Framework called Linq. If you want to use Linq you will have to build your project as a 3.5 Framework project and add a Reference to System.Linq.dll.   public static SPWorkflowAssociation GetWorkflowAssociationByID(SPList list, Guid id)  {     var query = from SPWorkflowAssociation asso in list.WorkflowAssociations                     where asso.BaseId == id                     select asso;      return query.FirstOrDefault();  }  In the above method we use a query to find a workflow based on the BaseID. With the SPWorkflowState.Running we check if the workflow is running.  After the executing of the query we have the results and check if it has objects. If there are more than zero SPWorkflow objects in the results a workflow with our BaseID is currently running.  When we have checked this we will have to find the SPWorkflowAssociation of our workflow so that we can start the workflow with this association.  The workflow association can also be find with the BaseID.   public static SPWorkflowAssociation GetWorkflowAssociationByID(SPList list, Guid id)  {     var query = from SPWorkflowAssociation asso in list.WorkflowAssociations                     where asso.BaseId == id                     select asso;      return query.FirstOrDefault();  }  In the above method we find the workflow association with the BaseID on the list of the item and return the first or default. We return the first or default because there can only be one association and it will return null by default when there are no associations.  Important: If you want to start a workflow trough code the workflow has to be associated with the parent list of the item.  After performing these actions we can finally start our workflow.   public static void StartWorkflow(SPItem item, Guid BaseID){     SPList list = item.ParentList;     SPWeb web = item.ParentList.ParentWeb;      if(!IsWorkflowRunning(item, BaseID){        SPWorkflowAssociation workflowAsso = GetWorkflowAssociationByID(list, BaseID);         if(workflowAsso != null){           web.Site.WorkflowManager.StartWorkflow(item, workflowAsso, \"\");        }     } ","categories": ["Development","SharePoint"],
        "tags": ["SharePoint","Start","Workflow"],
        "url": "/2009/06/starting-a-workflow-through-code/",
        "teaser": null
      },{
        "title": "Information Management Policy Configuration",
        "excerpt":" In 2008 I wrote a post about testing your custom expiration policy (here). In this post I explained that the expiration policy is only runs once day what is correct (depends on the setting in Central Administration), I also describe that you can’t run it at a specific time to test you custom policy, only if you would write custom code. For that reason I created a console application that you can find here to run the expiration policy. Today, when I was learning for the SharePoint 2007 Administration certificate I found something new that made my post about testing a expiration policy not true. I found out that you can run the expiration policy without writing custom code or using my application.  To do this follow the following steps:   1 – Goto “Central Administration”   2 – Click on “Operations”   3 – Click on “Information management policy configuration” within the section “Security Configuration”  4 – Click on the Policy you would like to test. In this example it is the expiration policy.   5 – Press on “Process Expired Items Now”       ","categories": ["SharePoint"],
        "tags": ["Information Management Policy"],
        "url": "/2009/06/information-management-policy-configuration/",
        "teaser": null
      },{
        "title": "SharePoint 2010 Preview",
        "excerpt":" Today I was reading a post of one of my old colleges. In this post he mentioned a link to a site that gives you a preview to SharePoint 2010.  On this site there are several video’s that show you highlights of SharePoint 2010.   Overview   IT Professional   Developer  Some key points that are mentioned in the videos:   Ribbon in SharePoint 2010.   Themes from PowerPoint presentations   Standard Silverlight webpart.   Rendering Visio in the browser   BCS read write functionality (formally know as BDC).   Backup and restore.   Visual Studio 2010 Extensions for deployment.   SharePoint WorkSpace.   Monitoring.   Developer Dashboard   Check Page performance.  I also took some screenshots for you from the videos:    Adding a webpart to the page    Central Administration    Layout of the new site    Page Performance  To see more about SharePoint 2010 you should definitely go to the SharePoint Conference 2009. But for know you just have to check the videos.   ","categories": ["SharePoint"],
        "tags": ["Preview","SharePoint 2010"],
        "url": "/2009/06/sharepoint-2010-preview/",
        "teaser": null
      },{
        "title": "Adding profile properties through code",
        "excerpt":" It can occur that you want to add a property to the property collection of the user profile store by using a feature.  If you want to use a feature you would have to create a feature with a FeatureReceiver. In the FeatureReceiver you would have to implement the FeatureActivated and FeatureDeactivating.  The feature.xml file will look like the following:   &lt;Feature  Id=\"b6300381-00fe-446a-b6b0-6962344b5d6f\"            Title=\"UserProfile CV Property\"            Description=\"Feature that adds a user profile property to the user profile store\"            Version=\"12.0.0.0\"            Hidden=\"FALSE\"            Scope=\"Farm\"            Creator=\"Maik van der Gaag\"            ImageUrl=\"motion10/FeaturesIcon.png\"            ImageUrlAltText=\"http://www.motion10.com\"            DefaultResourceFile=\"core\"            ReceiverAssembly=\"Motion10.SharePoint.UserProfileProperties, Version=1.0.0.0, Culture=neutral, PublicKeyToken=7a2aecb48a9c6f26\"            ReceiverClass=\"Motion10.SharePoint.UserProfileProperties.UserProfileProperty\"            xmlns=\"http://schemas.microsoft.com/sharepoint/\"&gt;  &lt;/Feature&gt;  In the FeatureActivated we will add the property when it isn’t already added and in the FeatureDeactivating we will delete the property when it is available in the user profile store.   public override void FeatureActivated(SPFeatureReceiverProperties properties) {     //get the userprofile manager     ServerContext context = ServerContext.Current;     UserProfileManager manager = new UserProfileManager(context);      //getting the property collection to extend     PropertyCollection col = manager.Properties;      if (PropertyExists(col, \"NewProperty\")) {        Property p= col.Create(false);        p.Name = \"NewProperty\";        p.DisplayName = \"NewProperty\";        p.Type = PropertyDataType.String;        p.DefaultPrivacy= Privacy.Public;        p.UserOverridePrivacy = false;        col.Add(p);     }  }   public override void FeatureDeactivating(SPFeatureReceiverProperties properties) {     ServerContext context = ServerContext.Current;     UserProfileManager manager = new UserProfileManager(context);     PropertyCollection col = manager.Properties;      if (PropertyExists(col, \"NewProperty\")) {        col.RemoveByName(PropertyName, false);     }  }   public static bool PropertyExists(PropertyCollection col, string name){  bool exists = false;   var query = from Property p in col                  where p.Name == name                  select p;   if (query.Count() &gt; 0) {     exists = true;  } ","categories": ["Development","SharePoint"],
        "tags": ["Properties","SharePoint","User Profiles"],
        "url": "/2009/06/adding-profile-properties-through-code/",
        "teaser": null
      },{
        "title": "Feature for adding a Icon to your SharePoint farm",
        "excerpt":"On the internet you can find several articles about how to add an icon to the SharePoint farm.. Microsoft also has a KB article that describes how you can accomplish it.  http://support.microsoft.com/default.aspx/kb/837849  The steps that have to be taken to add the a icon are (example with a pdf icon):  Copy the .gif file that you want to use for the icon to the following folder on the server:Drive:\\Program Files\\Common Files\\Microsoft Shared\\Web Server Extensions\\60\\Template\\Images   Edit the Docicon.xml file to include the .pdf file name extension. To do so:a. Start Notepad, and then open the Docicon.xml file. The Docicon.xml file is located in one of the following folders on the server: Drive:\\Program Files\\Common Files\\Microsoft Shared\\Web server extensions\\12\\Template\\Xml b. In the section of the Docicon.xml file, add an entry for the .pdf file name extension. To do so, add the following line, where NameofIconFile is the name of the .gif file:  &lt;Mapping Key=\"pdf\" Value=\"NameofIconFile.gif\" /&gt;  Restart Microsoft Internet Information Services (IIS).  Reading those articles I thought by myself that there must be a way to take these steps without manually editing the files within the 12 hive. To accomplish you have to create a wsp package that exists out of an image and a feature. The feature will have a receiver that kicks off a timer job. The timer job will add or delete the mapping section depending on the activation or deactivation of the feature. The feature.xml will have to look something like this and has to have a scope of farm because you will change a file that is used by the complete farm.:  &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt; &lt;Feature Id=\"5dfefa72-4a9b-4320-af45-03758c755079\"          Title=\"motion10 PDF Integration\"          Description=\"This feature will add the pdf icon to the farm.\"          Version=\"12.0.0.0\"          Hidden=\"FALSE\"          Scope=\"Farm\"          DefaultResourceFile=\"core\"          ReceiverAssembly=\"Motion10.SharePoint.IconIntegration, Version=1.0.0.0, Culture=neutral, PublicKeyToken=d7298385728e744a\" ReceiverClass=\"Motion10.SharePoint.IconIntegration.IconIntegration\" xmlns=\"http://schemas.microsoft.com/sharepoint/\"          Creator=\"Maik van der Gaag\"          ImageUrl=\"motion10/FeaturesIcon.png\"          ImageUrlAltText=\"http://www.motion10.com\"&gt;  &lt;Properties&gt;      &lt;Property Key=\"IconExtension\" Value=\"pdf\"/&gt;       &lt;Property Key=\"IconUrl\" Value=\"pdf_icon.png\"/&gt;  &lt;/Properties&gt;  &lt;/Feature&gt; In the feature.xml file I defined two properties with these properties we create the mapping element in the docicon.xml file. Note: This feature can be used to add all kinds of different icons to the farm. If you would like to add another icon you have to edit the two properties. Like we discussed the feature will have a receiver that kicks of the timer job. To accomplish this we have to implement the FeatureActivated and FeatureDeactivating. On the activation we will start the timer job to add the mapping and on the deactivation we will delete it.   using System;  using Microsoft.SharePoint;  using Microsoft.SharePoint.Administration;   namespace Motion10.SharePoint.IconIntegration  {      class IconIntegration: SPFeatureReceiver      {          /// &lt;summary&gt;          /// The name of the WSS Administration service          /// &lt;/summary&gt;          private static readonly string serviceName = \"WSS_Administration\";           public override void FeatureActivated(SPFeatureReceiverProperties properties)          {              RunJobNow(false, properties);          }           public override void FeatureDeactivating(SPFeatureReceiverProperties properties)          {              RunJobNow(true, properties);          }           public override void FeatureInstalled(SPFeatureReceiverProperties properties)          {          }           public override void FeatureUninstalling(SPFeatureReceiverProperties properties)          {          }           private void RunJobNow(bool delete, SPFeatureReceiverProperties prop)          {             if (prop == null)              {                  throw new ArgumentNullException(\"prop\", \"Argument 'prop' cannot be 'null'\");              }               SPFarm farm = prop.Definition.Farm;               InstallIconTimerJob newJob = null;               foreach (SPService service in farm.Services)              {                  if (service.Name == serviceName)                  {                       newJob = new InstallIconTimerJob(service, prop.Definition.Id, delete);                       SPJobDefinition def = service.GetJobDefinitionByName(newJob.Name);                      if (def != null)                      {                          def.Delete();                      }                       break;                  }              }               newJob.Schedule = new SPOneTimeSchedule(DateTime.Now.AddHours(-2.0));              newJob.Title = string.Format(\"{0} {1} Icon for Feature {2}\", delete ? \"Delete\" : \"Install\", prop.Definition.Properties[\"IconExtension\"], prop.Definition.Id);              newJob.Update();          }      }  } In the RunJobNow() method we create a new instance of our timer job by creating it with two properties. These properties are our feature id and a boolean whether we want to delete the mapping or add it. We also created an extension method on the SPService for retrieving a job definition by name. Here we will retrieve our own job definition if it is available and delete it if it is because we can’t create a new instance of the timer job when it is already available.  using System;  using System.Linq;  using Microsoft.SharePoint.Administration;   namespace Motion10.SharePoint.IconIntegration{      /// &lt;summary&gt;      /// SPService extension methods      /// &lt;/summary&gt;      public static class SPServiceExtensions {           /// &lt;summary&gt;;          /// Gets the jobdefintions by name.          /// &lt;/summary&gt;;          /// &lt;param name=\"service\"&gt;The service&lt;/param&gt;;          /// &lt;param name=\"name\"&gt;The name.&lt;/param&gt;          /// &lt;returns&gt;SPJobDefinition&lt;/returns&gt;;          /// &lt;exception cref=\"System.ArgumentNullException\"&gt;Exception is thrown when the service or name equal null&lt;/exception&gt;          public static SPJobDefinition GetJobDefinitionByName(this SPService service, string name) {              if (service == null) {                  throw new ArgumentNullException(\"service\", \"Argument 'service' cannot be 'null'\");              }               if (String.IsNullOrEmpty(name)) {                  throw new ArgumentNullException(&amp;quot;name&amp;quot;, &amp;quot;Argument 'name' cannot be 'null' or 'String.Empty'&amp;quot;);              }               var query = from SPJobDefinition job in service.JobDefinitions                          where job.Name == name                          select job;               return query.FirstOrDefault();          }      }  } Now that the feature is finished we can create the timer job (if you want to read more about the creation of a timer job you can read the following article http://msdn.microsoft.com/en-us/library/cc406686.aspx). The timer job will have five properties.     private bool _delete          {              get              {                  if (this.Properties.ContainsKey(deleteKey))                  {                      return Convert.ToBoolean(this.Properties[deleteKey]);                  }                  else                  {                      return false;                  }              }              set              {                  if (this.Properties.ContainsKey(deleteKey))                  {                      this.Properties[deleteKey] = value.ToString();                  }                  else                  {                      this.Properties.Add(deleteKey, value.ToString());                  }              }          }           private Guid _featureID          {              get              {                  if (this.Properties.ContainsKey(featureKey))                  {                      return new Guid(this.Properties[featureKey].ToString());                  }                  else                  {                      return Guid.Empty;                  }              }              set              {                  if (this.Properties.ContainsKey(featureKey))                  {                      this.Properties[featureKey] = value.ToString();                  }                  else                  {                      this.Properties.Add(featureKey, value.ToString());                  }              }           }           private string IconExtension          {              get              {                  SPFeatureDefinition def = Farm.FeatureDefinitions[_featureID];                  return def.Properties[\"IconExtension\"].Value;              }          }           private string IconUrl          {              get              {                  SPFeatureDefinition def = Farm.FeatureDefinitions[_featureID];                  return def.Properties[\"IconUrl\"].Value;              }          }           public string SPDocIconFile          {              get              {                  return SPUtility.GetGenericSetupPath(filePath);              }          } We save the value of the _featureid and _delete property in the property bag of our timer job because we have to access the properties after we created a new instance of the timer job. The other properties will retrieve the attributes for the mapping from the feature definition based on the feature id we saved in the property bag. The SPDocIconFile will create the complete path to the file we need.  using System;  using System.IO;  using System.Linq;  using System.Xml.Linq;  using Microsoft.SharePoint.Administration;  using Microsoft.SharePoint.Utilities;  namespace Motion10.SharePoint.IconIntegration  {      public class InstallIconTimerJob : SPJobDefinition      {          /// &lt;summary&gt;          /// Filepath for the spthemes.xml file          /// &lt;/summary&gt;          private readonly string filePath = \"TEMPLATE\\\\XML\\\\DOCICON.XML\";          /// &lt;summary&gt;          /// The property key for the delete property          /// &lt;/summary&gt;          private readonly string deleteKey = \"Icon_Installation_Deletekey\";          /// &lt;summary&gt;          /// The property key for the feature property          /// &lt;/summary&gt;          private readonly string featureKey = \"Icon_Installation_Featurekey\";          private bool _delete          {              get              {                  if (this.Properties.ContainsKey(deleteKey))                  {                      return Convert.ToBoolean(this.Properties[deleteKey]);                  }                  else                  {                      return false;                  }              }              set              {                  if (this.Properties.ContainsKey(deleteKey))                  {                      this.Properties[deleteKey] = value.ToString();                  }                  else                  {                      this.Properties.Add(deleteKey, value.ToString());                  }              }          }          private Guid _featureID          {              get              {                  if (this.Properties.ContainsKey(featureKey))                  {                      return new Guid(this.Properties[featureKey].ToString());                  }                  else                  {                      return Guid.Empty;                  }              }              set              {                  if (this.Properties.ContainsKey(featureKey))                  {                      this.Properties[featureKey] = value.ToString();                  }                  else                  {                      this.Properties.Add(featureKey, value.ToString());                  }              }          }          private string IconExtension          {              get              {                  SPFeatureDefinition def = Farm.FeatureDefinitions[_featureID];                  return def.Properties[\"IconExtension\"].Value;              }          }          private string IconUrl          {              get              {                  SPFeatureDefinition def = Farm.FeatureDefinitions[_featureID];                  return def.Properties[\"IconUrl\"].Value;              }          }          public string SPDocIconFile          {              get              {                  return SPUtility.GetGenericSetupPath(filePath);              }          }          public InstallIconTimerJob()              : base()          {          }          public InstallIconTimerJob(SPService service, Guid featureID, bool delete)              : base(\"Icon Installer\", service, null, SPJobLockType.None)          {              this._delete = delete;              this._featureID = featureID;          }          public override void Execute(Guid targetInstanceId)          {              base.Execute(targetInstanceId);              this.ChangeDocIconFile();          }          private void ChangeDocIconFile()          {              string spThemesContent = string.Empty;              if (File.Exists(SPDocIconFile))              {                  XDocument doc = XDocument.Load(SPDocIconFile);                  var element = from b in doc.Element(\"DocIcons\").Element(\"ByExtension\").Elements(\"Mapping\")                                where b.Attribute(\"Key\").Value.ToLower() == this.IconExtension.ToLower()                                select b;                  bool containsElement = (element != null &amp;&amp; element.Count() &gt; 0);                  if (_delete)                  {                      if (containsElement)                      {                          element.Remove();                          doc.Save(SPDocIconFile);                      } ","categories": ["Development","SharePoint"],
        "tags": ["Icon","SharePoint"],
        "url": "/2009/06/feature-for-adding-a-icon-to-your-sharepoint-farm/",
        "teaser": null
      },{
        "title": "Configure Kerberos authentication",
        "excerpt":" A few weeks ago I tried to set up a SharePoint farm that uses Kerberos authentication. I always thought it was just a setting in SharePoint but infect it is much more than that.   What is Kerberos    Kerberos is a computer network authentication protocol, which allows nodes to communicate over a non secure network to prove their identity to one another.   Kerberos makes use of a trusted third party for the authentication, termed a Key Distribution Center (KDC) which consists of two parts: an Authentication Server (AS) and a Ticket Granting Server (TGS). Kerberos works on the basis of tickets which serve to prove the identity.   For communication between two entities, the KDC generates a session key which they can use to secure their interaction. A KDC runs on every domain controller as a function of the Active Directory Domain Services (AD KS).   Why use Kerberos    There are many reasons why u should use Kerberos authentication rather than the default NTLM. The main reason is because it is more secure than NTML. Besides this reason you should also use it to get around the “double hop” (http://support.microsoft.com/kb/329986) authentication issue. The most common scenarios for the double hop issue with SharePoint are around Excel services and Data Connection.   Setting up Kerberos authentication    To make use of Kerberos nothing needs to be changed if the domain controller is set-up correctly. Kerberos clients are configured to request ticket-granting tickets (TGT’s) of the Kerberos Key Distribution Center automatically. If the ticket is received successfully the Kerberos client saves the ticket on the locale machine.   One of the first things you should consider when you want to use Kerberos authentication is which accounts you will be using for your services and web applications. Those accounts will have to get a Service Principal Name. The servers you will be using will have to allow delegation.   To trust a computer for delegation     Click Start, point to Program Files, point to Administrative Tools, and then click Active Directory Users and Computers.   Navigate to the computer account that needs to be set, for example, Active Directory Users and Computers/&lt;domain name&gt;/Computers/&lt;computer name&gt;.   Double-click the computer name in the list on the right pane. On the Account tab, select the Account is trusted for delegation check box and then click OK.  If every server in the farm is trusted for delegation you can start creating SPN’s (Service Principal Names) for the accounts you will be going to use for SQL Server and the SharePoint farm.   Service Principal Names    Service principal names are associated with the security principal (user or groups) in whose security context the service executes. Service Principal Names are created with a specific Syntax.   Service Principal Name syntax:    The basic syntax of service principal name is as follows:  [service type]/[instance name]:[port number]/[service name]  The elements of the syntax have the following meaning:   service type: Type of service, such as “http” for the http protocol.   instance name: Name of the instance of the service. Depending upon the service type, it is either the name or IP address of the host running the service.   port number: Number of the port used by the service on the host, if it is different from the default for the service type.   service name: Name of the service. The name can be the DNS name of a host, of a replicated service, or of a domain; or it can be the distinguished name of a service connection point object or of an RPC service object.  Note: When you use Kerberos authentication with SharePoint you have to know that Internet Explorer does not send the port number with the service principal name. For example for this address https://intra.motion10.com:443 the SPN will be http/intra.motion10.com.    How to create a SPN in Active Directory    To create a SPN for an account you have to follow the following steps.   Create the SPNs for your SQL Server service account   Log on to your Active Directory domain controller using the credentials of a user that has domain administrative permissions.   In the Run dialog box, type ADSIEDIT.MSC.   In the management console dialog box, expand the domain container folder.   Expand the container folder containing user accounts, for example CN=Users.   Locate the container for the SQL Server Service account, for example CN=mosssqlsvc.   Right-click this account, and then click Properties.   Scroll down the list of properties in the SQL Server Service account dialog box until you find servicePrincipalName.   Select the servicePrincipalName property and click Edit.   In the Value to Add field, in the Multi-Valued String Editor dialog box, type the SPN for example MSSQLSvc/sqlserver:1433 and click Add.   Click OK on the Multi-Valued String Editor dialog box, and then click OK on the properties dialog box for the SQL Server service account.  Important: You cannot give two accounts the same Service Principal Name. This will cause the authentication to fail or fall back on NTLM if it is possible.    After you created the Service Principal Names you have to trust the accounts for delegation:   Trust accounts for Delegation    Windows server 2003 Active Directory     On the Domain controller, click Start, point to Programs, point to Administrative Tools, and click Active Directory Users and Computers.   Under your domain, click the Users folder.   Under your user account, click Properties.   Navigate to the “Delegation” tab.   Choose “Trust this user/computer for delegation to any service (Kerberos)“.   Windows server 2000 Active Directory     On the Domain controller, click Start, point to Programs, point to Administrative Tools, and click Active Directory Users and Computers.   Under your domain, click the Users folder.   Under your user account, click Properties.   On the Account tab, select the Trusted for delegation check box.   Under the account that you are trying to delegate, clear the Account is sensitive cannot be delegated check box.  Note: If you do not have the delegation tab in the user properties you will have to use the steps of the Windows server 2000 Active Directory. This is because Windows 2003 Active Directory can run a in a 2000 mode.    Configure Kerberos for SQL server    Kerberos for SQL server has to be configured before you can install SharePoint Server 2007. This means we have to create a SPN for the SQL service account:    Service type: MSSQLSvc   Instance name: SQLSERVER / SQLSERVER.mydomain.com   Port number: 1433 (default SQL communication port)   The SPN’s will be:    MSSQLSvc\\SQLServer:1433   MSSQLSvc\\SQLServer.mydomain.com:1433   Testing the Kerberos authentication for SQL Server    If you have configured the SPN’s for the SQL service account you can test if it works by following the following steps.    Run SQL Server Management Studio in another server in the domain.   Connect to you SQL Server.   Check if Kerberos authentication is used by running the event viewer on your SQL host server and examine the Security log. In this log you should have a Success Audit that has used the Kerberos protocol.   If there isn’t a Success Audit that uses the Kerberos protocol you can check the following:    Is the SPN in active directory correct?   Do I have two accounts in Active Directory that have the same SPN?   Configure Kerberos for your Web Applications    If you want your web applications to make use of Kerberos you have to create Service Principal Names for the accounts that will used for running the web applications.   If have written down a few examples of SPN’s of several addresses of web applications.      Internet address Service Principal Name   http://intra.motion10.com http/intra.motion10.com   https://intra.motion10.com http/intra.motion10.com   http://mossadmin:43433 http/mossadmin   http://mysites.intranet.com http/mysites.intranet.com      Testing the Kerberos authentication for the web application authentications    If you have created SharePoint web applications that use Kerberos authentication, you are ready to test your configuration by following the following steps:    Start internet explorer and navigate to the web application that has Kerberos authentication enables and login.   On the SharePoint server open the event viewer and examine the Security log. In this log you should have a Success Audit that has used the Kerberos protocol.   Configure the SSP Infrastructure for Kerberos    For the SSP infrastructure to use Kerberos you have to follow the following steps:    Register a new SPN for the SSP Service.   Configure the SSP infrastructure to use Kerberos   Register a new SPN for the SSP Service    For the SSP infrastructure to use Kerberos a few new SPN’s need to be created. This is because the SSP infrastructure runs on every server in the SharePoint farm. It is also bound to the following ports TCP 56737 en TCP 56738 and the SPN also needs to include the name of the Shared Service Provider.   So you have to create a separate SPN for each server in the farm. In the following table you will see a few examples.       Computer Name:    MOSSQUERY   Name Shared Service Provider    SSP    MSSP/MOSSQUERY:56737/SSP   MSSP/MOSSQUERY:56738/SSP      Computer Name:    MOSSQUERY   Name Shared Service Provider    SharedServices4    MSSP/MOSSQUERY:56737/SharedServices4   MSSP/MOSSQUERY:56738/SharedServices4      Computer Name:    MOSSCRAWL   Name Shared Service Provider    SharedServices1    MSSP/ MOSSCRAWL:56737/ SharedServices1   MSSP/ MOSSCRAWL:56738/SharedServices1      &nbsp;  Configure the SSP infrastructure to use Kerberos    After you created the Service Principal Names for the SSP Infrastructure you have to inform the SSP infrastructure to use Kerberos. You can inform the SSP infrastructure by using the stsadm tool.   To configure your SSP infrastructure to use Kerberos authentication, perform the following procedure:   Log on to your Active Directory domain controller using the credentials of a user that has domain administrative permissions.   On one of your servers running Office SharePoint Server 2007, open a command prompt.   Change to the following directory: %COMMONPROGRAMFILES%\\microsoft shared\\web server extensions\\12\\bin.   Type the following command: stsadm –o setsharedwebserviceauthn –negotiate, and then press ENTER.  After following all the steps you successfully configured your SharePoint farm to use Kerberos authentication.   ","categories": ["Administration"],
        "tags": ["KDC","Kerberos","SharePoint","SPN"],
        "url": "/2009/08/configure-kerberos-authentication/",
        "teaser": null
      },{
        "title": "Fully certified SharePoint Specialist",
        "excerpt":" It took me long enough to get all off the WSS and MOSS certificates but today I finally passed the last exam (Configuring Windows SharePoint Services 3.0 : 70-631) and I have to say that that is one of the hardest exams for a developer. Know up to .NET Framework 3.5, Windows Workflow Foundation Applications (70-504).     ","categories": ["General"],
        "tags": ["Certified","SharePoint"],
        "url": "/2009/08/fully-certified-sharepoint-specialist/",
        "teaser": null
      },{
        "title": "Team Foundation Server Installation Error: 28938",
        "excerpt":" For the last couple of days my colleague and I have been installing Team Foundation Server 2008 on Windows Server 2008 with a separate SQL 2008 and SharePoint 2007 server.  The errors pops up like nothing I have ever seen before. One of the most annoying errors we came across was this one:  “Error:28938.TFSServerStatusValidator: The tool could not call the Team Foundation Server Registration Web Service. The call failed with the following status: 401 HTTP Unauthorized. Verify that Internet Information Services (IIS), Windows SharePoint Services, and ASP.Net are configured correctly and that IIS allows ASP.Net 2.0 Web service extensions.”  After days of research (even using Network Monitoring) to figure out what was going wrong we stumbled on something really small. When we installed IIS on the server it did not automatically install the Windows Authentication Role service as part of IIS. Because it did not install this service you cannot login. After the installation of it the Team Foundation Server Installation went like a charm.  If you have already installed IIS you have to do the following to install the Windows Authentication Feature:   Press start on the server were you would like to install the feature.   Administrative tools – Server manager   On the left side of the screen select roles and scroll down to Web Server IIS   Select add Role Services   In the setup screen select windows authentication and press next.   On the confirmation screen select install.   ","categories": ["DevOps"],
        "tags": ["28938","Error","TFS"],
        "url": "/2009/08/team-foundation-server-installation-error-28938/",
        "teaser": null
      },{
        "title": "Login to Team Foundation Server 2008 SP1 with another account / from another domain",
        "excerpt":" A few days ago I and my colleague Wesley had a lot of problems installing of Team Foundation Server on Windows Server 2008 with a Separate SQL Server 2008 and SharePoint Farm. After a couple days of hard work we finally got it running.  When we completed the installation we began testing it on our local systems. On our local system everything went fine but when we tried it on a Virtual Machine that we use for developing solution for SharePoint we could not connect to the Team Foundation Server.   The problem here was that the Virtual Machine has its own domain so the account that you are logged in with could not authenticate to the TFS server.  To get around this you have to specify a network account by following the following steps (Windows Server 2008):  1: Start – Control Panel  2: In the Control Panel select User Accounts.  3: On the left side of the screen select Manage your network passwords.    4: Click add and insert the right information.    5: Click “Ok” when you are done and on the other screen select “close”  Try connecting to the Team Foundation Server again. Now everything should work perfectly.   ","categories": ["DevOps"],
        "tags": ["2008","SP1","TFS"],
        "url": "/2009/08/login-to-team-foundation-server-2008-sp1-with-another-account-from-another-domain/",
        "teaser": null
      },{
        "title": "Microsoft Office SharePoint Server 2007 on Windows Server 2008 R2",
        "excerpt":" A few days ago I started setting up an internal development environment for SharePoint. This environment needed to exist of a Domain server, Database server and of course a SharePoint Server.  The R2 of Windows Server 2008 was just released so I thought lets install everything on the R2 version. All went fine until I begun with the installation of SharePoint 2007.  The installation told me that the version that I was installing was not compatible with the server version. This error was shown because you van only install SharePoint 2007 SP2 on the R2 version of windows server 2009. We had no installation disk of SP2 (I don’t even think one exists, correct me if I’m wrong), so we had to merge the current installation (SharePoint 2007 SP1) with the updates of SP2.  Follow these steps to accomplish this:   Copy the installation folder from your installation disc to your local hard drive.   Download Microsoft Office SharePoint Server 2007 SP2 and the Windows SharePoint Services SP2.   Extract the downloaded updates files to the installation folder you copied from your disc. You have to make sure you extract the files to the upgrades folder. By doing the following:  [update filename] /extract:”[upgrade folder location of the folder you copied from the disc]“   For example:Wssv3sp2-kb953338-x64-fullfile-en-us.exe /extract:C:\\Original\\Updates   Make sure you extract both the updates (Download Microsoft Office SharePoint Server 2007 SP2 and the Windows SharePoint Services SP2).   When you have extracted the update files remove the “Wsssetup.dll” from the updates folder  Know you are ready to install Download Microsoft Office SharePoint Server 2007 SP2 on Windows Server 2008 R2 from the folder on your hard disk. For more information you can always read the TechNet article ( http://technet.microsoft.com/en-us/library/cc261890.aspx).   ","categories": ["Administration"],
        "tags": ["SharePoint 2007","Windows Server 2008 R2"],
        "url": "/2009/09/microsoft-office-sharepoint-server-2007-on-windows-server-2008-r2/",
        "teaser": null
      },{
        "title": "Hiding the &quot;New Site&quot; option in the Site Actions menu - SharePoint 2010",
        "excerpt":" A long time ago I wrote an article about hiding the create site action within the site actions menu for SharePoint 2007. Within SharePoint 2010 you hide the custom actions the same way. Only finding the Id’s is a little bit different. In the preceding article I stated that you can find the id’s within the source of the page. This is still true but some of them can only be found within the Master Page. The table below display’s all custom actions with the Id’s of the actions.   &nbsp;     Custom action ID   Edit Page MenuItem_EditPage   Sync to SharePoint Workspace MenuItem_TakeOffline   New Page MenuItem_CreatePage   New DocumentLibrary MenuItem_CreateDocLib   New Site MenuItem_CreateSite   More Options MenuItem_Create   View All Site Content MenuItem_ViewAllSiteContents   Edit in SharePoint Designer MenuItem_EditSite   Site Permissions MenuItem_SitePermissions   Site Settings MenuItem_Settings     ","categories": ["Development","SharePoint"],
        "tags": ["Custom Action","SharePoint"],
        "url": "/2009/09/hiding-the-new-site-option-in-the-site-actions-menu-sharepoint-2010/",
        "teaser": null
      },{
        "title": "Using Windows Server as a Desktop Operating System",
        "excerpt":" On this planet there are a lot of geeks (Including me) that run Windows Server 2008 (R2) as their desktop operating system.  I installed my development laptop with a dual boot of Windows 7 and Windows Server 2008 R2. The Windows Server 2008 R2 install is really a virtual hard disk that I setup as a boot device.  If there are some developers who would like to setup a Virtual hard disk as boot device I would recommend the following article:   Boot from VHD First Impression  I recommend an installation of a virtual hard disk on a hyper-v server. After the installation you can run ‘sysprep’ with the option ‘shutdown’ and copy the virtual hard disk to your computer. After that you can set it up as a boot device as explained in the article above.  After you have followed the steps (or installed Windows Server) you have a server environment as your desktop operation system, but it still isn’t like a Windows 7 or Windows Vista environment. If you would like your windows server installation to look like windows Vista or Windows 7 you have to install the ‘Desktop Experience’ feature.  The ‘Desktop experience’ feature includes programs like:   Windows Media Player   Windows Aero and other desktop themes   Video for Windows (AVI support)   Windows Defender  To install the ‘Desktop experience’ follow the following steps:   Open Server Manager: click Start, point to Administrative Tools, and click Server Manager.   In the Features Summary section, click Add features.   Select the Desktop Experience check box, and then click ‘Next’ and then click ‘Install’.  Now you are ready to get a real desktop experience from Windows Server, but the Aero theme still isn’t working. To get this working you will have to activate the “Themes” services from windows and set the start-up type to automatic.    Click Start, click Run, type in services.msc, and then click OK.   In the list of installed services, right click Themes, and then click Properties.   In the start-up section click automatic, click apply and the click Start. And then click OK to close the window.  After this configuration change your desktop needs to be restarted. After the reboot you can change the windows theme. By right clicking on the desktop and choosing ‘Personalize’ and selecting the Windows 7 Theme in case of windows server 2008 R2.   ","categories": ["Administration"],
        "tags": ["Desktop","Windows Server 2008 R2"],
        "url": "/2009/10/using-windows-server-as-a-desktop-operating-system/",
        "teaser": null
      },{
        "title": "Prompting for Credentials",
        "excerpt":"Often people ask me a question why Windows keeps prompting to login when you open a document from SharePoint. To automatically login you can change the settings that I have written down in this blog post: SharePoint prompts to login multiple time when opening a document If you still have problems with windows that keeps prompting for credentials you should not stop the WebClient service like many blogs give as solution. Stopping this service is a resolution for this problem but not the correct solution. When you stop this service other problems can occur like:  When you open a document from SharePoint and press “Save” the save dialog will not open with the document library but with a local folder.  But what is the resolution I hear you thinking! The resolution for vista is a registry key setting.  Go to start – run, and type regedit.   When the registry editor window opens go to this path:  HKEY_LOCAL_MACHINE – SYSTEM – CurrentControlSet – Services – WebClient – Parameters.   Right-click the ‘Parameters’ subkey and create a new Multi-String Value.   Give the Multi-String Value the name ‘AuthForwardServerList’.   Open the new value and type in the list of SharePoint server URL’s that are trusted and click ‘Ok’   Close the registry editor.   Reboot your PC or restart the WebClient Service : start – Administrative Tools - Services  The steps I have written down should work on a PC with Windows Vista SP1. If you guys have found another solution please let me know by placing a comment.   ","categories": ["Administration","SharePoint"],
        "tags": ["Credentials","Login","Prompts"],
        "url": "/2009/10/prompting-for-credentials/",
        "teaser": null
      },{
        "title": "ASP.NET Checkbox with confirmation message",
        "excerpt":" For one of my assignments I had to create a checkbox that displays a confirmation message when you uncheck a checkbox. I thought I would share my solution with you guys.  &nbsp;  First off al you will have to create a new class file that we will call ‘ConfirmationCheckbox’ the class will inherit from ‘System.Web.UI.WebControls.Checkbox’ (the normal checkbox). For our checkbox you have to override the ‘OnPreRender’ to add JavaScript to the page and the ‘AddAttributesToRender’ to add the onclick to the checkbox for the notification. Besides that you have to create a property for the confirmation text.  using System;  using System.Collections.Generic;  using System.Linq;  using System.Text;  using System.Web.UI.WebControls;  using System.Web.UI;  using System.Globalization;   namespace Motion10.SharePoint.WebControls {      public class ConfirmationCheckBox : CheckBox {          public string ConfirmationText { get; set; }          protected override void OnPreRender(EventArgs e) {              base.OnPreRender(e);              this.Page.ClientScript.RegisterClientScriptResource(typeof(ConfirmationCheckBox), \"Motion10.SharePoint.JavaScript.js\");          }           protected override void AddAttributesToRender(HtmlTextWriter writer) {              this.Attributes[\"onclick\"] += string.Format(CultureInfo.CurrentCulture, \"Confirmation(this, '{0}');\", this.ConfirmationText);              base.AddAttributesToRender(writer);         }     } } In the code sample above you see the overridden methods, in the ‘AddAttributesToRender’ we add a call to the JavaScript method to the attribute ‘onclick’. The JavaScript that we will create later on in this article needs two parameters the object in this case the checkbox (this) and the text for the confirmation (this.ConfirmationText).  The JavaScript will be added to the page in the OnPreRender method of the CheckBox. The JavaScript is a embedded resource in our dll. For that reason you have to use “Motion10.SharePoint.Notification.JavaScript.js”.  If you want to create a JavaScript file as embedded resource you have to do the following:   Create a JavaScript file within your project.  Right click on the file and choose properties.  In the properties window set the Build Action to embedded resource.  After you have done this you can add the Resource to the ‘AssemblyInfo.cs’ file under the properties like this:  [assembly: WebResource(\"Motion10.SharePoint.JavaScript.js\", \"text/javascript\")] Finally you will have to point to the file by using the full assembly name followed by the filename. If you place the file in a folder you will also have to add the folder to the name.   &nbsp;  Know we have got this out off the way we still have to create the JavaScript for the confirmation message.   function Confirmation(el, message) {     if (!el.checked) {        el.checked = !confirm(message);     } } As you can see we have one method in the JavaScript file called Confirmation that needs to parameters. The ‘el’ stands for the checkbox and ‘message’ for the message that needs to be displayed. In the method we check if the checkbox is unchecked because we only want to display a notification when the checkbox is unchecked.    ","categories": ["Development"],
        "tags": ["ASP.net","Checkbox","Javascript"],
        "url": "/2009/10/asp-net-checkbox-with-confirmation-message/",
        "teaser": null
      },{
        "title": "Sitemap Provider for Community Server 2.1",
        "excerpt":" For my new blog I had to create a sitemap provider to provide my content to several search engines.  Because I did not find any solution on the internet I thought I would share my solution with you guys:  You can download the source here: MSFTPlayground.Sitemap.zip   The provider is meant for one blog. In the definitions file within the solution you can change the properties for you blog. You only have to add a httphandler to your web.config just like it is done in the file “web.config(extension)” within the solution.   Let me know what you guys think of this provider.   ","categories": ["Development"],
        "tags": ["Community Server","SEO","Sitemap"],
        "url": "/2009/10/sitemap-provider-for-community-server-2-1/",
        "teaser": null
      },{
        "title": "So long \"Blog About SharePoint\" and Hello &quot;Microsoft Playground&quot;",
        "excerpt":" Almost two years ago I started \"Blog About SharePoint\" to blog about SharePoint related subjects.  The last couple of months my focus has changed to more Microsoft products so I have created a new blog called \"Microsoft Playground\". Microsoft Playground will be a blog for articles regarding all kind of Microsoft technologies. I hope you guys will enjoy \"Microsoft Playground\" as much as you have enjoyed \"Blog About SharePoint\". RSS feed : Microsoft Playground   ","categories": ["General"],
        "tags": ["Microsoft Playground","SharePoint"],
        "url": "/2009/10/so-long-blog-about-sharepoint-and-hello-microsoft-playground/",
        "teaser": null
      },{
        "title": "SharePoint 2010: Getting Started with Development &ndash; Part 2",
        "excerpt":" More and more resources are coming available for starting development with SharePoint 2010. Today I received a link from MSDN Flash with some cool new developer resources. After watching the presentations you can even test your skill by answering a question. Besides that their are also code samples available for each module.  Get Started Developing on SharePoint 2010   Module 1: Getting Started: Building Web Parts in SharePoint 2010   Module 2: What Developers Need to Know About SharePoint 2010   Module 3: Building Blocks for Web Part Development in SharePoint 2010   Module 4: Accessing SharePoint 2010 Data and Objects with Server-Side APIs   Module 5: Accessing SharePoint 2010 Data and Objects with Client-Side APIs   Module 6: Accessing External Data with Business Connectivity Services in SharePoint 2010   Module 7: Developing Business Processes with SharePoint 2010 Workflows   Module 8: Creating Silverlight User Interfaces for SharePoint 2010 Solutions   Module 9: Sandboxed Solutions for Web Parts in SharePoint 2010   Module 10: Creating Dialog Boxes and Ribbon Controls for SharePoint 2010  Besides the getting started modules Microsoft also released resources to get started working with Visual Studio 2010 and SharePoint 2010:  SharePoint Development in Visual Studio 2010  You should also take a look at http://channel9.msdn.com/learn/courses/SharePoint2010Developer/. On this site there are several great movies to learn how to develop for SharePoint 2010.   ","categories": ["Development","Documentation","SharePoint"],
        "tags": ["Documentation"],
        "url": "/2009/11/sharepoint-2010-getting-started-with-development-part-2/",
        "teaser": null
      },{
        "title": "Deploying multiple wsp packages with one installation file",
        "excerpt":" Today the public beta of SharePoint 2010 is released and everyone started blogging about SharePoint 2010. I am still preparing a Virtual Machine because the beta does not fully works on a Windows Server 2008 R2 installation and I had prepared a virtual machine with this operating system. But know back to deploying multiple wsp packages.  At the moment I’m working for a client that want to install multiple wsp packages and keep it simple for administrators to change the deployment. To accomplish this I created a sample configuration file for a custom installation.  &lt;?xml version=\"1.0\" encoding=\"utf-8\" ?&gt; &lt;Solution&gt;     &lt;Package file=\"Solutions\\CustomPackage2.wsp\" deployment=\"Globally\"&gt;&lt;/Package&gt;     &lt;Package file=\"Solutions\\CustomPackage1.wsp\" deployment=\"WebApplication\"&gt;        &lt;WebApplication url=\"http://myportal.com\" /&gt;     &lt;/Package&gt; &lt;/Solution&gt; In the configuration file you can see that there are two packages. One of the packages has to be deployed globally and 1 of them for several web applications that is why the second packages has child elements.  Know that we have a sample configuration file we want we to start building a application to install those packages. First of all we will try to read out the xml file in a console application.  class Program  {     public const string SolutionElement = \"Solution\";     public const string PackageElement = \"Package\";     public const string FileAttribute = \"file\";     public const string DeploymentAttribute = \"deployment\";      public static void Main()     {         try        {           if (File.Exists(\"Installation.config\"))           {              //get the installation document              WriteSectionLine(\"Loading installation configuration file.....\");              var config = XDocument.Load(\"Installation.config\");               var packages = from XElement package in config.Element(SolutionElement).Elements(PackageElement)                             select package;               foreach (XElement element in packages)              {                 ProcessPackage(element);              }           }           else           {              Console.WriteLine(\"Configuration file 'Installation.config' is nog in the same directory as the solution installer.\");           }        }     catch (Exception exception)     {        Console.WriteLine(\"An exception ocurred during the installation\");        Console.WriteLine(\"-------------------------------------------------------------------\");        Console.WriteLine(exception.Message);        Console.WriteLine(\"-------------------------------------------------------------------\");     }     WriteSectionLine(\"Press any key to close.\");     Console.ReadKey();  } In the ‘Main’ of the console application we check if the configuration file exists and when it exists we load the file and run a Linq query for the package elements. When we have package elements within the configuration file we run the method Process Package for each package element.  private static void ProcessPackage(XElement element) {     var deployment = element.Attribute(DeploymentAttribute).Value;     if (deployment.ToUpperInvariant() == \"GLOBALLY\")     {        DeploymentGlobally(element);     }     else     {        DeploymenWebApplication(element);     } } In the process package we check the deployment attribute to check how to deploy the package, globally or for web applications.  private static void DeploymenWebApplication(XElement element)  {     var file = element.Attribute(FileAttribute).Value;     var fileName = Path.GetFileName(file);      WriteSectionLine(string.Format(CultureInfo.InvariantCulture, \"Processing Global Solution: {0}\", fileName));     var query = from childElement in element.Elements(\"WebApplication\")                 select childElement;      //delete / retract solution     foreach (XElement el in query)     {       var url = el.Attribute(\"url\").Value;       Console.WriteLine(Processes.RunStsAdmProcces(\"retractsolution\", string.Format(CultureInfo.InvariantCulture, \"-name {0} -url {1} -immediate\", fileName, url)));       Console.WriteLine(Processes.RunStsAdmProcces(\"execadmsvcjobs\", string.Empty));     }      Console.WriteLine(Processes.RunStsAdmProcces(\"deletesolution\", string.Format(CultureInfo.InvariantCulture, \"-name {0} -override\", fileName)));     Console.WriteLine(Processes.RunStsAdmProcces(\"addsolution\", string.Format(CultureInfo.InvariantCulture, \"-filename {0}\", file)));      foreach (XElement el in query)     {        var url = el.Attribute(\"url\").Value;        Console.WriteLine(Processes.RunStsAdmProcces(\"deploysolution\", string.Format(CultureInfo.InvariantCulture, \"-name {0} -url {1} -allowgacdeployment -immediate\", fileName, url)));        Console.WriteLine(Processes.RunStsAdmProcces(\"execadmsvcjobs\", string.Empty));     }  }  private static void DeploymentGlobally(XElement element) {     var file = element.Attribute(FileAttribute).Value;     var fileName = Path.GetFileName(file);      WriteSectionLine(string.Format(CultureInfo.InvariantCulture, \"Processing Global Solution: {0}\", fileName));      //preform processes     Console.WriteLine(Processes.RunStsAdmProcces(\"retractsolution\", string.Format(CultureInfo.InvariantCulture, \"-name {0} -immediate\", fileName)));     Console.WriteLine(Processes.RunStsAdmProcces(\"execadmsvcjobs\", string.Empty));     Console.WriteLine(Processes.RunStsAdmProcces(\"deletesolution\", string.Format(CultureInfo.InvariantCulture, \"-name {0} -override\", fileName)));     Console.WriteLine(Processes.RunStsAdmProcces(\"addsolution\", string.Format(CultureInfo.InvariantCulture, \"-filename {0}\", file)));     Console.WriteLine(Processes.RunStsAdmProcces(\"deploysolution\", string.Format(CultureInfo.InvariantCulture, \"-name {0} -allowgacdeployment -immediate\", fileName)));     Console.WriteLine(Processes.RunStsAdmProcces(\"execadmsvcjobs\", string.Empty));  } In the methods we read out additional information from the package element. In the ‘DeploymentWebApplication’ we read out the child elements and retract and delete the solution for each web application before we start installing the solution. Each time we retract or deploy a package we also run the execadmsvcjobs because we have to make sure no action for a package is running at the moment we start a new action.  The actions we do are started by the method RunStsAdmProcess this methods starts a new process that runs the stsadm tool of SharePoint. The methods needs two parameters the command and the argument.  internal static string RunStsAdmProcces(string command, string arguments)  {     var retVal = string.Empty;     var commonFilesPath = System.Environment.GetFolderPath(System.Environment.SpecialFolder.CommonProgramFiles);      using (var process = new Process())     {        //create the start info of the project        var processStartInfo = new ProcessStartInfo(command);        processStartInfo.UseShellExecute = false;        processStartInfo.RedirectStandardOutput = true;        processStartInfo.FileName = commonFilesPath + @\"\\Microsoft Shared\\web server extensions\\12\\BIN\\stsadm.exe\";        processStartInfo.Arguments = string.Format(CultureInfo.InvariantCulture, \" -o {0} {1}\", command, arguments);         //initialize the process ","categories": ["Administration","SharePoint"],
        "tags": ["Deployment","WSP"],
        "url": "/2009/11/deploying-multiple-wsp-packages-with-one-installation-file/",
        "teaser": null
      },{
        "title": "SharePoint 2010: Getting Started with Development",
        "excerpt":" When I was reading and searching information about SharePoint 2010 on the internet I found I really helpful page in the Microsoft download&nbsp; to get started with development for SharePoint 2010.  On the page a 10 Hands on Labs for you to try out in C# and Visual Basic.  Overview  HOL01 - Developing a Visual Web Part in Visual Studio 2010  This hands-on lab introduces the Visual Studio 2010 SharePoint development environment. It shows how to build a Visual Web Part using LINQ to SharePoint, and how to connect one Web Part to another Web Part on the page.   HOL02 - Developing a List Definition and Event Receiver in Visual Studio 2010  This hands-on lab walks you through building a list definition for SharePoint 2010 in Visual Studio 2010. It also shows how to build an event receiver for the list in Visual Studio 2010 and deploy it to SharePoint. After the list and event receiver are deployed, you can use the developer dashboard to evaluate the performance of the event receiver.   HOL03 - Developing Advanced Web Parts for SharePoint 2010 with Visual Studio 2010  This hands-on lab shows how to build a Web Part using several SharePoint-specific controls in Visual Studio 2010. Investigate advanced built-in Web Parts, including the Data View Web Part.   HOL04 - Developing with LINQ to SharePoint in Visual Studio 2010  This hands-on lab explores a variety of LINQ queries on SharePoint 2010, going into more depth than the introductory hands-on lab. It also walks you through an exercise of creating a custom content type in Visual Studio 2010.   HOL05 - Developing for SharePoint 2010 with the Client OM and REST in Visual Studio 2010  This hands-on lab introduces the Client object model for use in calling SharePoint 2010 APIs from a client machine. It also shows the use of ADO.NET Data Services to call REST services in SharePoint 2010.   HOL06 - Developing a BCS External Content Type with Visual Studio 2010  This hands-on lab walks you through building an external content type for Business Connectivity Services using Visual Studio 2010. It also builds a form for Microsoft Outlook and shows the data being edited offline in Outlook.   HOL07 - Developing a SharePoint 2010 Workflow with Initiation Form in Visual Studio 2010  This hands-on lab walks you through building a workflow in Visual Studio 2010 for SharePoint 2010. You add an initiation form to the workflow and use an external data exchange activity in the workflow.   HOL08 - Developing SharePoint 2010 User Interface with Silverlight in Visual Studio 2010  This hands-on lab walks you through building Microsoft Silverlight applications for use in SharePoint 2010. You will access SharePoint 2010 data in Silverlight using the Client object model.   HOL09 - Developing SharePoint 2010 Sandboxed Solutions in Visual Studio 2010  This hands-on lab walks you through building a Sandboxed Solution Web Part for SharePoint 2010. It will also add code to the Web Part that overloads the limits placed by the sandboxed solution, and you will review how the solution is shut down.   HOL10 - Developing SharePoint 2010 User Interface Ribbon and Dialog Customizations  This hands-on lab walks you through adding a custom action to the SharePoint 2010 ribbon, and creating a Web Part that uses the Dialog Framework.  You can download them here.  Besides that I subscribed myself for one of the MSDN Get Ready sessions that will be given in November and December of this year. I will be attending the SharePoint 2010 Development session. Let me know if you will be attending one of those sessions.   ","categories": ["Development","Documentation","SharePoint"],
        "tags": ["Documentation","SharePoint 2010"],
        "url": "/2009/11/sharepoint-2010-getting-started-with-development/",
        "teaser": null
      },{
        "title": "Hiding the &lsquo;Create Site&rsquo; option in the Site Actions menu",
        "excerpt":" Within SharePoint you have something called custom actions. You can use custom actions to create custom menu items within SharePoint. Besides custom actions you also have hide custom actions to hide custom actions. With the hide custom actions you can only hide custom actions that have been created with a CustomAction schema file.  The ‘Create Site’ menu options and also the ‘Create Page’ option are not created with a custom actions but on a different way. To hide these custom actions you can alter a file that resides in the masterpages gallery within a site collection.  In the masterpages gallery there is a folder called ‘Editing Menu’ in this folder there are four files you can use to alter certain menu options.   CustomEditingMenu.xml : Used to customize the page editing toolbar.   CustomQuickAccess.xml: Used to customize the quick access actions.   CustomSiteAction.xml: Used to alter the site actions menu.   RTE2ToolbarExtension.xml: Used to customize the HTML Editor Field control.  When you would like to hide the ‘Create Site’ action in the site actions menu you have to alter the CustomSiteAction.xml file by adding several nodes. Below there is an example of a CustomSiteAction.xml file that hides the ‘Create Site’ action:  &lt;xml version=\"1.0\" encoding=\"utf-8\" ?&gt; &lt;Console&gt;     &lt;references&gt; &lt;reference TagPrefix=\"cms\" assembly=\"Microsoft.SharePoint.Publishing, Version=12.0.0.0, Culture=neutral, PublicKeyToken=71e9bce111e9429c\"   namespace=\"Microsoft.SharePoint.Publishing.WebControls.EditingMenuActions\" /&gt;     &lt;/references&gt;     &lt;structure&gt;      &lt;ConsoleNode ConfigMenu=\"Delete\" ChangedNodeID=\"wsaCreateSite\" /&gt;     &lt;/structure&gt; &lt;/Console&gt; The reference is to a SharePoint assembly that creates the custom actions. The ‘ConsoleNode’ stated what should happen with a certain menu action in the above example it is ‘ChangeNodeID’ which is ‘wsaCreateSite’ (This id can be found be looking in the source of the page). If you would also like to hide the ‘Create Page’ action you add the following line to the CustomSiteAction file:  &lt;consolenode changednodeid=\"wsaCreatePage\" configmenu=\"Delete\" /&gt; These changes can also be done with a feature and a feature receiver if you guys would like to have a example how to accomplish this let me now and also if you would like to see some examples for the other files.    ","categories": ["Development","SharePoint"],
        "tags": ["Custom Action","ID","SharePoint"],
        "url": "/2009/12/hiding-the-create-site-option-in-the-site-actions-menu/",
        "teaser": null
      },{
        "title": "Microsoft Visual Studio 2010 Gets New Launch Date",
        "excerpt":" In late 2009 Microsoft added an extra test release to the schedule of Visual Studio 2010. This resulted in a push-back of the launch date of the product.  Microsoft had now rescheduled the official launch event for VS 2010 and .Net 4.0 from 22 March to April 12.  &nbsp;  More information about Visual Studio 2010 can be found here:   Planned packaging and pricing of Visual Studio 2010.   Information about Visual Studio 2010.   Enhancements over previous versions of Visual Studio that will be in the 2010 release.   &nbsp;  source: http://blogs.zdnet.com/microsoft/?p=4981   ","categories": ["Development"],
        "tags": ["Release"],
        "url": "/2010/01/microsoft-visual-studio-2010-gets-new-launch-date/",
        "teaser": null
      },{
        "title": "Inline Editing in SharePoint 2010 (UI and Code)",
        "excerpt":" Within SharePoint 2010 it is possible to enable inline editing on list items. This can be done by changing properties on a view of a SharePoint library what can be done in code and trough the user interface. To allow inline editing trough the user interface you have to do the following: Navigate to the list for which you want to allow inline editing:     Click on list and then list settings. On the list settings screen click on the list view you would like to edit on the bottom of the screen:     In the view screen you have a section called: Inline editing. Within that section you have a checkbox to allow inline editing:     When you have enabled this checkbox you can inline edit the items in your list:     Inline editing can also be enabled trough code. You can do this by following the code example below:   using(SPSite site = new SPSite(\"URL of the site\")) {     SPWeb web = site.RootWeb;     SPList list = web.Lists[\"Name of the list\"];      SPView view  = list.Views[\"Title of the view\"];     view.InlineEdit = \"TRUE\";     view.Update(); } The ‘InlineEdit’ property on the view is in the public beta version of SharePoint 2010 a string value. Hopefully they will change this to a Boolean value in the final version.   ","categories": ["Administration","Development","SharePoint"],
        "tags": ["Inline Editing","SharePoint 2010"],
        "url": "/2010/02/inline-editing-in-sharepoint-2010-ui-and-code/",
        "teaser": null
      },{
        "title": "Visual Studio 2010 Release Candidate is Available",
        "excerpt":" A few day’s ago the Release Candidate of Visual Studio 2010 became available. Visual Studio 2010 feels a lot faster and is about 1GB larger then older versions. Go try it out and let me know what you think.  VS2010 Release Candidate   Versions   You can download the following versions of VS2010 from the download center:    Ultimate Premium Professional Remote Debugger F# 2.0 Runtime        &nbsp;  ","categories": ["Development"],
        "tags": ["Release","Visual Studio 2010"],
        "url": "/2010/02/visual-studio-2010-release-candidate/",
        "teaser": null
      },{
        "title": "Failure in loading assembly",
        "excerpt":" In the application event log I found several errors that were pointing to assemblies that I wasn’t using anymore. The error looks like the message below: Error: Failure in loading assembly: Assembly, Version=1.0.0.0, Culture=neutral, PublicKeyToken=Token.  The application was trying to load an assembly that I wasn’t in the GAC or in the BIN folder. This causes the error so I searched trough the complete solution for references to the specified assembly but couldn’t find any. After searching I opened the web.config file of the web application and the one from central administration and saw that there were safecontrols registered to assemblies that I wasn’t using anymore.   &lt;SafeControl Assembly=\"Assembly, Version=1.0.0.0, Culture=neutral, PublicKeyToken=Token\" Namespace=\"Namespace\" TypeName=\"*\" Safe=\"True\" /&gt; By deleting these safecontrol the problem was solved.   ","categories": ["Development","SharePoint","Troubleshooting"],
        "tags": ["Error","Failure"],
        "url": "/2010/02/failure-in-loading-assembly/",
        "teaser": null
      },{
        "title": "SharePoint 2010 &amp; Office 2010 Launch Date",
        "excerpt":" Today, it is officially announced that May 12th, 2010, is the release date for SharePoint 2010 &amp;amp; Office 2010.  The RTM of the products they are trying to release to RTM this April.  Can’t wait to be working with the RTM :).   ","categories": ["Microsoft Office","SharePoint"],
        "tags": ["Office 2010","Release","SharePoint 2010"],
        "url": "/2010/03/sharepoint-2010-office-2010-launch-date/",
        "teaser": null
      },{
        "title": "(PDF) SharePoint Icon Integration",
        "excerpt":" A few months ago I have created a solution to automatically add a icon (PDF) to your SharePoint farm. I never had the change to test it on a multiple server farm. Last week I had the change to test it and altered it in several ways. The blog posts I made of the old solution can be found here: Feature for adding a Icon to your SharePoint farm I have placed the new solution on CodePlex: Description (SharePoint Icon Integration) SharePoint Icon Integration makes it easier for SharePoint Administrators / Developers to add a icon (PDF) to the SharePoint farm.  You will no longer have to alter the DocIcon.xml file manually. The solution is developed in C# and works in a multiple farm environment. Download Source Code   ","categories": ["Development","SharePoint"],
        "tags": ["CodePlex"],
        "url": "/2010/03/pdf-sharepoint-icon-integration/",
        "teaser": null
      },{
        "title": "SharePoint 2010 reaches RTM",
        "excerpt":" 16 April Microsoft reached an exciting engineering milestone: the release-to-manufacturing (RTM) for Office 2010, SharePoint 2010, Visio 2010 and Project 2010!  The Volume License customers with active Software Assurance (SA) on these products will be one of the first to receive the 2010 set of products. They will be able to download the product in English, French, Spanish, German, Russian, and Dutch via the Volume Licensing Service Center starting April 27.  &nbsp;  It is said that MSDN and Technet subscribers can download the version on April 22.  &nbsp;  Can’t wait to get my hands on the release.   ","categories": ["Azure DevOps"],
        "tags": ["RTM"],
        "url": "/2010/04/sharepoint-2010-reaches-rtm/",
        "teaser": null
      },{
        "title": "SharePoint 2010 Documentation",
        "excerpt":" A few day’s ago (April 15) Microsoft released a great set of documentation about SharePoint 2010. The following documentation can be downloaded:  &nbsp;   SharePoint Server 2010 performance and capacity test results and recommendations : A white paper set that describe the performance and capacity impact of specific feature sets included in Microsoft SharePoint Server 2010.   SharePoint Server 2010 capacity management: software boundaries and limits : A white paper that provides information to help you understand the tested performance and capacity limits of Microsoft SharePoint Server 2010.   Extranet Topologies for SharePoint 2010 Products : Illustrates the specific extranet topologies that have been tested with SharePoint 2010 Products.   Topologies for SharePoint Server 2010 : Describes common ways to build and scale farm topologies, including planning which servers to start services on.   Databases That Support SharePoint 2010 Products : Describes the Microsoft SQL Server databases on which SharePoint 2010 Products run.   ","categories": ["Documentation","SharePoint"],
        "tags": ["Documentation","SDK"],
        "url": "/2010/04/sharepoint-2010-documentation/",
        "teaser": null
      },{
        "title": "Installation of Microsoft SharePoint Server 2010",
        "excerpt":"Update: Since a few weeks I'm using Windows Server 2008 R2 with Hyper-V installed. This option works far better then VirtualBox. Besides this I also installed a Solid State Disk (This really increases the performance). To use Windows Server 2008 R2 as a desktop read the following article: Using Windows Server as a Desktop Operating System April 22 Microsoft released the RTM version of SharePoint Server 2010. For this occasion I have set up a new environment, and will share the setup with you guys. When you want to install a SharePoint Server 2010 environment you will have to check if you meet up with the software and hardware requirements from Microsoft. These requirements can be found here. Since my installation will be a development environment everything will be installed on a single virtual machine. For the creation of the virtual machine I have used the latest version off VirtualBox. I gave the virtual machine a fixed-sized storage off 40 GB. On the first run of the virtual machine I entered the setup disc of Windows Server 2008 R2 x64. When the installation was complete I edited the roles off the server and gave the server the following roles:  Domain server   Application server   Web server  When I was finished with the configuration off the roles. I looked at TechNet to find some information about the users you will have to configure for the SharePoint 2010 installation and I found some great articles for setting up the user rights and the accounts you need for the installation:  Administrative and service accounts required for initial deployment (SharePoint Server 2010)   Deployment scenarios (SharePoint Server 2010)  After I read the information I started the installation off SQL Server 2008 and SQL Server 2008 SP1. The information stated that you have to have a SQL Server running SP1 Cumulative Update 2. So I downloaded the SP1 Cumulative Update 2 from the Microsoft site. But you can also install SQL Server 2008 R2 (RTM will appear on MSDN on 3 May by the way :)):  Cumulative update package 2 for SQL Server 2008 Service Pack 1  Since the pop3 service isn’t in Windows Server 2008 I also installed a application that can be used for the mail.  MailEnabled Standard Edition  When you follow the guides from TechNet you will setup the rights in the right way and you can start with the installation of SharePoint. When you insert the SharePoint Server 2010 installation disc / iso it will startup with a splash screen:     As you can see you will have several options on this splash screen. You can choice to review the hardware and software requirements and the installation guide if you haven't read my links to TechNet yet. To begin the installation we will have to install the software prerequisites.To to install them click on ‘Install software prerequisites’.     The Microsoft SharePoint 2010 Products Preparation will install all of the software prerequisites. If you click on next the tool will start downloading all of the prerequisites you don’t have on the server. After downloading those it will automatically start the installation of the prerequisites.     When the installation off the prerequisites is complete you will have to check the screen if all installations were successful. If there was an installation unsuccessful don’t continue with the installation because it can break the complete SharePoint installation and you will have to start over. Try to fix it by reading the log files.     When the installation off the software prerequisites is completed you can start with the installation of the SharePoint 2010 binaries. To start the installation you will have to click on ‘Install SharePoint Server’ on the splash screen. The first thing you will have to do is insert the product key.     After you have entered your key you will have to accept the License Terms:     When you have accepted the license terms and have clicked continue you will have the option to change the file location. If you want you can set the file location to another folder but for my environment I have kept the default settings.     After you click on ‘Install Now’ the installation will start running.     After a while the installation is complete and you will have the ability to finish the installation. On last screen you will also have a checkbox to Run the SharePoint Products Configuration Wizard. You can also uncheck this checkbox and run it from the start menu: Start – All Programs – Microsoft SharePoint 2010 Products - SharePoint 2010 Products Configuration Wizard. For now we will keep the checkbox checked and click on ‘Close’.     The SharePoint Products Configuration Wizard will start up with the following screen.     When you click next the Configuration Wizard will give you a warning that it may have to reset several services. Since this is our first installation we can ignore this message and click on ‘Yes’.     When we have clicked ‘Yes’ the configuration wizard will ask us if we would like to connect to a existing server farm. Since we don’t have a server farm we will click on ‘Create a new server farm’ and click on ‘Next’.     On the next screen you can specify the configuration database settings. Here you will have to fill in the Database server and the name of the Configuration Database. Besides that you will also have to provide the account that are needed to access the database be aware that this account will also be used as the application pool account for Central Administration.     When you have filled in all the settings and you have clicked on ‘Next’ you will see that the SharePoint 2010 Configuration Wizard has a screen that the SharePoint 2007 Configuration has not. On this screen you will have to provide a password for the farm that you will have to use when you want to connect a new server to the farm.     On the next you will have to setup the Central Administration Web Application by providing a port number you want to run the Central Administration Web Application under and you will also have to provide which authentication provider you would like to use. If you choice to use Kerberos you will also have to create Service Principal Names (SPN). In this article you can read how this can be done.     When you click on ‘Next’ you will see a summary of all the properties you have filled in. Check these settings and click on ‘Next’ to run the configuration wizard.       When the configuration wizard is complete you have a new SharePoint development environment were in you can create new web applications.     In the future I will also write several guides to create several service applications. For now Happy SharePointing.   ","categories": ["Administration"],
        "tags": ["Install","SharePoint 2010"],
        "url": "/2010/04/installation-of-microsoft-sharepoint-server-2010/",
        "teaser": null
      },{
        "title": "SharePoint Memory Leak",
        "excerpt":"A few day’s ago I stumbled on a article about SharePoint Memory leaks because we were looking at memory leaks for 1 of out clients. I found a great article from Todd Carter about a giant memory leak within SharePoint that he found just a month ago. It is really a must read: SharePoint’s Sasquatch Memory Leak ","categories": ["Development","SharePoint","Troubleshooting"],
        "tags": ["Memory","SharePoint"],
        "url": "/2010/04/sharepoint-memory-leak/",
        "teaser": null
      },{
        "title": "Article in TechNet Magazine",
        "excerpt":" For the Dutch TechNet magazine I wrote an article about the security within SharePoint. This is my first TechNet article and I think it has turned out great. The link to the article in included below.  Veiligheid in en om SharePoint - TechNet Magazine, juli 2010  Also read the article of a colleague of mine which wrote an article on SharePoint and Communities that was in the same magazine.  SharePoint, de virtuele koffiecorner - TechNet Magazine, juli 2010   ","categories": ["Article"],
        "tags": ["MSDN","Security","TechNet"],
        "url": "/2010/06/article-in-technet-magazine/",
        "teaser": null
      },{
        "title": "SharePoint 2010 Custom Errors",
        "excerpt":" About a year ago I wrote a article on deactivating the custom errors off SharePoint 2007. That had to be done in the following manner:  Step 1:  Change  &lt;SafeMode MaxControls=\"200\" CallStack=\"false\" &gt; to &lt;SafeMode MaxControls=\"200\" CallStack=\"true\" &gt; Step 2: &lt;customerrors mode=\"Off\" /&gt; If you do this on a SharePoint 2010 installation you will still get the following error:  &nbsp;Server Error in '/' Application.    Runtime Error Description: An application error occurred on the server. The current custom error settings for this application prevent the details of the application error from being viewed.   Details: To enable the details of this specific error message to be viewable on the local server machine, please create a tag within a \"web.config\" configuration file located in the root directory of the current web application. This tag should then have its \"mode\" attribute set to \"RemoteOnly\". To enable the details to be viewable on remote machines, please set \"mode\" to \"Off\".   This errors states that you did not make the change. This is because there is another web.config file that is being loaded. This file can be found in the following location:  &nbsp;C:\\Program Files\\Common Files\\Microsoft Shared\\Web Server Extensions\\14\\TEMPLATE\\LAYOUTS\\web.config  If you change this file you will be able to see the .net errors. If you would like to see custom errors on Central Administration you will also have to change the web.config file in the admin folder off the 14 hive:  &nbsp;C:\\Program Files\\Common Files\\Microsoft Shared\\Web Server Extensions\\14\\TEMPLATE\\ADMIN\\web.config  Happy Coding.    ","categories": ["SharePoint","Troubleshooting"],
        "tags": ["SharePoint 2010","web.config"],
        "url": "/2010/06/sharepoint-2010-custom-errors/",
        "teaser": null
      },{
        "title": "Console Application for SharePoint 2010 [No User Profile Application available to service the request. Contact your farm administrator.]",
        "excerpt":" Today I was working on some cool features to update the User Profile store in SharePoint 2010. To test this feature I usually use a Console Application. In my Console Application I was keep getting an error on a line like below:   UserProfileManager profileConfigManager = new UserProfileManager(ServiceContext); The exception I kept getting was : 'No User Profile Application available to service the request. Contact your farm administrator.' I made sure the User Profile Application was running correctly and it was. I already clicked on the \"Administrators\" button in the toolbar of the Service Application to give me Full Control, but it still wasn't working.  Till I noticed that there is also a 'Permissions' button. I clicked the button and saw that my current user wasn't in the permissions box. I gave my user 'Full Control' and now everything works great from the console application.   Happy Coding!    ","categories": ["Development"],
        "tags": ["Console Application","Troubleshouting"],
        "url": "/2010/08/console-application-for-sharepoint-2010-no-user-profile-application-available-to-service-the-request-contact-your-farm-administrator/",
        "teaser": null
      },{
        "title": "Provisioning My Sites SharePoint 2010",
        "excerpt":" In certain scenarios you would like to provision the My Sites of the users before you go live with the environment. Within SharePoint 2010 you have got several options to complete this task.   One of the options is to create a PowerShell script. With PowerShell you have the ability to read through the user profile store that is used for a certain web application and create a My Site for each profile in the profile store. If you want to use PowerShell to accomplish this task you can use the following script:   [Reflection.Assembly]::LoadWithPartialName(\"Microsoft.SharePoint\")  [Reflection.Assembly]::LoadWithPartialName(\"Microsoft.Office.Server\")    $siteurl = \"http://yoursiteurl\"        $site = New-Object Microsoft.SharePoint.SPSite($siteurl)  $context = [Microsoft.Office.Server.ServerContext]::GetContext($site)  $upm = New-Object Microsoft.Office.Server.UserProfiles.UserProfileManager($context)   $upm.Count; $profiles = $upm.GetEnumerator() foreach ($userobject in $Profiles ) {      # start loop here      $user = $userobject.item(\"AccountName\")     $user     if ($upm.UserExists($user)) {          $profile = $upm.GetUserProfile([string]$user)                # there are other exceptions you can catch, check out the UserProfiles class          trap [Microsoft.Office.Server.UserProfiles.PersonalSiteExistsException] {              Write-Host \"personal site already exists for $user\"              continue          }          $profile.CreatePersonalSite();         trap {         Write-Host \"Error creating site for $user\" -ForeGroundColor Red         continue         }         Write-Host \"Personal site for $user created...\" -ForeGroundColor Green     }  else {        Write-Host: \"user $user did not exist\"      }  } # end loop here  $site.Dispose()  The second option you have is creating a C# application. This c# application will to exactly the same as the powershell script. The method to create the MySites is displayed below.   private static void CreateMysite(string siteUrl)         {             using (SPSite site = new SPSite(p_2))             {                 SPServiceContext serviceContext = SPServiceContext.GetContext(site.WebApplication.ServiceApplicationProxyGroup, SPSiteSubscriptionIdentifier.Default);                 UserProfileManager profileConfigManager = new UserProfileManager(serviceContext);                 try                 {                     long count = profileConfigManager.Count;                      for (int i = 1; i &lt;= count; i++)                     {                             UserProfile profile = profileConfigManager.GetUserProfile(i);                             Console.WriteLine(profile.DisplayName);                             profile.CreatePersonalSite(1043);                     }                 }                 catch (UserNotFoundException) {}                 catch(PersonalSiteExistsException){}             }         } ","categories": ["Development"],
        "tags": ["My Sites","Script","SharePoint 2010"],
        "url": "/2010/09/provisioning-my-sites-sharepoint-2010/",
        "teaser": null
      },{
        "title": "SharePoint Site Search Settings (Search Scope DropDown)",
        "excerpt":" Today I was working on some SharePoint master pages and I noticed that the search scopes dropdown was gone. After searching around in the site collection administration settings I found a setting under the 'search settings' link.     The property 'Site Collection Search Dropdown Mode' can be set to several options. Default this property is set to 'Do not show scopes dropdown, and default to contextual scope'. In this setting you have the following options: Do not show scopes dropdown, and default to contextual scope. You will get result of the 'All Sites' scope when you are at the top level. If you are in another level you will get results from your context. Do not show scopes dropdown, and default to target results page The search results will be displayed as described above. It will only use your target result page for displaying the results. Show scopes dropdown Display's the scopes that are defined in the scope display group of the site. Show, and default to ’s’ URL parameter Displays the scopes that are defined in the scope display group of the site. This option will add the scope to the query string using the 's' parameter. Show and default to contextual scope Displays the scope drop-down list and selects the \"This site\" and \"This list\" by default. Show, do not include contextual scopes This option will not display the contextual scopes. Show, do not include contextual scopes, and default to ’s’ URL parameter This option will not display the contextual scopes, and will also send the scope within the query string using the 's' parameter. These settings can all be set trough the UI but in most situations we as developers need to set these properties within a feature receiver. When you search through the object model you will not find a possibility off doing it through code. The next option is to look with reflector in the code behind of the page and you will find a way to accomplish this task. Each site or web has a property bag by setting the \"SRCH_ENH_FTR_URL\" you set the search site URL of the site collection. And by setting the \"SRCH_SITE_DROPDOWN_MODE\" property you change the display mode of the search dropdown. Here below you can find an example in code:    private void ApplySearchSettings(SPWeb web)  {        if(web.IsRootWeb)        {             string searchCenterUrl = (new Uri(web.Url).GetLeftPart(UriPartial.Authority)) + \"searchcenter\";             web.AllProperties[\"SRCH_SITE_DROPDOWN_MODE\"] = \"ShowDD_DefaultContextual \";             web.AllProperties[\"SRCH_ENH_FTR_URL\"] = searchCenterUrl;             web.Update();        }  } The property \"SRCH_SITE_DROPDOWN_MODE\" has several options you can fill in:    HideScopeDD – Do not show scopes dropdown, and default to contextual scope.  HideDD_NoScope - Do not show scopes dropdown, and default to target results page.  ShowDD – Show scopes dropdown.  ShowDD_DefaultURL – Show, and default to ‘s’ URL parameter.  ShowDD_DefaultContextual – Show and default to contextual scope.  ShowDD_NoContextual – Show, do not include contextual scopes.  ShowDD_NoContextual_DefaultURL – Show, do not include contextual scopes, and default to ‘s’ URL parameter  For the scopes dropdown modes you can also look at the following URL in de SDK of Microsoft:  http://msdn.microsoft.com/en-us/library/microsoft.sharepoint.portal.webcontrols.dropdownmodes.aspx   ","categories": ["SharePoint"],
        "tags": ["Search Drop Down","SharePoint"],
        "url": "/2010/09/sharepoint-site-search-settings-search-scope-dropdown/",
        "teaser": null
      },{
        "title": "Creating Metadata Properties through code - SharePoint 2010",
        "excerpt":" When you work on SharePoint project you often see solutions that are built by using the search engine of SharePoint. To perform queries against the search engine you define 'Metadata properties' these metadata properties need to be mapped to a column within SharePoint. Here comes the first problem when you define those properties through the UI you first have to create an item where in the column is filled in. Then you have to perform all Full Crawl to be able to set the mapping to the Metadata property. When you define the properties within a feature receiver you can create the mapping and you do not have to perform the crawl. Ok, so let's start with creating the properties within a feature receiver. First we will have to define an object in which we can save the mapping like below:         [CLSCompliant(false)]     public class SearchMapping     {         /// &lt;summary&gt;         /// Gets or sets the category.         /// &lt;/summary&gt;         /// &lt;value&gt;The category.&lt;/value&gt;         public string Category { get; set; }          /// &lt;summary&gt;         /// Gets or sets the type of the variant.         /// &lt;/summary&gt;         /// &lt;value&gt;The type of the variant.&lt;/value&gt;         public int VariantType { get; set; }          /// &lt;summary&gt;         /// Gets or sets the name.         /// &lt;/summary&gt;         /// &lt;value&gt;The name.&lt;/value&gt;         public string Name { get; set; }          /// &lt;summary&gt;         /// Gets or sets the property set ID.         /// &lt;/summary&gt;         /// &lt;value&gt;The property set ID.&lt;/value&gt;         public Guid PropertySetId{get;set;}          /// &lt;summary&gt;         /// Gets or sets the type.         /// &lt;/summary&gt;         /// &lt;value&gt;The type.&lt;/value&gt;         public ManagedDataType MappedType { get; set; }          /// &lt;summary&gt;         /// Initializes a new instance of the &lt;see cref=\"SearchMapping\"/&gt; class.         /// &lt;/summary&gt;         /// &lt;param name=\"name\"&gt;The name.&lt;/param&gt;         /// &lt;param name=\"category\"&gt;The category.&lt;/param&gt;         /// &lt;param name=\"variantType\"&gt;Type of the variant.&lt;/param&gt;         /// &lt;param name=\"propertySetId\"&gt;The property set ID.&lt;/param&gt;         /// &lt;param name=\"type\"&gt;The type.&lt;/param&gt;         public SearchMapping(string name, string category, int variantType, Guid propertySetId, ManagedDataType type)         {             Category = category;             VariantType = variantType;             Name = name;             PropertySetId = propertySetId;             MappedType = type;         }     } &nbsp;  Within the object you can see that we have all the properties to define a mapping. You can get the values of these properties by creating a metadata property through the UI and click on the field you have mapped to the property. For a custom field within SharePoint the values will look like the following:    Category:The category of the field. This will be 'SharePoint'.  VariantType:The variant type will represent the datatype. For a text field this will be '31'.  Name:The name of the mapping mostly the internal name of the column prefixed with ows_  PropertySetId:The id of the property set for SharePoint columns this is: '00130329-0000-0130-c000-000000131346'.  MappedType:This is the 'ManagedDataType' of the property.  When we have the mapping we can define the feature activation method.          /// &lt;summary&gt;         /// Occurs after a Feature is activated.         /// &lt;/summary&gt;         /// &lt;param name=\"properties\"&gt;An &lt;see cref=\"T:Microsoft.SharePoint.SPFeatureReceiverProperties\"/&gt; object that represents the properties of the event.&lt;/param&gt;         public override void FeatureActivated(SPFeatureReceiverProperties properties)         {             if (properties == null)                 throw new ArgumentNullException(\"properties\", \"Argument 'properties' cannot be 'null'\");              SPWebApplication webApplication = (SPWebApplication)properties.Feature.Parent;             SPServiceContext serviceContext = SPServiceContext.GetContext(webApplication.ServiceApplicationProxyGroup, SPSiteSubscriptionIdentifier.Default);             SearchServiceApplicationProxy searchApplicationProxy = serviceContext.GetDefaultProxy(typeof(SearchServiceApplicationProxy)) as SearchServiceApplicationProxy;              // Service Application Info object to retrieve the application id for the search service.             SearchServiceApplicationInfo searchApplictionInfo = searchApplicationProxy.GetSearchServiceApplicationInfo();              // Retrieve the search application instance for the specified id.             SearchServiceApplication searchApplication = SearchService.Service.SearchApplications.GetValue&lt;SearchServiceApplication&gt;(searchApplictionInfo.SearchServiceApplicationId);             var searchSchema = new Schema(searchApplication);              SearchMapping office12 = new SearchMapping(\"12\", \"Office\", 64, new Guid(\"f29f85e0-4ff9-1068-ab91-08002b27b3d9\"), ManagedDataType.DateTime);             SearchMapping basic15 = new SearchMapping(\"15\", \"Basic\", 64, new Guid(\"b725f130-47ef-101a-a5f1-02608c9eebac\"), ManagedDataType.DateTime);             SearchMapping PageImage = new SearchMapping(\"ows_PageImageUrl\", \"SharePoint\", 31, new Guid(\"00130329-0000-0130-c000-000000131346\"), ManagedDataType.Text);              CreateMappedSearchProperty(searchSchema, new List&lt;SearchMapping&gt;() { office12, basic15 }, \"Created\", ManagedDataType.DateTime);             CreateMappedSearchProperty(searchSchema, new List&lt;SearchMapping&gt;() { PageImage }, \"PageImage\", ManagedDataType.Text);         } Within the feature we get the SearchServiceApplication by getting it from the SearchService. When we have the SearchServiceApplication we can get the schema of the SearchApplcation. With this schema we can begin to create the new Metadata properties. In the code you see that I create three types of 'SearchMapping' object these mapping will be used for two properties. the 'office12' and 'basic15' for a Metadata property called 'Created' and 'PageImage' for a property called 'PageImage'. The 'Created' metadata property was one that was OOTB within SharePoint 2007 and does not exist within SharePoint 2010. After we created the mapping object I call on a method called CreateMappedSearchProperty() this method will create the property and needs the schema, mappings (as a generic list of 'searchmapping') title and type as parameters.  private static void CreateMappedSearchProperty(Schema searchSchema, List&lt;SearchMapping&gt; mappings, string propertyName, ManagedDataType type) {    ManagedProperty managedProperty = CreateProperty(searchSchema, propertyName, type);    var mappingCollection = CreateNewMappingCollection(searchSchema, mappings, managedProperty);    managedProperty.SetMappings(mappingCollection);    managedProperty.Update(); } We create or get the managed property if it already exist by calling the method CreateProperty() that needs the schema, property name and type as parameters. When we have the property we create a new mapping collection by calling the method CreateNewMappingCollection() that needs the schema, list of mappings, and the property as parameters. This will return a 'MappingCollection' object that we will pass into the managedProperty. To save the property we call the Update() method on the managed property. The methods that are described above look like.  private static MappingCollection CreateNewMappingCollection(Schema searchSchema, List&lt;SearchMapping&gt; mappings, ManagedProperty prop)         {             var retVal = new MappingCollection();              foreach(SearchMapping mapping in mappings){                 //get the property categories                 CategoryCollection categories = searchSchema.AllCategories;                                  if (categories.Contains(mapping.Category))                 {                     var category = categories[mapping.Category];                     CrawledProperty crawledProperty = category.CreateOrGetCrawledProperty(mapping.Name, false, mapping.PropertySetId, mapping.VariantType);                     retVal.Add(new Mapping(crawledProperty.Propset, mapping.Name, mapping.VariantType, prop.PID));                  }             }              return retVal;         }                      private static ManagedProperty CreateProperty(Schema searchSchema, string propertyName, ManagedDataType managedDataType)         {             //if the managedproperty is available only update the mapping else create the managedProperty             if (!searchSchema.AllManagedProperties.Contains(propertyName)) ","categories": ["SharePoint"],
        "tags": ["Metadata","Search","SharePoint 2010"],
        "url": "/2010/09/creating-metadata-properties-through-code-sharepoint-2010/",
        "teaser": null
      },{
        "title": "Exception: [COMException (0x81072101): &lt;nativehr&gt;0x81072101&lt;/nativehr&gt;&lt;nativestack&gt; &lt;/nativestack&gt; Cannot complete this action.",
        "excerpt":" When I was working on a project I had to create a list instance of one of our own list definitions. I created a feature with the following element.xml.  &lt;xml version=\"1.0\" encoding=\"utf-8\"&gt; &lt;elements xmlns=\"http://schemas.microsoft.com/sharepoint/\"&gt;     &lt;listinstance title=\"Public Navigation\"                   description=\"Public Navigation List\"                   url=\"Lists/NavigationPublic\"                   templatetype=\"15555\"&gt;     &lt;/listinstance&gt; &lt;/elements&gt; When I activate this feature I got the following exception: [COMException (0x81072101): 0x81072101Cannot complete this action.     When I looked into the log files of SharePoint I found the following entries:  &nbsp;09/20/2010 16:11:43.49 w3wp.exe (0x1ED8) 0x1AB4 SharePoint Foundation Fields 88yt High GetUserListSchema(): Failed to get the list schema XML for feature '41202ea8-d927-475f-b710-48fe3af7e403', template '15555': hr=0x81072101. eee79fed-2362-41c1-aaa2-29b93ddee0ca   09/20/2010 16:11:43.49 w3wp.exe (0x1ED8) 0x1AB4 SharePoint Foundation General 8l2r High Failed to find the list schema for FeatureId '41202ea8-d927-475f-b710-48fe3af7e403', list template ID 15555. Cannot create list in web \"http://sharepoint/sites/Test\" at URL \"Lists/NavigationTop\". eee79fed-2362-41c1-aaa2-29b93ddee0ca 09/20/2010 16:11:43.49 w3wp.exe (0x1ED8) 0x1AB4 SharePoint Foundation General 8kh7 High 0x81072101Cannot complete this action. Please try again. eee79fed-2362-41c1-aaa2-29b93ddee0ca   This looked to me like the feature could not find the schema.xml that was defined for the list definition. I then went looking through the list instance definition and found out that I can insert a feature ID attribute. Point this feature ID attribute to the feature that installs the schema.xml and everything will go just fine.  &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt; &lt;Elements xmlns=\"http://schemas.microsoft.com/sharepoint/\"&gt;   &lt;ListInstance Title=\"Public Navigation\"                 TemplateType=\"15555\"                 FeatureId=\"aba8f789-1478-4ac7-ad36-95fceb12a23c\"                 Url=\"Lists/NavigationPublic\"                 Description=\"Public Navigation List\" /&gt; &lt;/Elements&gt;  ","categories": ["SharePoint","Troubleshooting"],
        "tags": ["COMException","Troubleshouting"],
        "url": "/2010/09/exception-comexception-0x81072101-nativehr0x81072101nativehrnativestack-nativestack-cannot-complete-this-action/",
        "teaser": null
      },{
        "title": "Default save location of a document initiated in SharePoint",
        "excerpt":"For the last couple of day's I have been developing word templates. In these templates I had to get the url of the site in which the document was initiated. By default when you click the new button in a document library of SharePoint and create the document in word and you save the document the save dialog will open with the correct library were you clicked on new. In the available object you have with word development I could not find a solution for this problem. But then I thought word has had to be opened with a specific argument so I went looking in de command line arguments. So to find the default save location when you initiated a new document from word you have to do the following. &nbsp;          public static string GetInitiationLocation()         {             string retVal = string.Empty;             string[] commandLineArgs = Environment.GetCommandLineArgs();             if (commandLineArgs.Count() &gt;= 3)             {                 string[] fArgument = commandLineArgs[2].Split(new string[] { \"=\" }, StringSplitOptions.RemoveEmptyEntries);                 if (fArgument.Count() == 3)                 {                     retVal = fArgument[2];                 }             }             return retVal;         }  ","categories": ["Microsoft Office","SharePoint"],
        "tags": ["Office","SharePoint"],
        "url": "/2010/10/default-save-location-of-a-document-initiated-in-sharepoint/",
        "teaser": null
      },{
        "title": "How to check for User Profile changes in SharePoint",
        "excerpt":" In SharePoint each user has a User profile and has the ability to change profile properties. In certain situations you would like to see what the users has changed on their profile, but how do you accomplish this? Within SharePoint the user profile manager or the user profile of a user has a collection called 'UserProfileChangeCollection' in this collection you can find a complete log of changed user profiles. When a change is for example of the type 'UserProfileSingleValueChange' you can read out the values that have been changed in the profile. Beneath here is a little example to read out all of the changes:    using (SPSite site = new SPSite(\"yoursite\"))             {                 SPServiceContext serviceContext = SPServiceContext.GetContext(site.WebApplication.ServiceApplicationProxyGroup, SPSiteSubscriptionIdentifier.Default);                 UserProfileManager profileConfigManager = new UserProfileManager(serviceContext);                 try                 {                     UserProfileChangeCollection collection = profileConfigManager.GetChanges(query);                     foreach (UserProfileChange change in collection)                     {                         Console.WriteLine(change.AccountName);                         Console.WriteLine(change.ChangedProfile);                         Console.WriteLine(change.ChangeType);                         Console.WriteLine(change.EventTime);                         Console.WriteLine(change.Id);                         Console.WriteLine(change.ObjectType);                          if (change is UserProfileSingleValueChange)                         {                             UserProfileSingleValueChange propertyChange = (UserProfileSingleValueChange)change;                             Console.WriteLine(propertyChange.ProfileProperty.Name);                         }                     }                 }                 catch (UserNotFoundException)                 {                 } With this you have the ability to find all of the changes that have occurred. But this can become a great list of changes. To find the changes you want you can also specify a UserProfileChangeQuery.    using (SPSite site = new SPSite(\"yoursite\"))             {                 SPServiceContext serviceContext = SPServiceContext.GetContext(site.WebApplication.ServiceApplicationProxyGroup, SPSiteSubscriptionIdentifier.Default);                 UserProfileManager profileConfigManager = new UserProfileManager(serviceContext);                 try                 {                     UserProfileChangeQuery query = new UserProfileChangeQuery(true, true);                     query.Add = false;                     query.Custom = false;                     query.Delete = false;                     query.MultiValueProperty = false;                     query.SingleValueProperty = true;                     query.Update = true;                     query.UpdateMetadata = true;                                          UserProfileChangeCollection collection = profileConfigManager.GetChanges(query);                     foreach (UserProfileChange change in collection)                     {                         Console.WriteLine(change.AccountName);                         Console.WriteLine(change.ChangedProfile);                         Console.WriteLine(change.ChangeType);                         Console.WriteLine(change.EventTime);                         Console.WriteLine(change.Id);                         Console.WriteLine(change.ObjectType);                          if (change is UserProfileSingleValueChange)                         {                             UserProfileSingleValueChange propertyChange = (UserProfileSingleValueChange)change;                             Console.WriteLine(propertyChange.ProfileProperty.Name);                         }                                            }                 }                 catch (UserNotFoundException)                 {                 }             } &nbsp;  On the query object you can set several properties to find the changes you want. The properties you can define you can find here. Happy SharePointing!   ","categories": ["SharePoint"],
        "tags": ["User Profile","UserProfileChangeCollection"],
        "url": "/2010/10/how-to-check-for-user-profile-changes/",
        "teaser": null
      },{
        "title": "Problems with SharePoint 2007 Migration to SharePoint 2010",
        "excerpt":" At the office one of mine colleagues (Jos van Vlimmeren) was trying to perform a migration of a SharePoint 2007 site to SharePoint 2010. He had a backup of the database and attached it into the SQL Server of the SharePoint 2010 farm. Before he mounted the content database to the web application he used the Test-SPContentDatabase PowerShell cmdlet to check if the database was ok. From the test he got several errors on out of the box webparts like the following:  ---------------------------------------------------------------------------------------------------------------  Category : MissingWebPart  Error : True  UpgradeBlocking : False  Message : WebPart class [bc877bd0-b48e-3165-7c9e-1e2f98c2a42a] is referenced [30] times in the database [SP_ContentDb], but is not installed on the current farm. Please install any feature/solution which contains this web part.  Remedy : One or more web parts are referenced in the database [SP_ContentDb], but are not installed on the current farm. Please install any feature or solution which contains these webparts.   -----------------------------------------------------------------------------------------------------------------   To find more information about the errors he used a tool to verify which webparts were giving the errors. The tool works with the output file you get from the Test-SPContentdatabase cmdlet. To get a output file from the cmdlet you can do the following:   Test-SPContentdatabase -name SP_ContentDb_Operations -WebApplication http://localhost | Out-file c:\\TestSP_Content_Operation.txt  The output file can be read out by the tool that comes from codeplex. The tool will state the names of the webparts and the urls of the sites on which the webparts reside. The tool can be found on the following location (http://sp2010extmigrareport.codeplex.com/). The tool can be used in the command prompt with the following syntax:  sharepoint2007migration.console.exe \"C:\\TestSP_Content_Operations.txt\" c:\\ReportSP_ContentDb_Operations.txt   After he had run the Test-SPContentDatabase he mounted the content database with the Mount-SPContentDatabase PowerShell cmdlet to attach the database to an existing web application. When he saw that the site wasn’t working he realized that something was wrong. Like the errors the webparts were still referencing the webpart assemblies in version 12. So you have to change the references to the 14 version assemblies. This can be done by adding a couple of binding redirects. The bindings we had to add to the web.config were:       &lt;dependentAssembly xmlns=\"urn:schemas-microsoft-com:asm.v1\"&gt;         &lt;assemblyIdentity name=\"Microsoft.SharePoint.Portal\" publicKeyToken=\"71e9bce111e9429c\" culture=\"neutral\" /&gt;         &lt;bindingRedirect oldVersion=\"11.0.0.0-12.0.0.0\" newVersion=\"14.0.0.0\" /&gt;       &lt;/dependentAssembly&gt;       &lt;dependentAssembly xmlns=\"urn:schemas-microsoft-com:asm.v1\"&gt;         &lt;assemblyIdentity name=\"Microsoft.SharePoint.Publishing\" publicKeyToken=\"71e9bce111e9429c\" culture=\"neutral\" /&gt;         &lt;bindingRedirect oldVersion=\"12.0.0.0\" newVersion=\"14.0.0.0\" /&gt;       &lt;/dependentAssembly&gt;       &lt;dependentAssembly xmlns=\"urn:schemas-microsoft-com:asm.v1\"&gt;         &lt;assemblyIdentity name=\"Microsoft.Office.Server.Search\" publicKeyToken=\"71e9bce111e9429c\" culture=\"neutral\" /&gt;         &lt;bindingRedirect oldVersion=\"12.0.0.0\" newVersion=\"14.0.0.0\" /&gt;       &lt;/dependentAssembly&gt;       &lt;dependentAssembly xmlns=\"urn:schemas-microsoft-com:asm.v1\"&gt;         &lt;assemblyIdentity name=\"Microsoft.SharePoint.Search\" publicKeyToken=\"71e9bce111e9429c\" culture=\"neutral\" /&gt;         &lt;bindingRedirect oldVersion=\"12.0.0.0\" newVersion=\"14.0.0.0\" /&gt;       &lt;/dependentAssembly&gt;  ","categories": ["Administration","SharePoint"],
        "tags": ["Migration"],
        "url": "/2010/12/problems-with-sharepoint-2007-migration-to-sharepoint-2010/",
        "teaser": null
      },{
        "title": "New version SPDisposeCheck",
        "excerpt":" The SPDisposeCheck v14.0.4762.1000 was updated and released 12/13/2010.&nbsp;   SPDisposeCheck is a tool that helps developers and administrators check custom SharePoint solutions that use the SharePoint Object Model helping measure against known Microsoft dispose best practices. This tool may not show all memory leaks in your code and may produce false positives which need further review by subject matter experts.  The SPDisposeCheck updated tool remains a standalone command line utility and we’ve added a Visual Studio 2008/2010 IDE Add-In which calls out to the SPDisposeCheck.&nbsp; This Add-In was originally presented at the SPC 2009 and is now available publically.&nbsp; In addition this version has been tested with both WSS 3.0 + MOSS 2007 and SharePoint 2010 (Foundation and Server) environments.   ","categories": ["Development","SharePoint","Tools"],
        "tags": ["SharePoint","SPDisposeCheck","Tool"],
        "url": "/2010/12/new-version-spdisposecheck/",
        "teaser": null
      },{
        "title": "CRM 4.0 SDK",
        "excerpt":" In the last couple of projects I have been at we had a lot of integration question and development request to integrate several objects from CRM. Not In all project I think the right solution was chosen because one of my colleagues pointed me to the CRM 4.0 SDK.  With this SDK you do not have to add a service reference to visual studio to talk to CRM but you can open the CRM context by using a connection string:  &lt;connectionStrings&gt;   &lt;add name=\"Crm\" connectionString=\"Authentication Type=Passport;     Server=https://your-org-name.crm.dynamics.com/your-orgid;     User ID=your-windowslive-id; Password=your-wlid-password;     Device ID=your-device-id; Device Password=your-device-password\"/&gt; &lt;/connectionStrings&gt; And opening the datacontext with that connection string for example with:  var crm = new Xrm.XrmDataContext(\"CRMOnline\"); The SDK really makes it simple and easy to integration CRM with SharePoint or other .Net based web application.  So if you want to integrate CRM with another application you really should take a look at the:  Microsoft Dynamics CRM 4.0 Software Development Kit (SDK)   ","categories": ["Documentation"],
        "tags": ["CRM 4.0","SDK"],
        "url": "/2011/01/crm-4-0-sdk/",
        "teaser": null
      },{
        "title": "Problems with the SharePoint 2010 Rest services ListData.svc",
        "excerpt":" On a current project I'm working on we found a strange behavior problem with the Rest services of SharePoint.  When you create a normal list (custom list or whatever list you want) you can open the list by using the ListData service like this:  http://portal.development.com/_vti_bin/ListData.svc/TestList/  When you navigate to this page you will see a RSS like representation of the the list what actually is a XML response of the service:    The problem occurs when we add a type of column to the list. Lets add a column of the type calculated field and call it test calculation. As data type we will take single line of text and add a calculation to calculate the length of the title like Len([Title]).  If we navigate to the ListData service we will see that the page is still the same like the above screenshot. When we look at the XML we will see that the Test calculation field is available in the XML.  &lt;entry m:etag=\"W/&amp;quot;1&amp;quot;\"&gt;     &lt;id&gt;http://portal.development.com/_vti_bin/ListData.svc/TestList(1)&lt;/id&gt;     &lt;title type=\"text\"&gt;Test&lt;/title&gt;     &lt;updated&gt;2011-01-11T07:58:56+01:00&lt;/updated&gt;     &lt;author&gt;       &lt;name /&gt;     &lt;/author&gt;     &lt;link rel=\"edit\" title=\"TestListItem\" href=\"TestList(1)\" /&gt;     &lt;link rel=\"http://schemas.microsoft.com/ado/2007/08/dataservices/related/CreatedBy\" type=\"application/atom+xml;type=entry\" title=\"CreatedBy\" href=\"TestList(1)/CreatedBy\" /&gt;     &lt;link rel=\"http://schemas.microsoft.com/ado/2007/08/dataservices/related/ModifiedBy\" type=\"application/atom+xml;type=entry\" title=\"ModifiedBy\" href=\"TestList(1)/ModifiedBy\" /&gt;     &lt;link rel=\"http://schemas.microsoft.com/ado/2007/08/dataservices/related/Attachments\" type=\"application/atom+xml;type=feed\" title=\"Attachments\" href=\"TestList(1)/Attachments\" /&gt;     &lt;category term=\"Microsoft.SharePoint.DataService.TestListItem\" scheme=\"http://schemas.microsoft.com/ado/2007/08/dataservices/scheme\" /&gt;     &lt;content type=\"application/xml\"&gt;       &lt;m:properties&gt;         &lt;d:ContentTypeID&gt;0x010005A0A71B03FA954DB854B647353C250C&lt;/d:ContentTypeID&gt;         &lt;d:Title&gt;Test&lt;/d:Title&gt;         &lt;d:TestCalculation&gt;4.00000000000000&lt;/d:TestCalculation&gt;         &lt;d:Id m:type=\"Edm.Int32\"&gt;1&lt;/d:Id&gt;         &lt;d:ContentType&gt;Item&lt;/d:ContentType&gt;         &lt;d:Modified m:type=\"Edm.DateTime\"&gt;2011-01-11T07:58:56&lt;/d:Modified&gt;         &lt;d:Created m:type=\"Edm.DateTime\"&gt;2011-01-11T07:58:56&lt;/d:Created&gt;         &lt;d:CreatedById m:type=\"Edm.Int32\"&gt;1&lt;/d:CreatedById&gt;         &lt;d:ModifiedById m:type=\"Edm.Int32\"&gt;1&lt;/d:ModifiedById&gt;         &lt;d:Owshiddenversion m:type=\"Edm.Int32\"&gt;1&lt;/d:Owshiddenversion&gt;         &lt;d:Version&gt;1.0&lt;/d:Version&gt;         &lt;d:Path&gt;/Lists/TestList&lt;/d:Path&gt;       &lt;/m:properties&gt;     &lt;/content&gt;   &lt;/entry&gt; Now we change the formula of the calculation field to =LEN([TITLE]) = 5 and change the type to Yes/No. In the representation of the list we see that everything is working like it supposed to:    Now lets open the ListData Service and you will notice what I meant with the problem within the ListData service. You will get a Internal Server error like below:    Even if we change the formula to a very simple formula like =Yes the ListData service will still fail. I hope Microsoft will solve this problem in the first Service Package and till then we will just have to work around the problem.   ","categories": ["Development"],
        "tags": ["ListData","Problems","Rest services"],
        "url": "/2011/01/problems-with-the-sp2010-rest-services-listdata-svc/",
        "teaser": null
      },{
        "title": "SharePoint Content Query web part like a SharePoint List View",
        "excerpt":" I have been working on a solution to aggregate documents from sub sites to the top level site of SharePoint with the out of the box features. To accomplish this you will have to work with the Content Query web part of SharePoint.  The Content Query web part works great but it doesn't look like a list view and that was the styling I wanted. So let’s get working and create the styling we want.  To get the content query web part to look like a list view you have to get trough the following steps:  Step 1  Enable the “SharePoint Server Publication Infrastructure” feature if you haven’t already got the feature enabled. This feature makes sure you have the “Content Query” web part available within the web part gallery.     Step 2  Go to the page were you would like to display the “Content Query” web part and place it on the page. The web part can be found under the “Content Rollup” category.   Edit the web part and specify the query you would like to preform. If you have specified the query save the web part and export it.  Note: if you want to query a certain type of content and this content is available within the subsites but not on the root you can not use the UI to create the query. If you want to use the UI you have to make the type available within the root site.    Step 3  You can add your custom columns to the query. This can be accomplish by editing the “CommonViewFields” property of the web part:  &lt;property name=\"CommonViewFields\" type=\"string\"&gt;LinkFilename,Text;Title,Text;&lt;/property&gt; The value of the property must be in the following format [InternalName],[Type];[InternalName],[Type];  Step 4  If you would like to read out those properties by a more meaningful name you can change the “DataColumnRenames” property. This property will take care of the renaming for us.  &lt;property name=\"DataColumnRenames\" type=\"string\"&gt;Title,Title;LinkFilename,Name&lt;/property&gt; The value of the property must be in the following format [InternalName],[New Name];[InternalName],[New Name];  Step 5  Now that we have defined our columns we will need to edit the xslt template to render the columns. We will have to open SharePoint Designer and open the Style Library of the site.    First we will edit the “ContentQueryMain.xsl” file. Within the file find the following line (79):  &lt;xsl:template name=\"OuterTemplate.Body\"&gt;  Within this template a call will be made the ItemTemplate. Within the call we will have to pass a new parameter called “LastRow” so that we know when we will have to close our grid.  For this search for the following lines (128):  &lt;xsl:call-template name=\"OuterTemplate.CallItemTemplate\"&gt;     &lt;xsl:with-param name=\"CurPosition\" select=\"$CurPosition\" /&gt; &lt;/xsl:call-template&gt; And change it to:  &lt;xsl:call-template name=\"OuterTemplate.CallItemTemplate\"&gt;     &lt;xsl:with-param name=\"CurPosition\" select=\"$CurPosition\" /&gt;     &lt;xsl:with-param name=\"LastRow\" select=\"$LastRow\" /&gt; &lt;/xsl:call-template&gt; Now that we pass the parameter we also have to change the template to accept the parameter.  Go to line 147 and you see the “CallItemTemplate”. Copy the second line and past it directly beneath it and make it look like this:  &lt;xsl:template name=\"OuterTemplate.CallItemTemplate\"&gt; &lt;xsl:param name=\"CurPosition\" /&gt; &lt;xsl:param name=\"LastRow\" /&gt; Because we want to use this within our custom item template we also have give the parameter through to the template by adding a when statement just before the &lt;xsl:otherwise&gt; within the CallItemTemplate:  &lt;xsl:when test=\"@Style='SPGrid'\"&gt;  &lt;xsl:apply-templates select=\".\" mode=\"itemstyle\"&gt;   &lt;xsl:with-param name=\"CurPos\" select=\"$CurPosition\" /&gt;   &lt;xsl:with-param name=\"Last\" select=\"$LastRow\" /&gt;  &lt;/xsl:apply-templates&gt; &lt;/xsl:when&gt; In this statement we specify that it only has to pass-through the parameter when the item template is SPGrid. So our custom template is going to be called “SPGrid”.  Step 5  Now it is&nbsp; time to edit the “ItemStyle.xsl” file. Within this file we add our custom xslt item template:  &lt;xsl:template name=\"SPGrid\" match=\"Row[@Style='SPGrid']\" mode=\"itemstyle\"&gt;     &lt;xsl:param name=\"CurPos\" /&gt;     &lt;xsl:param name=\"Last\" /&gt;      &lt;xsl:variable name=\"SafeImageUrl\"&gt;       &lt;xsl:call-template name=\"OuterTemplate.GetSafeStaticUrl\"&gt;         &lt;xsl:with-param name=\"UrlColumnName\" select=\"'ImageUrl'\"/&gt;       &lt;/xsl:call-template&gt;     &lt;/xsl:variable&gt;     &lt;xsl:variable name=\"SafeLinkUrl\"&gt;       &lt;xsl:call-template name=\"OuterTemplate.GetSafeLink\"&gt;         &lt;xsl:with-param name=\"UrlColumnName\" select=\"'LinkUrl'\"/&gt;       &lt;/xsl:call-template&gt;     &lt;/xsl:variable&gt;     &lt;xsl:variable name=\"DisplayTitle\"&gt;       &lt;xsl:call-template name=\"OuterTemplate.GetTitle\"&gt;         &lt;xsl:with-param name=\"Title\" select=\"@Title\"/&gt;         &lt;xsl:with-param name=\"UrlColumnName\" select=\"'LinkUrl'\"/&gt;       &lt;/xsl:call-template&gt;     &lt;/xsl:variable&gt;     &lt;xsl:variable name=\"LinkTarget\"&gt;       &lt;xsl:if test=\"@OpenInNewWindow = 'True'\" &gt;_blank&lt;/xsl:if&gt;     &lt;/xsl:variable&gt;      &lt;xsl:variable name=\"tableStart\"&gt;       &lt;xsl:if test=\"$CurPos = 1\"&gt;        &lt;![CDATA[          &lt;table width=\"100%\" class=\"ms-listviewtable\" cellpadding=\"1\" cellspacing=\"0\" border=\"0\"&gt;           &lt;tr class=\"ms-viewheadertr ms-vhltr\"&gt;             &lt;th class=\"ms-vh-icon\"&gt;&lt;/th&gt;             &lt;th class=\"ms-vh2\"&gt;Name&lt;/th&gt;           &lt;/tr&gt;]]&gt;          &lt;/xsl:if&gt;     &lt;/xsl:variable&gt;      &lt;xsl:variable name=\"tableEnd\"&gt;       &lt;xsl:if test=\"$CurPos = $Last\"&gt;         &lt;![CDATA[ &lt;/table&gt; ]]&gt;       &lt;/xsl:if&gt;     &lt;/xsl:variable&gt;      &lt;xsl:value-of select=\"$tableStart\" disable-output-escaping=\"yes\"/&gt;     &lt;tr class=\"ms-alternating ms-itmhover\"&gt;       &lt;td class=\"ms-vb-icon\"&gt;           &lt;xsl:if test=\"string-length(@DocumentIconImageUrl) != 0\"&gt;             &lt;div class=\"image-area-left\"&gt;               &lt;img class=\"image\" src=\"{@DocumentIconImageUrl}\" title=\"\" /&gt;             &lt;/div&gt;         &lt;/xsl:if&gt;       &lt;/td&gt;       &lt;td class=\"ms-vb2\"&gt;         &lt;a href=\"{$SafeLinkUrl}\" title=\"{@LinkToolTip}\"&gt;                  &lt;xsl:value-of select=\"$DisplayTitle\"/&gt;         &lt;/a&gt;       &lt;/td&gt;       &lt;td&gt;         &lt;xsl:value-of select=\"@Title\"/&gt;       &lt;/td&gt;     &lt;/tr&gt;     &lt;xsl:value-of select=\"$tableEnd\" disable-output-escaping=\"yes\"/&gt; ","categories": ["SharePoint"],
        "tags": ["Content Query","List View","XSLT"],
        "url": "/2011/01/sharepoint-content-query-web-part-like-a-sharepoint-list-view/",
        "teaser": null
      },{
        "title": "SharePoint 2010 Content Organizer Feature",
        "excerpt":" Within SharePoint 2010 Microsoft has changed a lot of the content management features. One of those features is content routing which you can do with the Content Organizer Feature.  When you activate the Content Organizer feature on your site (Web scoped feature). It will automatically create a Drop-off library for you.    All the documents that you upload to the site will be uploaded to the Drop-off library and that library will then route you document to the correct library or folder.  Settings  The settings for the content organizing can be changed by using two web settings features that can be found under the category “Site Administration” on the “Site Settings” screen.  With the option “Content Organizer Rules” you can specify specific rules to route you content within the Site or outside your site collection.  When you want your content to be routed outside your site collection the other site were you want the content to move to also has to have the “Content Organizer” feature enabled.  Besides that you will have to enable the option “Sending to another site” within the “Content Organizer Settings”    After you have enabled this option you also have to specify the site were the content has to be routed to as a sent to location within central administration. This can be done by following these steps:   Navigate to “Central Administration”   Click on “General Application Settings”   And then click on “Configure send to connections”    On the screen that appears you can specify your sent to location.  If you specify a rule that routes a document to another site collection the document will be placed into the drop-off library of the destination site collection.  The document does get routed to a certain place directly but will be prosed by a Timer job. This timer job is scheduled ones a day. You can find this timer job by following these steps:   Navigate to “Central Administration”   Click on “Monitoring”   Click on “Review Job Definitions” under the category “Timer Jobs”   Then find the “Content Organizer Processing” Job specified for you web application.    By clicking the name of the job you can reschedule the job to run within another time frame. Besides that you can also click the “Run now” button.  Routing Documents and Workflows  When you route documents to a document library on which you have connected workflows, those workflows will not start automatically. If you take a look in the ULS log you will find a message like this:  “Declarative workflows cannot automatically start if the triggering action was performed by System Account. Cancelling workflow auto-start.”  It is said that Microsoft knows this bug so lets hope the will fix it in some kind of service pack.  So what can you do about this issue. You will have to create a Item event receiver on the list that will start the workflow associations of the list item:  public override void ItemAdded(SPItemEventProperties properties) {     SPListItem addedItem = properties.ListItem;     SPWorkflowManager manager = addedItem.ParentList.ParentWeb.Site.WorkflowManager;     SPWorkflowAssociationCollection assoCollection = addedItem.ContentType.WorkflowAssociations;      foreach (SPWorkflowAssociation asso in assoCollection)     {         manager.StartWorkflow(addedItem, asso, \"\");     }      base.ItemAdded(properties); } Custom routing  To be more flexible with the routing of your documents you can also create a custom router. I will discuss this option in a later post if there are enough people that will want to here about it.   ","categories": ["DMS","SharePoint"],
        "tags": ["Content Organizer","SharePoint 2010"],
        "url": "/2011/01/sharepoint-2010-content-organizer-feature/",
        "teaser": null
      },{
        "title": "Http Redirect with Query string in IIS 7",
        "excerpt":" In the last couple of years I have had several url’s for my blog:   The first: http://www.smeikkie.nl   The second: http://www.blogaboutsharepoint.com   The latest one: http://msftplayground.com  When I migrated my blog to my own Virtual Private Server I placed all of my articles on SharePoint Foundation. In order to also access my blog with the other url’s I created some alternate access mappings and changed the bindings within IIS.  But what I really wanted was a Http redirection with a 301 status code. If you enable this by default in IIS you will be redirected to the top level. But it is also possible to redirect with the query string.  Step 1:  Create a new website and add the binding you want.  Step 2:  Click on the website. In the feature view you will find Http redirect:    Step 3:  Add the url you want to redirect to and add “$V$Q”. To pass the query string. Also select “Redirect all request to exact destination.  From the dropdown choose Permanent (301):     ","categories": ["Administration"],
        "tags": ["Http Redirect","IIS 7","Query string"],
        "url": "/2011/01/http-redirect-with-query-string-in-iis-7/",
        "teaser": null
      },{
        "title": "Re-Submitting files with the Content Organizer (Submitting files through code)",
        "excerpt":" About a week ago I wrote an article about the “Content Organizer” feature of SharePoint 2010.   The “Content Organizer” feature makes it possible to route all kind of different information within your SharePoint site. If you want to read more about this feature click here.   In certain scenarios you would like to be able to re-submit documents when certain metadata of that document is changed. Out of the box this isn’t possible this means you will have to create a custom solution.   What you can do is create a list settings page that you will have to register on document libraries.   With this page you can enable the re-submitting feature and select the fields that have to be changed if the file needs to be re-submitted.   If you enable the re-submitting feature on the page it will install an event receiver on the current list. This event receiver will run on the ItemUpdated event. In the ItemUpdated event you can then check which fields are changed and then send the document for re-routing.   I hear you asking how can I submit a file for routing. Let’s take a look at the ItemUpdated receiver I have written.  /// &lt;summary&gt; /// An item is being updated. /// &lt;/summary&gt; public override void ItemUpdated(SPItemEventProperties properties) {     base.ItemUpdated(properties);      if (CheckForReRouteItem(properties))     {         if (ReRoutItem(properties))         {             properties.ListItem.Delete();         }     } } This code will be executed by the event Receiver. The method Check ForReRouteItem will check if certain fields were changed.  private bool CheckForReRouteItem(SPItemEventProperties properties) {     List&lt;string&gt; changesFields = GetField(properties);      foreach (string field in changesFields)     {         if (properties.AfterProperties[field] != properties.BeforeProperties[field])         {             return true;         }     }      return false; } In the above method the fields for which we have to check if they are changed are retrieved by another method called GetFields(properties). This method will retrieve the fields that are saved in the property bag by the list settings page.  If a field is changed the method will return true to indicate that the files needs to be submitted to the Content Organizer.  private bool ReRoutItem(SPItemEventProperties properties) {     Guid webId = properties.Web.ID;     Guid siteId = properties.SiteId;     bool retVal = false;      SPSecurity.RunWithElevatedPrivileges(delegate()     {         using (SPSite site = new SPSite(siteId))         {             using (SPWeb web = site.OpenWeb(webId))             {                 SPFile file = properties.ListItem.File;                 string fileType = properties.ListItem.ContentType.Name;                 SPFieldCollection fields = file.Item.Fields;                 List&lt;RecordsRepositoryProperty&gt; itemProperties = new List&lt;RecordsRepositoryProperty&gt;();                  foreach (SPField spField in fields)                 {                     try                     {                         string fieldValue = file.Item[spField.Id] != null ? file.Item[spField.Id].ToString() : string.Empty;                         string value = spField.GetFieldValue(fieldValue) as string;                         RecordsRepositoryProperty property = new RecordsRepositoryProperty                         {                             Name = spField.Title,                             Type = spField.TypeAsString,                             Value = value                         };                         itemProperties.Add(property);                     }                     catch (ArgumentException) { }                 }                  string result = string.Empty;                 OfficialFileResult outcome = OfficialFileCore.SubmitFile(properties.Web, file.OpenBinary(), itemProperties.ToArray(), fileType, file.Web.Url + file.Url, out result);                 if (outcome == OfficialFileResult.Success)                 {                     retVal = true;                 }             }         }     });      return retVal; } The above method performs the actual submitting of the file to the Content Organizer by calling OfficialFileCore.SubmitFile().  This method will return an OfficialFileResult to indicate whether the submitting was successful of not.   Important: With submitting the file we had a problem because it was giving us an outcome of NotFound every time we submitted the file. After debugging the SharePoint dlls we found out that Microsoft performs the following method EcmDocumentRoutingWeb.IsSubmitter(web)):  internal static bool IsSubmitter(SPWeb web) {     if (web == null)     {         return false;     }     bool flag = false;     try     {         string str = web.Properties[\"_dlc_RepositoryUsersGroup\"];         if (!string.IsNullOrEmpty(str))         {             flag = web.IsCurrentUserMemberOfGroup(int.Parse(str, NumberFormatInfo.InvariantInfo));         }     }     catch (FileNotFoundException exception)     {         ULS.SendTraceTag(0x636d7539, ULSCat.msoulscat_DLC_Routing, ULSTraceLevel.Medium, \"Routing Engine: User does not have permissions to submit to this site. Exception: {0}\", new object[] { exception.Message });         flag = false; ","categories": ["DMS","SharePoint"],
        "tags": ["Content Organizer","Submitting"],
        "url": "/2011/02/re-submitting-files-with-the-content-organizer-submitting-files-through-code/",
        "teaser": null
      },{
        "title": "Office 15 images have been discovered",
        "excerpt":" Last year Office 14 was released. And now there are images discovered of Office 15.   Take a quick look:  http://tweakers.net/nieuws/73196/eerste-beelden-van-microsoft-office-15-duiken-op.html  http://winreview.ru/Microsoft-Office/979/Pervij-vzglyad-na-Microsoft-Office-15027031000  &nbsp;   ","categories": ["Microsoft Office"],
        "tags": ["Office 15","Preview"],
        "url": "/2011/03/office-15-images-have-been-discovered/",
        "teaser": null
      },{
        "title": "Problems with the SPSiteDataQuery and Content Type Name",
        "excerpt":" Last week a colleague and I&nbsp; were working on a solution to find items by its content type within a site collection using the SPSiteDataQuery.  When we moved the solution to the testing environment we discovered that the solution was only returning results that were in a single list.   the code we used was the following:  SPSiteDataQuery query = new SPSiteDataQuery(); query.Webs = \"&lt;Webs Scope=\\\"Recursive\\\"&gt;\";   //Ask for all lists created from the contacts template. query.Lists = \"&lt;Lists BaseType='1' MaxListsLimit='0'/&gt;\";   // Get the Title (Last Name) and FirstName fields. query.ViewFields = \"&lt;FieldRef Name=\\\"Dossiername\\\" /&gt;\"; query.ViewFields += @\"&lt;FieldRef Name='Dossiernumber'/&gt;\"; query.ViewFields += @\"&lt;FieldRef Name='ContentType'/&gt;\";   string where = \"&lt;Where&gt;&lt;Eq&gt;\"; where += \"&lt;FieldRef ID='ContentType' /&gt;\"; where += \"&lt;Value Type='Computed'&gt;Dossier&lt;/Value&gt;\"; where += \"&lt;/Eq&gt;&lt;/Where&gt;\";   query.Query = where;     DataTable results = web.GetSiteData(query);   foreach (DataRow row in results.Rows){     Console.WriteLine(\"{0} -- {1} -- {2}\", row[\"ContentType\"], row[\"Dossiername\"], row[\"Dossiernumber\"]); } So this code only returns the items from 1 list. You can solve this problem by searching on the content type id, instead of the content type name. Like the code below:  SPSiteDataQuery query = new SPSiteDataQuery(); query.Webs = \"&lt;Webs Scope=\\\"Recursive\\\"&gt;\";   query.Lists = \"&lt;Lists BaseType='1' MaxListsLimit='0'/&gt;\";   query.ViewFields = \"&lt;FieldRef Name=\\\"Dossiername\\\" /&gt;\"; query.ViewFields += @\"&lt;FieldRef Name='Dossiernumber'/&gt;\"; query.ViewFields += @\"&lt;FieldRef Name='ContentType'/&gt;\";   SPContentType cTypeCollection = web.ContentTypes[\"Dossier\"]; string where = string.Format(                   @\"&lt;Where&gt;                     &lt;BeginsWith&gt;                         &lt;FieldRef Name='ContentTypeId'/&gt;                              &lt;Value Type='Text'&gt;{0}&lt;/Value&gt;                      &lt;/BeginsWith&gt;                   &lt;/Where&gt;\", cTypeCollection.Id);  // Set the query string. query.Query = where;      DataTable results = web.GetSiteData(query);   foreach (DataRow row in results.Rows){     Console.WriteLine(\"{0} -- {1} -- {2}\", row[\"ContentType\"], row[\"Dossiername\"], row[\"Dossiernumber\"]); }  ","categories": ["Development","SharePoint"],
        "tags": ["SPSiteDataQuery"],
        "url": "/2011/03/problems-with-the-spsitedataquery-and-content-type-name/",
        "teaser": null
      },{
        "title": "SSL and Send To Locations",
        "excerpt":" When you are using SharePoint in combination with SSL and Send To locations you have to register you certificate with SharePoint when you would like to register a Send To location that runs under a https address.  When you try to add the location you will receive the following error:  Verification failed: URL is not a valid routing destination.  You will also find a entry within your event viewer which let you see what certificate you need to register:   Log Name:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Application  Source:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Microsoft-SharePoint Products-SharePoint Foundation  Date:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5/02/2011 16:83:51 PM  Event ID:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8311  Task Category:&nbsp;&nbsp; Topology  Level:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Error  Keywords:  User:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Development\\svc_spfarm  Computer:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Diablo.development.com&nbsp;  Description:  An operation failed because the following certificate has validation errors:\\n\\nSubject Name: CN=intra.development.com, OU=IT, O=Development, L=Diablo, S=NC, C=US\\nIssuer Name: CN=Development-Diablo-CA, DC=Development, DC=com\\nThumbprint: 3ACA9260547711E0B9C08CEADFD72085\\n\\nErrors:\\n\\n The root of the certificate chain is not a trusted root authority   &nbsp;  This states that you will have to register your certificate you can register your certificate by using PowerShell:  Add-PSSnapin Microsoft.SharePoint.PowerShell   $rootca = New-Object System.Security.Cryptography.x509Certificates.x509Certificate2(\"[Your Certificate]\")   New-SPTrustedRootAuthority –Name \"[Certificate Name]\" –Certificate $rootca  Tanx to: http://blogs.technet.com/b/blairb/archive/2010/08/18/using-ssl-with-sharepoint-2010-send-to-connections.aspx  ","categories": ["DMS","SharePoint"],
        "tags": ["Send To Locations","SSL"],
        "url": "/2011/03/ssl-and-send-to-locations/",
        "teaser": null
      },{
        "title": "Encoding SharePoint Field names.",
        "excerpt":" During my last projects I have been asked the same questions. The question was how can you get the normal name of a SharePoint field name that is encoded like this: Test_x0020_ Name.  The field names get encoded because XML elements names can’t contain spaces or other different characters. In the example above the x0020 represents the space and the _ represents a delimiter.  You can decode those SharePoint fields by using:  System.XML.XmlConvert.DecodeName(\"[Fieldname]\") ; You can encode fieldnames if you would like by using:  System.XML.XmlConvert.EncodeName(\"[FieldName]\")  Hope this helps you when developing for SharePoint and remember when you have questions about Microsoft technology don't be afraid to ask.  ","categories": ["Development"],
        "tags": ["Encoding","Fields","XMLConvert"],
        "url": "/2011/04/encoding-sharepoint-field-names/",
        "teaser": null
      },{
        "title": "Fully certified SharePoint 2010 Specialist (MCPD &amp; MCITP)",
        "excerpt":" ​It took me long enough to get all off the SharePoint certificates but Yesterday I finally passed the last exam.  I know have the following SharePoint 2010 certificates:   70-573: MCTS: Microsoft SharePoint 2010, Application Development 70-667: MCTS: Microsoft SharePoint 2010, Configuring 70-668: PRO: Microsoft SharePoint 2010, Administrator 70-576: PRO: Designing and Developing Microsoft SharePoint 2010 Applications  I have to say that 70-668 was the hardest exams of them all. But I think that everyone that has enough experience with SharePoint can pass these exams you just have to filter out the wrong answers.  For now I will take a few months of rest in which I will move to my new house in Vlaardingen and wait till the moment my lovely girlfriend will give birth to our son/daughter.  And then I will go for the .NET Framework 3.5 exams especially (.NET Framework 3.5, Windows Workflow Foundation Applications (70-504)).   ","categories": ["Development"],
        "tags": ["Certificates","MCITP","MCPD"],
        "url": "/2011/04/fully-certified-sharepoint-2010-specialist-mcpd-mcitp/",
        "teaser": null
      },{
        "title": "CKS - Development Tools Version 2.0",
        "excerpt":" ​On 17 may a new version of the CKS Developemt tools edition was released. The community kit for SharePoint: Development tools edition is a extension for Visual Studio 2010 SharePoint templates. These extensions help you to have a greater productivity when your are developing SharePoint Solutions.  &nbsp;  New in version 2.0:   WSPBuilder conversion tool (WCT) - This release of the WSPBuilder project importer project template allows a developer to migrate a SharePoint root based project structure to the new SharePoint Tool SPIs based file layout. The WCT is packaged with both Foundation and Server editions of CKSDev. This version incorporates even more translations from 12 root to SharePoint SPI. Keyboard shortcuts - Shorter Keyboard shortcuts for the menu items provided by the extension. New key strokes based on community feedback, keyboard shortcut chords to Alt+C for copy operations or Alt+R for recycle operations or Alt+A for attach to processes  operations. Contextual web part SPI - New web part SPI which adds a contextual ribbon button to the web part template. Central administration page SPI - New central administration page SPI. Options for CKSDev - New options for Visual Studio to enable/disable the CKSDev extension elements. Updated Branding SPI - Improvements to the Branding SPI including the css references and master page update.  You can download the version from the visual studio extension gallery:   CKS - Development Tools Edition (Server) CKS - Development Tools Edition (Foundation)  &nbsp;   ","categories": ["SharePoint","Tools"],
        "tags": ["Developer Tools","Development"],
        "url": "/2011/05/cks-development-tools-version-2-0/",
        "teaser": null
      },{
        "title": ".NET Library for quick and easy Twitter integration",
        "excerpt":" A few days ago I wanted to create a solution to automatically tweet new blog posts. Because I did not have enough time to write a custom component to communicate to twitter I started looking for a .NET Twitter library.  After some time I found a very simple and effective library named Twitterizer. With Twitterizer you can easily implement the OAuth security mechanism.  After adding the reference to the Twitterizer library I created a ItemEventReceiver that starts on the ItemAdded event. In that event I post the title and the url to twitter.  public override void ItemAdded(SPItemEventProperties properties) {     base.ItemAdded(properties);      string message = string.Format(CultureInfo.InvariantCulture, \"New post #msftplayground :{0} - {1}\", ToTinyURLS(properties.Web.Site.Url, properties.ListItem), properties.ListItem.Title);      //TODO Get OAuth from web     OAuthTokens tokens = GetTwitterToken(properties);     TwitterResponse&lt;TwitterStatus&gt; response = TwitterStatus.Update(tokens, message); }  private OAuthTokens GetTwitterToken(SPItemEventProperties properties) {     OAuthTokens retVal = null;          string consumerKeySecret = EncryptionUtility.DecryptedString(properties.Web.GetPropertyBagProperty(Constants.PropertyBagKeyConsumerKeySecret, string.Empty));     string consumerKey = EncryptionUtility.DecryptedString(properties.Web.GetPropertyBagProperty(Constants.PropertyBagKeyConsumerKey, string.Empty));     string accessToken = EncryptionUtility.DecryptedString(properties.Web.GetPropertyBagProperty(Constants.PropertyBagKeyAccessToken, string.Empty));     string accessTokenSecret = EncryptionUtility.DecryptedString(properties.Web.GetPropertyBagProperty(Constants.PropertyBagKeyAccessTokenSecret, string.Empty));      if (!string.IsNullOrEmpty(consumerKeySecret) | !string.IsNullOrEmpty(consumerKey) | !string.IsNullOrEmpty(accessToken) | !string.IsNullOrEmpty(accessTokenSecret))     {         retVal  = new OAuthTokens();         retVal.AccessToken = accessToken;         retVal.AccessTokenSecret = accessTokenSecret;         retVal.ConsumerKey = consumerKey;         retVal.ConsumerSecret = consumerKeySecret;     }     return retVal;  } ","categories": ["Development"],
        "tags": ["ASP.net","Library","Twitter"],
        "url": "/2011/06/net-library-for-quick-and-easy-twitter-integration/",
        "teaser": null
      },{
        "title": "Creating a SharePoint Designer Action for the Sandbox",
        "excerpt":" One of the new features in SharePoint 2010 was the possibility to create Sandbox solutions. Sandbox solutions are custom build solutions that run in another process with another CAS policy.  Sandbox solutions allow you to do the following:   Deploy custom code to you site collection when you do not have access to Central Administration. Sandbox solutions cannot jeopardize the stability of your farm. With SharePoint in a hosted environment you will be able to deploy Sandboxed solutions.  With the ability to deploy Sandbox solutions you also have some important restrictions:   A certain part off the SharePoint API is not available. You cannot read/write to the file system. You cannot call assemblies deployed out of Global Assembly Cache.  After all Sandbox solutions are a powerful feature of SharePoint. SharePoint designer gives you the possibility to develop Custom actions. But when you try to develop those for the sandbox it is totally different.  Let’s start of by creating a empty SharePoint project. When you created a empty project add a new Class file. Mark the class public and add a method that returns a Hashtable that has a SPUserCodeWorkflowContext object as parameter. The Hashtable will give you the ability to return values in the workflow. The SPUserCodeWorkflowContext on the other hand is a ‘In’ parameter.  using System; using System.Collections.Generic; using System.Linq; using System.Text; using System.Collections; using Microsoft.SharePoint.UserCode; using Microsoft.SharePoint;  namespace Motion10.SharePoint.Sandbox.Designer {     public class SPCreateSubSiteAction     {         public Hashtable CreateSubSite(SPUserCodeWorkflowContext context)         {         }     } } In the above code example you see that I created a method called ‘CreateSubSite’ within this method you can add the logic of your custom action. In the upcoming examples we will create a action that creates a sub site.  To add the custom action to SharePoint you have to create a new elements file in which you have  to specify the custom action.  &lt;WorkflowActions&gt;   &lt;Action Name=\"Create Sub Site\"           SandboxedFunction=\"true\"           Assembly=\"$SharePoint.Project.AssemblyFullName$\"           ClassName=\"Motion10.SharePoint.Sandbox.Designer.SPCreateSubSiteAction\"           FunctionName=\"CreateSubSite\"           AppliesTo=\"all\"           Category=\"Motion10 Designer Actions\"&gt;   &lt;/Action&gt; &lt;/WorkflowActions&gt; To be able to add parameters to the method that you need to create a sub site we will have to define a ‘RuleDesigner’ section to enable users to fill in those parameters.  &lt;RuleDesigner Sentence=\"Create sub site with the title: %1, url: %2 and description: %3.\"&gt;   &lt;FieldBind Field=\"Title\" Text=\"Title\" Id=\"1\" DesignerType=\"TextBox\" /&gt;   &lt;FieldBind Field=\"Url\" Text=\"Url\" Id=\"2\" DesignerType=\"TextBox\" /&gt;   &lt;FieldBind Field=\"Description\" Text=\"Description\" Id=\"3\" DesignerType=\"TextBox\" /&gt; &lt;/RuleDesigner&gt; The ‘RuleDesigner’ section has to be placed within the ‘Action’ section. In the ‘Action’ section you also have to place a ‘Parameters’ section to pass the actual values in the workflow action.  &lt;Parameters&gt;   &lt;Parameter Name=\"__Context\" Type=\"Microsoft.SharePoint.WorkflowActions.WorkflowContext, Microsoft.SharePoint.WorkflowActions\" Direction=\"In\" DesignerType=\"Hide\" /&gt;   &lt;Parameter Name=\"Title\" Type=\"System.String, mscorlib\" Direction=\"In\" DesignerType=\"ParameterNames\" /&gt;   &lt;Parameter Name=\"Url\" Type=\"System.String, mscorlib\" Direction=\"In\" DesignerType=\"ParameterNames\" /&gt;   &lt;Parameter Name=\"Description\" Type=\"System.String, mscorlib\" Direction=\"In\" DesignerType=\"ParameterNames\" /&gt; &lt;/Parameters&gt; With all of this in place we can extend our method with the correct parameters and create a simple action to create a sub site.  public Hashtable CreateSubSite(SPUserCodeWorkflowContext context, string Title, string Url, string Description) {     //hashtable that will be returned can be used for return values.     Hashtable retVal = new Hashtable();      using (SPSite site = new SPSite(context.CurrentWebUrl))     {         using (SPWeb web = site.OpenWeb())         {             try             {                 using (SPWeb newWeb = web.Webs.Add(Url, Title, Title, 1033, web.WebTemplate, false, false))                 {                     SPWorkflow.CreateHistoryEvent(web, context.WorkflowInstanceId, 0, web.CurrentUser, TimeSpan.Zero, \"Information\", \"Sub site created from a Sandbox activity\", string.Empty);                 }             }             catch (Exception ex)             {                 SPWorkflow.CreateHistoryEvent(web, context.WorkflowInstanceId, 0, web.CurrentUser, TimeSpan.Zero, \"Error\", ex.Message,string.Empty );             }         }     }     return retVal; } Make sure you add the elements file to feature that is scoped to ‘Site’ and deploy the Sandbox solution. Open SharePoint Designer and you will see that you have the action available!  ","categories": ["Development"],
        "tags": ["Sandbox","SharePoint Designer"],
        "url": "/2011/06/creating-a-sharepoint-designer-action-for-the-sandbox/",
        "teaser": null
      },{
        "title": "Office 2010 and SharePoint 2010 Service Pack 1",
        "excerpt":" Two days ago Microsoft released the first Service Pack for the 2010 Office suite. This means there is also a Service Pack for SharePoint 2010.  In the Office service pack there are a lot of fixes and to much to list here. Here are some changes:   Outlook fixes an issue where “Snooze Time” would not reset between appointments. The default behavior for PowerPoint \"Use Presenter View\" option changed to display the slide show on the secondary monitor. Integrated community content in the Access Application Part Gallery. Better alignment between Project Server and SharePoint Server browser support. Improved backup / restore functionality for SharePoint Server The Word Web Application extends printing support to “Edit Mode.” Project Professional now synchronizes scheduled tasks with SharePoint task lists. Internet Explorer 9 “Native” support for Office Web Applications and SharePoint Office Web Applications Support for Chrome Inserting Charts into Excel Workbooks using Excel Web Application Support for searching PPSX files in Search Server  Visio Fixes scaling issues and arrowhead rendering errors with SVG export Proofing Tools improve spelling suggestions in Canadian English, French, Swedish and European Portuguese. Outlook Web Application Attachment Preview (with Exchange Online only) Office client suites using “Add Remove Programs” Control Panel, building on our work from Office 2007 SP2  Issues that are fixed for SharePoint 2010 can be read in the following excel sheet:   Service Pack 1 Changes to SharePoint 2010 – Microsoft  You can find the updates on the Microsoft update center:   Microsoft Update Center   ","categories": ["Administration"],
        "tags": ["Service Pack","SharePoint 2010"],
        "url": "/2011/07/office-2010-and-sharepoint-2010-service-pack-1/",
        "teaser": null
      },{
        "title": "Birth of my Beautiful son Tibbe Abbel van der Gaag",
        "excerpt":"   For the last couple of weeks it has been quiet on my blog.  It also has did not work for almost a week. This was all because of the birth of my son Tibbe.  He was born on 11 July and he is now 6 weeks and 3 days old.  In the following weeks I will be picking up a lot of work and  I will again be posting some interesting SharePoint stuff!!!!   ","categories": ["General"],
        "tags": ["Tibbe"],
        "url": "/2011/08/birth-of-my-beautiful-son-tibbe-abbel-van-der-gaag/",
        "teaser": null
      },{
        "title": "Visual Studio cannot debug managed applications because a kernel debugger is enabled on the system.",
        "excerpt":" I developed some awesome features for my SharePoint environment and to my Surprise they weren’t working that well.  So I thought lets attach it to the debugger and take a look what is going wrong. When I attached the debugger I got the following error message:  “Visual Studio cannot debug managed applications because a kernel debugger is enabled on the system.”  After a couple of hours investigating the problem I found a strange behavior on the start up of my development machine it was starting up in some kind of debug mode. To check if you machine is running in debug mode do the following:   Start – Run – msconfig Select the boot tab and click on advanced options. If the debug check box is marked it is running in a debug mode.    Unmark the checkbox and you will be able to debug your code.   ","categories": ["Administration"],
        "tags": ["Kernel debugger","Visual Studio"],
        "url": "/2011/08/visual-studio-cannot-debug-managed-applications-because-a-kernel-debugger-is-enabled-on-the-system/",
        "teaser": null
      },{
        "title": "Deleting the extended web application",
        "excerpt":" This post I’m as a reminder because a few days back when I was working on a production environment for one of our clients. I had created an extended web application and needed to remove it. In SharePoint 2007 I have done this several times but in 2010 I had to take a good look.  To delete the extended web application you need to do the following:  &nbsp;    Start Central Administration. Go to Manage Web Applications. Select the web application you extended. Click on the Arrow beneath Delete and select Remove SharePoint from IIS Web Site. Select the Web site. Finally click OK   ","categories": ["Administration","SharePoint"],
        "tags": ["Extended Web Application","SharePoint 2010"],
        "url": "/2011/09/deleting-the-extended-web-application/",
        "teaser": null
      },{
        "title": "Slow file transfer from Windows Explorer view to another Windows Explorer view of SharePoint.",
        "excerpt":" Today I was replacing certain SharePoint documents for one of the projects we are working on. I noticed that the transfer of the document from Windows Explorer View to Windows Explorer view was very slow!! I mean 16,1 kb per Second slow!!!.  Investigating this problem I found that IE is configured to “Automatically detect settings”:    This was causing the WebDAV connection to work very slow. Disabling this obtain automatically made my transfer much faster. You can find this option by doing the following:   In IE open the tools menu and click on Internet options. Click on the connections tab. Click on the Lan settings button. Uncheck the Checkbox and press ‘Ok’ to save the settings.   ","categories": ["Administration","SharePoint"],
        "tags": ["File transfer","WebDAV","Windows Explorer"],
        "url": "/2011/09/slow-file-transfer-from-windows-explorer-view-to-another-windows-explorer-view-of-sharepoint/",
        "teaser": null
      },{
        "title": "Deleting a Project Service Application within SharePoint 2010",
        "excerpt":" A couple of days ago I deleted my Project Server Web application because I wanted to recreate it.  When I tried to create a new Project Web App Site I got the following messages:   The database specified is already used by another project server. Enter a different server or database name and try again.    The first idea that I came up with was deleting the databases from the database server. After deleting the databases I retried my action but the problem was still there.  The next thought that came up in my head was deleting the Project Service Application. I pressed delete on the Manage Service Application Page and got the following error message:   Service application cannot be deleted due to existing project sites.    The problem is with the Project Service Application. The Project service application remains a connection with a specific site collection even If you delete the web application or the specific site collection. The only thing you can do to delete the connected site collection (that does not exist anymore) from the Project Service Application by using PowerShell.  First you will have to use PowerShell to retrieve a collection of the sites that are referenced with the service application. Get the ID of the reference you want to remove and than remove that reference .  Let take the first step, get the service application and print the site collections.  $svApp = Get-SPServiceApplication -Name \"Project Web App Service Application\" $svApp.SiteCollection   This will get a complete list of all the associated site collections:    Next you can copy the ID and remove it by using the following PowerShell command:  $svApp.SiteCollection.Remove(\"a09c6964-efc2-4917-9448-ce86d0bc95d3\") And now you can remove the Service Application or recreate the site collection. To remove the service application by using powershell use the following command:  Remove-SPServiceApplication $svApp -RemoveData  ","categories": ["Administration"],
        "tags": ["Delete","Project server","SharePoint 2010"],
        "url": "/2011/11/deleting-a-project-service-application-within-sharepoint-2010/",
        "teaser": null
      },{
        "title": "Using the ConditionedActivityGroup in Workflow Foundation 3.5",
        "excerpt":" When you create a SharePoint approval workflow the While activity is often used to check if a Task is approved by placing the “OnTaskChanged” activity within the While Activity.    When you also want to check if the Moderation status on the workflow item is approved you can’t do this in the same While activity.  If you only check the task status there will be situation in which the item is approved and the approval workflow is still running because the item was approved directly. If you would like to check de moderation status of an item and the task status you can use a “ConditionedActivityGroup”.  The “ConditionedActivityGroup” will cause the workflow to continue when the moderation status is changed or when the status of the task is changed. To use the “ConditionedActivityGroup” drag the activity within the workflow designer.    Next drop the activities you want to check within the small window at the top that states “Drop Activities Here”. For our example we will use the “OnTaskChanged” and the “OnWorkflowItemChanged” activities.    Open the workflow in code view and add three boolean properties:   IsFinished EnableItemChanged EnableTaskChanged  public bool IsFinished { get; set; }  public bool EnableItemChanged { get; set; }  public bool EnableTaskChanged { get; set; } These properties will be used to activate the conditions within the condition group. The “IsFinshed” property will be used to finish the conditioned group. To activate the conditions we will also have to set the “EnableItemChanged” and “EnableTaskChanged” properties to true within the initialization of the workflow.  /// &lt;summary&gt; /// Initializes a new instance of the worfklow. /// &lt;/summary&gt; public Workflow1() {     InitializeComponent();     this.EnableItemChanged = true;     this.EnableTaskChanged = true; } &nbsp;  After writing those lines of code open the workflow designer and select the “ConditionedActivityGroup” and change the “UntilCondition” property of the activity to: Declarative Rule Condition.  Select the “…” at the end of the “ConditionName” property and create a new “Condition”. The value of the condition has to be “this.IsFinished”.    This will mean that the ConditionedActivityGroup is finished when “IsFinished” equals true.  The next step is to set the “WhenCondition” on the activities that are in the group activity. These properties we also be a “Declarative Rule Condition” and set the condition values to “this.EnableTaskChanged” for the task activity and “this.EnableItemChanged” for the item activity.  Besides this we will also set the “Invoked” property of the activities to perform a code action when the event take place (You can also let Visual Studio do this by double clicking the activities).  &nbsp;    &nbsp;  With all this in place when can start to write some code for the invoke methods. Because the “WhenCondition” mean that when they are set to true these conditions can occur. We will have to set these properties to true each time a Condition is met because the Workflow instance will automatically set them to false. In our example this will occur when the task status is set to approved or the item is approved.  public const string FieldId = \"YourFieldID\";  /// &lt;summary&gt; /// Tasks the item changed. /// &lt;/summary&gt; /// &lt;param name=\"sender\"&gt;The sender.&lt;/param&gt; /// &lt;param name=\"e\"&gt;The &lt;see cref=\"System.Workflow.Activities.ExternalDataEventArgs\"/&gt; instance containing the event data.&lt;/param&gt; private void TaskItemChanged(object sender, ExternalDataEventArgs e) {     //retrieve the task and check its status     SPListItem task = workflowProperties.TaskList.GetItemById(CreateTaskActivity.ListItemId);     string status = task[YourFieldID] != null ? task[YourFieldID].ToString() : string.Empty;      if (status.ToUpperInvariant() != \"PENDING\") {         IsFinished = true;         EnableTaskChanged = false;     } else {         IsFinished = false;         EnableTaskChanged = true;     } }  /// &lt;summary&gt; /// Workflows the item changed. /// &lt;/summary&gt; /// &lt;param name=\"sender\"&gt;The sender.&lt;/param&gt; /// &lt;param name=\"e\"&gt;The &lt;see cref=\"System.Workflow.Activities.ExternalDataEventArgs\"/&gt; instance containing the event data.&lt;/param&gt; private void WorkflowItemChanged(object sender, ExternalDataEventArgs e) {     //View the moderation status of the workflow item     SPModerationStatusType type = workflowProperties.Item.ModerationInformation.Status;     Status = type.ToString();     if (type == SPModerationStatusType.Approved || type == SPModerationStatusType.Denied) {         IsFinished = true;         EnableItemChanged = false;     } else {         IsFinished = false;         EnableItemChanged = true;     } ","categories": ["Development"],
        "tags": ["ActivityGroup","Workflow","WWF 3.5"],
        "url": "/2011/11/using-the-conditionedactivitygroup-in-workflow-foundation-3-5/",
        "teaser": null
      },{
        "title": "System.ServiceModel.FaultException: The server was unable to process the request due to an internal error.",
        "excerpt":" For one of our clients we are working with a claims based web application. This web application has a custom membership provider that is registered at the web application, central administration and security token service level.  When we navigate to the site everything works perfectly but when we try to login with valid credentials we would receive an error like below:  Server Error in '/' Application.      &nbsp;  Runtime Error Description: An application error occurred on the server. The current custom error  The server was unable to process the request due to an internal error.  For more information about the error, either turn on IncludeExceptionDetailInFaults (either from ServiceBehaviorAttribute or from the &lt;serviceDebug&gt; configuration behavior) on the server in order to send the exception information back to the client, or turn on tracing as per the Microsoft .NET Framework 3.0 SDK documentation and inspect the server trace logs.  Description: An unhandled exception occurred during the execution of the current web request. Please review the stack trace for more information about the error and where it originated in the code.  Exception Details: System.ServiceModel.FaultException: The server was unable to process the request due to an internal error.  For more information about the error, either turn on IncludeExceptionDetailInFaults (either from ServiceBehaviorAttribute or from the &lt;serviceDebug&gt; configuration behavior) on the server in order to send the exception information back to the client, or turn on tracing as per the Microsoft .NET Framework 3.0 SDK documentation and inspect the server trace logs.  Source Error:  An unhandled exception was generated during the execution of the current web request. Information regarding the origin and location of the exception can be identified using the exception stack trace below.  Stack Trace:  [FaultException: The server was unable to process the request due to an internal error.  For more information about the error, either turn on IncludeExceptionDetailInFaults (either from ServiceBehaviorAttribute or from the &lt;serviceDebug&gt; configuration behavior) on the server in order to send the exception information back to the client, or turn on tracing as per the Microsoft .NET Framework 3.0 SDK documentation and inspect the server trace logs.]  Microsoft.IdentityModel.Protocols.WSTrust.WSTrustChannel.ReadResponse(Message response) +1161205  Microsoft.IdentityModel.Protocols.WSTrust.WSTrustChannel.Issue(RequestSecurityToken rst, RequestSecurityTokenResponse&amp; rstr) +73  Microsoft.IdentityModel.Protocols.WSTrust.WSTrustChannel.Issue(RequestSecurityToken rst) +36  Microsoft.SharePoint.SPSecurityContext.SecurityTokenForContext(Uri context, Boolean bearerToken, SecurityToken onBehalfOf, SecurityToken actAs, SecurityToken delegateTo) +26405809  Microsoft.SharePoint.SPSecurityContext.SecurityTokenForFormsAuthentication(Uri context, String membershipProviderName, String roleProviderName, String username, String password) +26406316  Microsoft.SharePoint.IdentityModel.Pages.FormsSignInPage.GetSecurityToken(Login formsSignInControl) +188  Microsoft.SharePoint.IdentityModel.Pages.FormsSignInPage.AuthenticateEventHandler(Object sender, AuthenticateEventArgs formAuthenticateEvent) +123  System.Web.UI.WebControls.Login.AttemptLogin() +152  System.Web.UI.WebControls.Login.OnBubbleEvent(Object source, EventArgs e) +124  System.Web.UI.Control.RaiseBubbleEvent(Object source, EventArgs args) +70  System.Web.UI.Page.RaisePostBackEvent(IPostBackEventHandler sourceControl, String eventArgument) +29  System.Web.UI.Page.ProcessRequestMain(Boolean includeStagesBeforeAsyncPoint, Boolean includeStagesAfterAsyncPoint) +2981  &nbsp;   In our situation this error occurs within in the “Security Token Service Application”. The problem with this error message is that it isn’t the exact message of what is going wrong. To receive that error message that is occurring do the following:   Open IIS and navigate to “SharePoint Web Services” under “Sites”. Click the node open and select “SecurityTokenServiceApplication” use your other mouse button to open the context menu and select “Explore”.  Windows explorer will open a new window. In this window you will see a web.config file. Open this file within a text editor and look for the following section:  &lt;behaviors&gt;   &lt;serviceBehaviors&gt;     &lt;behavior name=\"SecurityTokenServiceBehavior\"&gt;       &lt;!-- The serviceMetadata behavior allows one to enable metadata (endpoints, bindings, services) publishing.            This configuration enables publishing of such data over HTTP GET.            This does not include metadata about the STS itself such as Claim Types, Keys and other elements to establish a trust.       --&gt;       &lt;serviceMetadata httpGetEnabled=\"true\" /&gt;       &lt;!-- Default WCF throttling limits are too low --&gt;       &lt;serviceThrottling maxConcurrentCalls=\"65536\" maxConcurrentSessions=\"65536\" maxConcurrentInstances=\"65536\" /&gt;              &lt;/behavior&gt;   &lt;/serviceBehaviors&gt; &lt;/behaviors&gt; In this section we will place a extra debug tag to include the errors that are occurring. The tag looks like this:  &lt;serviceDebug httpHelpPageEnabled=\"true\" includeExceptionDetailInFaults=\"true\" /&gt; This tag needs to be placed in the behavior tag. After you place the tag in the section it will look like this:  &lt;behaviors&gt;   &lt;serviceBehaviors&gt;     &lt;behavior name=\"SecurityTokenServiceBehavior\"&gt;       &lt;!-- The serviceMetadata behavior allows one to enable metadata (endpoints, bindings, services) publishing.            This configuration enables publishing of such data over HTTP GET.            This does not include metadata about the STS itself such as Claim Types, Keys and other elements to establish a trust.       --&gt;       &lt;serviceMetadata httpGetEnabled=\"true\" /&gt;       &lt;!-- Default WCF throttling limits are too low --&gt;       &lt;serviceThrottling maxConcurrentCalls=\"65536\" maxConcurrentSessions=\"65536\" maxConcurrentInstances=\"65536\" /&gt;       &lt;serviceDebug httpHelpPageEnabled=\"true\" includeExceptionDetailInFaults=\"true\" /&gt;     &lt;/behavior&gt;   &lt;/serviceBehaviors&gt; &lt;/behaviors&gt; After changing the “web.config” we received the correct message and we saw that we mistyped the role provider name.  Server Error in '/' Application.   &nbsp;   &nbsp;  Cannot get Role Manager with name Empty. The role manager for this process was not properly configured. You must configure the role manager in the .config file for every SharePoint process.   ","categories": ["Administration"],
        "tags": ["FaultException","Troubleshouting"],
        "url": "/2011/11/system-servicemodel-faultexception-the-server-was-unable-to-process-the-request-due-to-an-internal-error/",
        "teaser": null
      },{
        "title": "Windows PowerShell command builder",
        "excerpt":" Today I found something interesting on the internet which I think can help everyone who is working with SharePoint. Microsoft has built a Silverlight application that can be used to build PowerShell commands.       Microsoft says:  the Windows PowerShell Command Builder enables IT professionals and power users to visually assemble commands related to SharePoint 2010 Products and Office 365 on a Design Surface in a browser and take those commands to their respective products. The Windows PowerShell Command Builder provides an intelligent user experience. After you drag a verb or noun object on the Design Surface, the interface will hide either the verbs or nouns that are not associated with the verb or noun placed on the Design Surface. After you construct a command, you can copy the command to Windows PowerShell script, the SharePoint 2010 Management Shell, or other desired location to be saved or executed.  You use the tool by clicking here. You can also install it on your desktop by right clicking on the application.  Microsoft also released some guides to get started with the tool:   http://www.microsoft.com/download/en/details.aspx?id=27588    ","categories": ["Development"],
        "tags": ["Developer Tools","Powershell"],
        "url": "/2011/12/windows-powershell-command-builder/",
        "teaser": null
      },{
        "title": "Microsoft Certified Technology Specialist (MCTS): Microsoft .NET Framework 3.5, Windows Workflow Foundation Applications",
        "excerpt":" 18 months ago I decided to become Microsoft Certified Technology Specialist (MCTS): Microsoft .NET Framework 3.5, Windows Workflow Foundation Applications. I bought a book (Windows Workflow Foundation Step by Step) and started reading. After reading about 150 pages I was to busy to continue reading.  Three months ago I started reading the book again and I finally finished it just before the end of 2011.  Today I had my exam to my surprise I had to answer 70 questions were my past exams had only 50 questions. It was a really tough exam. I needed 140 minutes to finish the exam.  But I passed the exam this is really one to be proud off! If you want to try this exam I really recommend you (when you read the some book) to look at the differences between WWF 3.0 and 3.5. Beside that I also recommend you to take this training (6462A: Visual Studio 2008: Windows Workflow Foundation (2 Days)) which one I did not attend.     ","categories": ["Development"],
        "tags": ["Certificates","MCTS","WWF 3.5"],
        "url": "/2012/01/microsoft-certified-technology-specialist-mcts-microsoft-net-framework-3-5-windows-workflow-foundation-applications/",
        "teaser": null
      },{
        "title": "Review: Microsoft SharePoint 2010 Development Cookbook",
        "excerpt":" Recently I have been asked to review a new SharePoint 2010 book named “Microsoft SharePoint 2010 Development Cookbook” written by Ed Musters.  When I received the book I expected a book with a lot of recipes to get things done in SharePoint. The sub-title off the book “Over 45 recipes to take you from beginner to professional in SharePoint Development” made me expect to really learn how things are done within SharePoint.  The book starts off with a recipe to create a SharePoint development environment. The way this is done in the book is not that representative to a real world scenario. Installing everything on the same account van cause serious issues when you are working within DTAP environments.  The rest of the book consists off all kind of different recipes that you can use separately. The main problem that I had with the recipes was that they don’t go very deep into the subject. This makes the sub-title from beginner to professional not really true.  It was ok to read this book but I do not recommend it!!   ","categories": ["General"],
        "tags": ["Book","Review","SharePoint 2010"],
        "url": "/2012/02/review-microsoft-sharepoint-2010-development-cookbook/",
        "teaser": null
      },{
        "title": "What's New for SharePoint Development in Visual Studio 2012",
        "excerpt":" Now that the Visual Studio 2012 Release Candidate is released (31 may) it is the question, what kind of new features there are in Visual Studio 2012 when you are looking at SharePoint Development.  Most of the time the release candidates that Microsoft releases contain all of the features that should be in the Final Release.  Some of the new features to facilitate SharePoint Development are:   Create Lists and Content Types by Using New Designers Create Site Columns Create Silverlight Web Parts Publish SharePoint Solutions to Remote SharePoint Servers Test SharePoint Performance by Using Profiling Tools Create Sandboxed Visual Web Parts Improved Support for Sandboxed Solutions. Support for JavaScript Debugging and IntelliSense for JavaScript Streamlined SharePoint Project Templates Test Your Code by Using Microsoft Fakes Framework  You can read all about it on this MSDN page:  http://msdn.microsoft.com/en-us/library/ee290856(v=vs.110).aspx   ","categories": ["Development"],
        "tags": ["SharePoint Development","Visual Studio 2012"],
        "url": "/2012/06/whats-new-for-sharepoint-development-in-visual-studio-2012/",
        "teaser": null
      },{
        "title": "Install SharePoint 2013 Public Beta on Windows Server 2012 RC &ndash; Part I &ndash; Installation of Windows Server 2012 RC",
        "excerpt":" With the new version of SharePoint almost in Public Beta (I have hopes it will be this month) it is time to show you how you can install it on the new Release Candidate of Windows Server 2012.  Since the new version of SharePoint isn’t there yet this series of blog post will begin with installing Windows Server 2012 RC on a Virtual Machine. After this post the following post will follow:   Part II – Installation of Active Directory and DNS Services on Windows Server 2012 RC Part III – Installation of SQL Server 2012 on Windows Server 2012 RC Part IV – Installation of the Prerequisites on Windows Server 2012 RC Part V – Installation of SharePoint 2013 on Windows Server 2012 RC Part VI – Configuring SharePoint 2013 on Windows Server 2012 RC  So let us begin with the installation of Windows Server 2012 RC. First off make sure you have a ISO available of Windows Server 2012 RC. If you don not have the image already download it from the Microsoft site:  http://www.microsoft.com/en-us/server-cloud/windows-server/2012-default.aspx  When you have downloaded the image you can create a new virtual machine and assign a appropriate amount of memory. I have created my VM with 6 GB of memory.    Attach the image file to the virtual machine and then boot it. It will automatically boot the installation wizard of Windows Server 2012 with the language selection screen.    With the right language selected click ‘Next’  to proceed to the next step. The next step is easiest step of the installation. You only have to press the button “Install now”.    When “Install now” is pressed the installation medium will load the actual setup files. In the mean time it will show you a dark blue screen with the waiting mouse cursor and the text “Setup is Starting”.    When all the files are loaded it is time to begin the actual setup of Windows. Basically the complete setup really look like the installation of Windows Server 2008 R2 so I think you can guess what is coming next.    Yep…. It is the Operating System selection screen. The only version available at the moment is the Windows Server 2012 Release Candidate Datacenter edition. You still have to take a good look because the installation disc also has the Server Core Installation. Choose the “ Server with a GUI” option and click next. Accept the license and again click next to go to the installation type screen.    Like the installation of Windows 7 and Windows Server 2008 R2 you have the option to upgrade your current Operation System. Since this is a complete new installation we choose for “Custom: Install Windows Only (Advanced)”.  In the next screen you will be asked to select the disk you would like to use for your installation. Select the right disk and click on “Next” .    After you selected the disk Windows will start installing the files to the specified disk.    When the installation is done the server will reboot and starts preparing Windows. Just like the old version you then will be asked to give in your Administrator password.    When you click the “Finish” the setup will finalize. After the finalization you will see the new ctrl+alt+delete screen.    Press ctrl+alt+delete and login to the server the desktop will be loaded and you are ready to start installing new windows component with the new Server Manager.    Like described at the top of the post this is the first post in a series of 6 posts. In the next post I will show you how to install the Active Directory component and the DNS component on Windows Server 2012 RC.   ","categories": ["Administration"],
        "tags": ["Beta","SharePoint 2013","Windows Server 2012"],
        "url": "/2012/06/install-sharepoint-2013-public-beta-on-windows-server-2012-rc-part-i-installation-of-windows-server-2012-rc/",
        "teaser": null
      },{
        "title": "Install SharePoint 2013 Public Beta on Windows Server 2012 RC &ndash; Part II &ndash; Installation of Active Directory and DNS Services on Windows Server 2012 RC",
        "excerpt":" 6With the new version of SharePoint almost in Public Beta (I have hopes it will be this month) it is time to show how you can install it on the new Release Candidate of Windows Server 2012.  Since the new version of SharePoint isn’t there yet this series of blog post will continue with Installation of Active Directory and DNS Services on Windows Serve 2012 RC. After this post the following post will follow:   Part III – Installation of SQL Server 2012 on Windows Server 2012 RC Part IV – Installation of the Prerequisites on Windows Server 2012 RC Part V – Installation of SharePoint 2013 on Windows Server 2012 RC Part VI – Configuring SharePoint 2013 on Windows Server 2012 RC  So let us begin with the activation of the Active Directory role on Windows Server 2012 RC. So start up your Virtual Machine and log in to Windows Server 2012.  When continuing from the previous blog post the “Server Manager” will be opened. If it does not open it.    Since we did not rename the machine in the previous post we will start by renaming the server. Select “Local Server”  within the server manager and click on your existing “Computer Name”. This will open  an new window.    In the new window click on the “Change”  button to change the computer name. Another window will open that gives you the option to fill in your computer name. For my virtual machine I choose “DEV_BETA”.    With the new name filled in click on “OK” to close the window. You will then get a warning message that you must restart your computer  to apply the changes. Click “OK”  again.    When you virtual machine is restarted open the server manager and select “Local Server”. We will now add a static IP address to the server for the DNS. Click on “IPv4 address assigned” next to “ Ethernet” to open the “Network Connections” screen. On the network connections screen select the internal network card with your second mouse button and open the properties.  For this demo environment we will disable the IPv6 option and alter the IP address of the IPv4 option.    It does not matter to which address you change your local IP address. Close all off the windows and return to the Server Manager.  On the right top side of the “Server Manager”  you will see an option called “Manage”  click on this option and then select “Add Roles and Features”.    The “Add Roles and Features Wizard” will start up with the Splash screen on which you can select “Next”.    In the next step you will be offered the choice to select the type off installation you want to perform. This different types give you the option to install on a running machine, virtual machine or on a offline virtual hard disk. Since we will be installing it on the local machine select the option “Role-based or feature-based installation” and click “Next”.    Another new step in the wizard is the Server Selection step. This step offers you the option to select a Running machine of Virtual Hard disk to install on. We will just select our current server from the server pool and continue with the wizard.    The next step finally offers us the possibility to select the roles we want to install. Select the “Active Directory Domain Services” role and the “DNS Server”  role. They will both give us a warning informing that they need to install required features.       On both the warning messages click “Add Features” and continue to the “Features” step.  At the moment we do not have to have any features so we will just click “Next”.    After the features step we will get two steps informing us about the Active Directory Domain Services and the DNS Server. On both steps click “Next” to proceed to the installation.      On the “Confirmation” screen click “Install” to start the installation. Wait till installation is finished to close the “Add Roles and Features Wizard”.    Now that the roles are installed we still need to configure active directory. With the wizard closed you will be back at the “Server Manager”. The Server Manager now haves a few more menu options on the left side of the screen. Select “AD DS” to start with the configuration of AD.    At the top of the screen you will see a yellow status bar stating that you have to configure Active Directory. At the end of the status bar click more to open the details.    The message that you will find states that you will have to perform the post-deployment configuration. You can start this configuration by selecting the action on the same row called: “Promote this server to a domain”.  With this action clicked the “Active Directory Domain Services Configuration Wizard” will start in which we will configure the domain.    The first step offers the option to join a existing forest or to create a new forest. Since this is the first server we will have to create a new forest and insert a root domain name for our domain. Lets call it dev.local.    In the next step we will have to choose a domain functional level and forest function level. We will set this option to Windows Server 2012 Release Candidate. We set this to the new version because it will give us the newest advanced features within the domain.  Fill in the password you would like to use for the Directory Restore Mode to proceed to the next step. This step gives the option to set up DNS delegation. Since we will not use this option leave it to default and click “Next”.    Depending on the root domain name you chose three steps back the wizard will try to generate a NetBIOS name. Check if the name is correct. In my installation it generated the NetBIOS name “DEV” which is correct for me. If the name isn’t correct, you can change it.    Next up is setting the location where Active Directory will save his files. It will save files for:   The Active Directory Database The Active Directory Log Files The Active Directory SYSVOL (The SYSVOL folder is a public folder that will contain the public system files for the domain).  For now we can leave these locations to the default and continue to the review step.    On the review step check the filled in information. If everything is correct click “Next” to start the configuration of your Domain services.    Before the configuration of the domain services starts it will run a prerequisites step. Check if there aren’t any significant warnings / errors and if there are not any continue by clicking “Install”.    After the installation the server will restart and you will have to login to the Domain. You can do this by using the same administrator password as before. But you will now be a Domain Administrator.    To alter the Active Directory Users or Computer do the following:   Go to the Start Menu by using the Windows button or by going to the low left corner with your mouse. With the Start Menu open select “Active Directory Users and Computers”.  or   Open the Server Manager Select “Tools” on the Tap bar. Select “Active Directory Users and Computers” from the “Tools” menu.    To alter the DNS configuration do the following:   Go to the Start Menu by using the Windows button or by going to the low left corner with your mouse. With the Start Menu open select “DNS”.  or   Open the Server Manager Select “Tools” on the Tap bar. Select “DNS” from the “Tools” menu.    This was the second post in a series of 6 posts. In the next post I will show you how to install SQL Server 2012 on Windows Server 2012 RC.   ","categories": ["Administration"],
        "tags": ["Active Directory","SharePoint 2013","Windows Server 2012"],
        "url": "/2012/06/install-sharepoint-2013-public-beta-on-windows-server-2012-rc-part-ii-installation-of-active-directory-and-dns-services-on-windows-server-2012-rc/",
        "teaser": null
      },{
        "title": "Do not start Server Manager automatically at Logon &ndash; Windows Server 2012",
        "excerpt":" After the installation of the new Windows Server version (Windows Server 2012 RC) it was annoying for me that the “Server Manager” started every time when I logged in to the machine.  In Windows Server 2008 R2 you could disable the auto start option from the home screen. I started searching for the option but couldn’t find it. After half an hour  I found the solution:   Open the “Server Manager”. Select “Manage” on the top bar. Select “Server Manager Properties” from the drop down menu. Select the option “Do not start Server Manager automatically at logon”.     ","categories": ["Administration"],
        "tags": ["Server Manager","Windows Server 2012"],
        "url": "/2012/06/do-not-start-server-manager-automatically-at-logon-windows-server-2012/",
        "teaser": null
      },{
        "title": "Microsoft announces Yammer take over.",
        "excerpt":" Microsoft has confirmed that it will take over Yammer for 1,2 milliard dollar. Nice to know it that Yammer will be part of the Microsoft Office department.  Since Yammer already supports a integration with SharePoint we will just have to wait and see if it will be integrated in SharePoint 2013 (SharePoint 15).  Yammer also send a e-mail to all his users informing them about the acquiring by Microsoft.     Dear Yammer Customer,  I am pleased to announce that Yammer has signed a definitive agreement to be acquired by Microsoft. After the close of the deal, Microsoft will continue to invest in Yammer's freemium, stand-alone service, and the team will remain under my direction within the Microsoft Office Division. With the backing of Microsoft, our aim is to massively accelerate our vision to change the way work gets done with software that is built for the enterprise and loved by users.  As a Yammer customer, you will continue to get a secure, private social network—delivered with the same focus on simplicity, innovation, and cross-platform experiences. Over time, you’ll see more and more connections to SharePoint, Office365, Dynamics and Skype. Yammer’s expertise in empowering employees, driving adoption, and delivering rapid innovation in the cloud will not only continue to power our stand-alone service, but also anchor the communication and collaboration experiences in Office 365.  You can find more information in this press release and our blog post.  Sincerely,  David Sacks  Yammer CEO and Founder     ","categories": ["Microsoft Office","SharePoint"],
        "tags": ["Microsoft","Take Over","Yammer"],
        "url": "/2012/06/microsoft-announces-yammer-take-over/",
        "teaser": null
      },{
        "title": "Install SharePoint 2013 Public Beta on Windows Server 2012 RC &ndash; Part III &ndash; Installation of SQL Server 2012 on Windows Server 2012 RC",
        "excerpt":" With the new version of SharePoint almost in Public Beta (I have hopes it will be this month) it is time to show how you can install it on the Release Candidate of Windows Server 2012.  Since the new version of SharePoint isn’t there yet this series of blog post will continue with the Installation of SQL Server 2012 on Windows Serve 2012 RC. After this post the following post will follow:   Part IV – Installation of the Prerequisites on Windows Server 2012 RC Part V – Installation of SharePoint 2013 on Windows Server 2012 RC Part VI – Configuring SharePoint 2013 on Windows Server 2012 RC  So let us begin with booting up the Virtual Machine we got this far and add the ISO of SQL Server 2012 to the machine.  When continuing from the previous blog post the “Server Manager” will be opened. Close the Server Manager and open “Active Directory Users and Computers” to create the service accounts we need for the installation of SQL Server:  To open Active Directory Users or Computer do the following:   Go to the Start Menu by using the Windows button or by going to the low left corner with your mouse. With the Start Menu open select “Active Directory Users and Computers”.  Open the Users OU and use your other mouse button to add users:    When you have the “New Object - User” window open create the following users:     First name Last name User Logon name User logon name   SQL DB Engine svc_sqldbengine svc_sqldbengine   SQL Reporting svc_sqlreporting svc_sqlreporting   SQL Agent svc_sqlagent svc_sqlagent    &nbsp;  Make sure you select the options:   User cannot change password. Password never expires.  This will make sure the password of the accounts will not be reset or be disabled.    When the accounts are created it is time to start the installation of SQL server. Run the Setup wizard that's on the SQL Server 2012 CD / DVD.  When you run the setup the SQL Server Installation Center will open. Press the Installation option on the left and then select “New SQL Server stand-alone installation or add feature to an existing installation”.            The installation setup will start with running a few startup rules. When you have passed all of the rules click Ok to continue with the installation.    A new window will open and ask for your product key. If your product key hasn’t been filled in yet fill it in and continue by clicking next. On the next step accept the license terms and click next again.    The wizard will now start installing the required files for the installation and will start up another wizard. This wizard will automatically check the conditions of the server.    Most of the the the wizard will give two warnings:   Computer domain controller: You can ignore this warning because we will use this pc as a development pc. For production servers never install SQL on a domain controller this is for security reasons. Windows Firewall: This warning states that the Firewall is blocking SQL server. Because the machine will be a one machine farm we can ignore this warning. When you are creating a farm with multiple pc’s you will have to open the SQL server port. By default this is: 1433.  Because we can ignore the warnings we will click next and proceed to the next step. The next step will give us the option to select the installation type. You will have three options:   SQL Server Feature Installation: This option will give you the possibility to install specific features of SQL Server. SQL Server PowerPivot for SharePoint: This option will give you the possibility to install PowerPivot for SharePoint. All Features with Defaults.    We will select the first option: “SQL Server Feature Installation” because we want to install SQL server and select the features we want. We do not want to install everything because this can downgrade the performance of our virtual machine.  In the next step we can select the features we want to install. We will select the following features:   Database Engine Services.  SQL Server Replication Full-Text and Semantic Extractions for Search Data Quality Services   Management Tools – Basic  Management Tools – Complete    At the moment we will not install “Reporting Services Add-in for SharePoint Products” because we do not have a instance running of SharePoint server. Click next to continue with the installation.    The wizard will perform a couple of checks based on the features you have selected. If you pass all of the Rules click next to continue to the Instance Configuration. On the instance configuration screen you have the option to change the name of the SQL Server Instance. We will keep the default settings and click “next”  to continue.    The Disk Space Requirements Step will show you if you have enough hard drive space to install SQL server if you pass this step click next to continue. This will bring us to the “Server Configuration” step. In this step we will have to fill in the accounts and passwords for the services.       Fill in the accounts that we have created in the first step:     Service Account Name   SQL Server Agent svc_sqlagent   SQL Server Database Engine svc_sqldbengine    &nbsp;  When you proceed you will have the option to select the authentication mode off SQL Server and were the data files are saved. For now we will leave it to the default settings and we will only fill in the SQL Server Administrators. I always fill in the BUILTIN\\Administrators group but because were are installing it on a domain controller it cannot find this group that’s why we will just add the current user.    Now that we have configured these last steps we are almost finished with the installation. The next step can be ignored and you can simple click next. The wizard will again check a few rules, when you pass these rules click next to go to the confirmation screen.       In the confirmation window check every setting and click “Install”  to start the installation.      When the installation is complete you can close the wizard. You can then op SQL Server management studio by following these steps:   Go to the Start Menu by using the Windows button or by going to the low left corner with your mouse. With the Start Menu open select “SQL Server Management Studio”.    With SQL Server installed we can start the Installation of SharePoint 2013. We will start this fourth part of the blog series when the SharePoint 2013 Public Beta is released.   ","categories": ["Administration"],
        "tags": ["Installation","SharePoint 2013","SQL Server 2012","Windows Server 2012"],
        "url": "/2012/07/install-sharepoint-2013-public-beta-on-windows-server-2012-rc-part-iii-installation-of-sql-server-2012-on-windows-server-2012-rc/",
        "teaser": null
      },{
        "title": "Holiday &amp; SharePoint 2013 Public Beta",
        "excerpt":" Due to my holiday in Argelès-sur-Mer in the South of France on which I enjoyed  a week with no Computer and no Internet. Believe it or not !!!!  During that week Microsoft had the nerve to release the SharePoint 2013 Public Beta. I can’t wait to get back in the office on Monday and start with the installation of the Public Beta to finish up the series of blog posts I have started on the Installation of SharePoint 2013 Public Beta on the Windows Server 2012 RC (The first one can be found here).  If you want to download the Public Beta follow these links:  Download SharePoint 2013 Foundation:  http://www.microsoft.com/en-us/download/details.aspx?id=30345  SharePoint Server 2013:  http://technet.microsoft.com/en-US/evalcenter/hh973397.aspx?wt.mc_id=TEC_121_1_33  Other SharePoint 2013 content including:   Microsoft Office Web Apps Server Preview SharePoint Designer 2013 Duet™ Enterprise for Microsoft SharePoint 2013 and SAP Language packs Architecture documents Lab guides  http://www.microsoft.com/en-us/download/search.aspx?q=SharePoint%202013&amp;p=1&amp;r=40&amp;t=32   ","categories": ["Administration"],
        "tags": ["Public Beta","SharePoint 2013"],
        "url": "/2012/07/holiday-sharepoint-2013-public-beta/",
        "teaser": null
      },{
        "title": "Install SharePoint 2013 Public Beta on Windows Server 2012 RC &ndash; Part IV &ndash; Installation of the Prerequisites on Windows Server 2012 RC",
        "excerpt":" With the new version of SharePoint in Public Beta (links). It is time to start with the installation of the prerequisites.  When you do not already have the image of the SharePoint 2013 Public Beta download it. The link for the download can be found in my previous post.  This post will point to the installation of the prerequisites for SharePoint 2013. The prerequisites that we will install is a bit more than the prerequisites that are on the image off the SharePoint 2013 Preview.  Because the machine will be a dev/test machine we will do a lot of research on it so we will also install the following:   ILSPY: http://wiki.sharpdevelop.net/ILSpy.ashx we will use this to look into some SharePoint dll’s. Microsoft Office Professional Plus 2013 Preview: You can download this from MSDN http://msdn.microsoft.com/nl-nl/subscriptions/downloads/default(en-us).aspx. Microsoft Office Visio 2013 Preview: You can download this from MSDN http://msdn.microsoft.com/nl-nl/subscriptions/downloads/default(en-us).aspx. SharePoint Designer 2013: SharePoint Designer 2013 is the tool of choice for the rapid development of SharePoint applications. http://www.microsoft.com/en-us/download/details.aspx?id=30346 SharePoint 2013 Technical Library in Compiled Help: Downloadable CHM version of SharePoint 2013 content on TechNet. http://www.microsoft.com/en-us/download/details.aspx?id=30382 SharePoint Server 2013 Client Components SDK: The SharePoint Server 2013 Client Components SDK can be used to enable remote and local development with SharePoint Server 2013. http://www.microsoft.com/en-us/download/details.aspx?id=30355.  First off we will create a new Admin user called SharePoint Administrator. With this user we will perform the SharePoint installation but we will also use this user to login on the Beta machine.  Open Active Directory Users or Computer by doing the following:   Go to the Start Menu by using the Windows button or by going to the low left corner with your mouse. With the Start Menu open select “Active Directory Users and Computers”.  Open the Users OU and use your other mouse button to add a new user. When you have the “New Object - User” window open create the following user.     First name Last name User Logon name User logon name   SharePoint Administrator spadmin spadmin    &nbsp;  Make sure you select the options:   User cannot change password. Password never expires.  With this user in place search for the domain administrators group and add the current user to that group.  Now logoff the current user and login in with the SharePoint Administrator account to install the prerequisites.  Download ILSPY from the ILSPY website and place the zip file on your virtual machine. Extract the file and place it in a easy location. For easy use I have placed a shortcut on my desktop.    Next up is the Microsoft Office Professional Plus 2013 Preview. If you have downloaded the image add it to the virtual machine. If you do not have the image just skip this step.  The first step of the Installation is the acceptance off the license terms. Accept the terms and click continue.    The setup will continue with a step that is know when you have ever installed Office for know we will select “Customize”  because I want to check what it will be installing.    In the customize installation window you can see all the component the install will install on you machine. Notice that Microsoft Lync is a new one. Looks like it is now integrated in the Office Suite .  Make sure you do not install the Visio Viewer, this because we will also install the complete version. Select “Install Now”  to start the installation.    When the installation is done start the installation off the full version of Visio. The Visio installation will also start with the license term screen. Accept the license again and click “Continue” to Continue. This time we will just click “Install Now” to Install Visio.    Next up is the preview release of SharePoint designer double click the exe file to start the installation. This will also start with the “License Terms” screen. Accept the license and click “Continue”. On this installation we will also use the “Install Now” option. With the installation ready it is time to place the documentation on the machine.    Place the TechNet documentation file (CHM) on your desktop and start the Installation of the Client Components SDK. Don’t forget to “Unblock” the chm file on your desktop or else you will not be able to read the file.    With that all in place we will install the prerequisites of SharePoint. Mount the image on the server to start the installation. The installation will have a splash screen like the one in SharePoint 2010.    Select “Install Software Prerequisites” to start the installation of the Prerequisites.  This will start another installer that will install and configure the following components:   Microsoft .NET Framework 4.5 Windows Management Framework 3.0 (CTP2) Application Server Role, Web Server (IIS) Role Microsoft SQL Server 2008 R2 SP1 Native Client Microsoft Sync Framework Runtime v1.0 SP1 (x64) Windows Server AppFabric Microsoft Identity Extensions Microsoft Information Protection and Control Client Microsoft WCF Data Services 5.0 Cumulative Update Package 1 for Microsoft AppFabric 1.1 for Windows Server (KB2671763)    Click next to start the installation off these component. I myself are quite happy to see that SharePoint 2013 requires .Net Framework 4.5     Accept the license terms and click next to continue. This will start the installation of the prerequisites.    With the installation off the prerequisites done it is time to install SharePoint 2013. We will discuss the installation off SharePoint 2013 in the following post.   ","categories": ["Administration"],
        "tags": ["Beta","Developer Tools","Development","SharePoint 2013","Windows Server 2012"],
        "url": "/2012/07/install-sharepoint-2013-public-beta-on-windows-server-2012-rc-part-iv-installation-of-the-prerequisites-on-windows-server-2012-rc/",
        "teaser": null
      },{
        "title": "Install SharePoint 2013 Public Beta on Windows Server 2012 RC &ndash; Part V &ndash; Installation of SharePoint 2013 on Windows Server 2012 RC",
        "excerpt":" Now that the prerequisites are done it is time to start the installation of SharePoint 2013. First off we will have to some configuration changes.  The first thing we will do is creating a farm account to run Central Administration and the farm services.  Open Active Directory Users or Computer by doing the following:   Go to the Start Menu by using the Windows button or by going to the low left corner with your mouse. With the Start Menu open select “Active Directory Users and Computers”.  Open the Users OU and use your other mouse button to add a new user. When you have the “New Object - User” window open create the following user.     First name Last name User Logon name User logon name   SharePoint Farm svc_spfarm svc_spfarm    Make sure you select the options:   User cannot change password. Password never expires.  Next up is the configuration of a SQL Alias. We will create a SQL alias to be able to migrate the SQL Databases to another Virtual machine if we want.   Go to the Start Menu by using the Windows button or by going to the low left corner with your mouse. With the Start Menu open use your other mouse button and select “All apps”. Open “Command Prompt” and type in cliconfg. Within SQL Server Client Network Utility go to the Alias tab and click add. Type in a Alias Name (SQL2012_SP) and fill in the server name of the server that host SQL in our case it will be the local machine. Also select the following Network library type : TCP/IP. Select “Ok”  to add the alias and select “Apply” and “Ok” to close the configuration wizard.    With these configuration steps done it is time to mount the image and start the setup of SharePoint 2013.  During the first step of the installation you will have to insert you product key. I received my product key to mail. Fill it in and press “Continue”.    Accept the license terms and select “Continue”  again.    The next screen will give you the option to change the file location. For this machine we will leave it to default and start the installation.       When the installation is finished the “Run the Configuration Wizard” screen will open. Leave the check box checked and click “Close”.    The configuration wizard will be opened. On the welcome screen click “Next”.       When you click next the same warning screen will appear as it did with SharePoint 2010 since this is a new installation we can ignore this message and click “Ok”. The next step gives you the option to connect to an existing farm or to create a new server farm. Since this is a new installation we will select “create a new server farm”.    In the next step we will to specify the SharePoint SQL server and the name for the configuration database. We also need to specify the Database access account.  Fill in the following information:  Database server: SQL2012_SP (This is the alias we created)  Database name: SharePoint_Config  Username: dev\\svc_spfarm  Password: *********    On the next step we need to specify the passphrase we want to use when we want to add another server to the farm. Use a a known password that you will remember and click next.    The next step in the wizard will allow you to configure Central Administration. Use a port number u find handy and select NTLM authentication for this demo machine.      A summery of the configuration will follow. Check all of the settings and click on “Next” to start the configuration.      When the configuration is finished the successful window will be shown. Click finish to close the wizard, automatically a explorer window will be opened that opens Central Administration. With a model dialog asking if you want to help improve SharePoint. Select “Yes, I am willing to participate” and click “Ok”.  When the model dialog is closed you will get the option to start the configuration wizard to configure the farm. Select the option “No, I will configure everything myself”.    The Central Administration home screen will be opened.    In the next post we will start the configuration of the different SharePoint 2013 Services.   ","categories": ["Administration"],
        "tags": ["Installation","SharePoint 2013","Windows Server 2012"],
        "url": "/2012/07/install-sharepoint-2013-public-beta-on-windows-server-2012-rc-part-v-installation-of-sharepoint-2013-on-windows-server-2012-rc/",
        "teaser": null
      },{
        "title": "Install SharePoint 2013 Public Beta on Windows Server 2012 RC &ndash; Part VI &ndash; Configuring SharePoint 2013 on Windows Server 2012 RC",
        "excerpt":" Last week we finished the installation of SharePoint 2013. For us to test the new version off SharePoint we still have to do some configuration.  The first thing we will do is creating a couple of service accounts that we will use to create Service Application and Web Applications.  Open Active Directory Users or Computer by doing the following:   Go to the Start Menu by using the Windows button or by going to the low left corner with your mouse.   With the Start Menu open select “Active Directory Users and Computers”.   Open the Users OU and use your other mouse button to add a new user. When you have the “New Object - User” window open create the following user.     First name Last name User Logon name User logon name   SharePoint Application Pool svc_spapppool svc_spapppool   SharePoint Services svc_spservices svc_spservices   SharePoint Search svc_spsearch svc_spsearch   SharePoint Content Access spcontentaccess spcontentaccess    &nbsp;  Make sure you select the options, for all of the accounts:   User cannot change password.   Password never expires.   For us to create a new site we will also have to register a url within the DNS (You can also do this within your host file).&nbsp; To alter the DNS configuration do the following:   Go to the Start Menu by using the Windows button or by going to the low left corner with your mouse.   With the Start Menu open select “DNS”.   Select the forward lookup zone of your domain and add a new A Host record. For this machine I chose the following domain: sp2013.dev.local.    For us to access this URL this machine we will also have to disable the LoopBackCheck, this is because Windows Server 2012 also had the same security enabled as Windows Server 2008 R2. To disable the LoopBackCheck check this blog post:  http://msftplayground.com/2009/04/access-denied-while-crawling-sites/  Now that we have done these configurations we can start with the last configuration steps off SharePoint. First off we will create some managed accounts:  Create Managed Accounts  To create managed accounts follow the following steps:   Open “Central Administration”   Go to the Security section.   Select “Configure Managed Accounts”&nbsp; under “General Security”.   Then add managed accounts for all the accounts we created in the AD besides the spcontentaccess account.     With the managed accounts in place it is time to create our web application.  Create Web Application  To create a new web application follow these steps:   With “Central Administration” opened go to the “Application Management” section.   Select “Manage web applications”.   On the toolbar select “New”&nbsp; within the toolbar.   Create the web application with the following information.      Setting Value   IIS Web Site Create a new IIS web site.  Name: SharePont – SP2013  Port:80  Host Header: sp2013.dev.local  Path: Keep the default value   Security Configuration Allow Anonymous:false  Use Secure Sockets Layer(SSL): false   Claims Authentication Type Integrated Windows Authentication: NTLM   Sign In Page Url Default Sign In Page   Public Url URL: http://sp2013.dev.local:80  Zone: Default   Application Pool Create new application pool:  Name: SharePoint Web Application  Configurable: DEV\\sp_spapppool   Database Name and Authentication Database Server: SQL2012_SP  Database Name: WSS_Content_2013  Windows Authentication   Service Application Connections Default    &nbsp;  Press ‘Ok’ to create the web application.    When the web application is created we will define some managed paths to create site collections.  Define Managed Paths   With “Central Administration” opened go to the “Application Management” section.   Select “Manage web applications”.   Select the web application for which you want to define the managed paths.   On the toolbar select “Managed Paths” within the toolbar.   In the Define Managed Paths window add the following Managed Paths:      Path Type   personal Explicit   hub Explicit   content Explicit    &nbsp;  With the Managed Paths in place it is time to create a few site collections.  Create Site Collections  First off we will create a MySite host site collection. To start creating site collections follow these steps:   With “Central Administration” opened go to the “Application Management” section.   Select \"Create Site Collections\"   Make the site collection with the following information:      Setting Value   Web Application Make sure the right web application is selected   Title and Description Title: SP2013 MySite  Description: MySite Host   Web Site Address Url: http://sp2013.dev.local/personal    Template Selection Enterprise - My Site Host   Site Collection Administrators Choose the users you want to administrator the site collection. For this machine we will use dev\\spadmin    &nbsp;  Creating a site collection with these settings will create a my site host. We will also create a Team site and a Hub site.  Settings for the Hub site     Setting Value   Web Application Make sure the right web application is selected   Title and Description Title: SP2013 Hub  Description: Hub Host   Web Site Address Url: http://sp2013.dev.local/hub    Template Selection Enterprise - My Site Host   Site Collection Administrators Choose the users you want to administrator the site collection. For this machine we will use dev\\spadmin    &nbsp;  Settings for the Team site     Setting Value   Web Application Make sure the right web application is selected   Title and Description Title: SP2013  Description: Team site   Web Site Address Url: http://sp2013.dev.local/   Template Selection Enterprise - My Site Host   Site Collection Administrators Choose the users you want to administrator the site collection. For this machine we will use dev\\spadmin    &nbsp;  With the site collections in place we can start creating the several Service Applications.  Create User Profile Service&nbsp; Application  To create a User Profile Service Application follow the following steps:   With “Central Administration” opened go to the “Application Management” section.   Select “Manage Service Applications”.   On the toolbar select \"New\"&nbsp; and select \"User Profile Service Application\"   Create the Application with the following settings.      Setting Value   Name User Profile Service Application   Application Pool Create new application pool:  Application pool name:  SharePoint Service Application Pool  Configurable: DEV\\svc_spservices   Synchronization Database Database Server: SQL2012_SP  Database Name: Sync_DB   Profile Database Database Server: SQL2012_SP  Database Name: Profile_DB   Social Tagging Database Database Server: SQL2012_SP  Database Name: Social_DB   Profile Synchronization Instance DEV_BETA   My Site Host Url http://sp2013.dev.local/personal    My Site Managed Paths /content   Site Naming Format User name    &nbsp;  Create Managed Metadata service Application   The following service application we will be creating is the Managed Metadata Service Application.  Follow these steps:   With “Central Administration” opened go to the “Application Management” section.   Select “Manage Service Applications”.   On the toolbar select \"New\" and select \"Managed Metadata service application\"   Create the Application with the following settings.      Setting Value   Name Managed Metadata Service   Database Database Server: SQL2012_SP  Database Name: SharePoint_Managed_Metadata   Application Pool Use existing application pool:  Application pool name:  SharePoint Service Application Pool   Content Type Hub http://sp2013.dev.local/hub  Select, Report Syndication import errors from site collections using this service application.    &nbsp;  Create App Management Service Application  Next up is a new service application called “App Management Service Application”. This service application will give you the ability to manage the new App solutions that are build into SharePoint 2013.  Follow these steps to create this service application:   With “Central Administration” opened go to the “Application Management” section.   Select “Manage Service Applications”.   On the toolbar select \"New\" and select \"App Management Service\"   Create the Application with the following settings.      Setting Value   Name App Management Service Application   Database Database Server: SQL2012_SP  Database Name: App_Managed_DB   Application Pool Use existing application pool:  Application pool name:  SharePoint Service Application Pool   Create App Management Service Application Proxy Select, create App Management Service Application Proxy and add it to the default proxy group.    &nbsp;  Create Business Data Connectivity Service Application  A service application that was also in SharePoint 2010 is the BDC Application.  Follow these steps to create it:   With “Central Administration” opened go to the “Application Management” section.   Select “Manage Service Applications”.   On the toolbar select \"New\" and select “Business Data Connectivity Service Application\"   Create the Application with the following settings.      Setting Value   Name Business Data Connectivity Service Application   Database Database Server: SQL2012_SP  Database Name: BDC_Service_DB   Application Pool Use existing application pool:  Application pool name:  SharePoint Service Application Pool    &nbsp;  When I was creating this service application I received multiple errors. Even when I retried to create at went wrong with the same errors. When I checked the application everything looked fine and seem to be working correctly.  Create Machine Translation Service Application  Another new Service Application is the Machine Translation Service Application. This application give you the option to translate all kind off documents. The texts that are in the documents will be translated by the online Bing translation services.  To create this application use the following steps:   With “Central Administration” opened go to the “Application Management” section.   Select “Manage Service Applications”.   On the toolbar select \"New\" and select “Machine Translation Service Application\"   Create the Application with the following settings.      Setting Value   Name Machine Translation Service Application   Application Pool Use existing application pool:  Application pool name:  SharePoint Service Application Pool   Partition mode Don’t select this option.   Add to default proxy list Select this option to add it to the default proxy list.   Database Database Server: SQL2012_SP  Database Name: Machine_Translation_Service_DB    &nbsp;  Create Search Service Application  The Search Service Application is the next service application we will create. There changed a couple off small things to the search service application but creating is still the same.  Follow these steps:   With “Central Administration” opened go to the “Application Management” section.   Select “Manage Service Applications”.   On the toolbar select \"New\" and select “Search Service Application\"   Create the Application with the following settings.      Setting Value   Name Search Service Application   Search Service Account DEV\\svc_spsearch   Application Pool for Search Admin Web Service Create New Application Pool.  Application pool name:  SharePoint – Admin Search Service.  Configurable: DEV\\svc_spsearch   Application Pool for Search Query &amp; Site Settings Web Service Create New Application Pool.  Application pool name:  SharePoint – Query and Site Settings Pool.  Configurable: DEV\\svc_spsearch    &nbsp;  Create Secure Store Service  We will also create a Secure Store Service for when we would like to test connections to other systems like CRM.  Follow these steps to create the application:   With “Central Administration” opened go to the “Application Management” section.   Select “Manage Service Applications”.   On the toolbar select \"New\" and select “Secure Store Service Application\"   Create the Application with the following settings.      Setting Value   Name Secure Store Service Application   Database Database Server: SQL2012_SP  Database Name: Secure_Store_Service_DB   Application Pool Use existing application pool:  Application pool name:  SharePoint Service Application Pool   Enable audit Enable the audit log and set the days until purge to 30.    &nbsp;  Create Work Management Service  The work management service is the last service application we will create trough the UI. This is also one off the new service applications. The work management service application will give users the ability to edit and change there tasks on one single place within the SharePoint farm. This service will take care of changing values on other places and aggregating the tasks. One off the great things it is also possible to synchronize exchange tasks!!!  To create the Work Management Service follow these steps:   With “Central Administration” opened go to the “Application Management” section.   Select “Manage Service Applications”.   On the toolbar select \"New\" and select “Work management Service Application\"   Create the Application with the following settings.      Setting Value   Name Work Management Service Application   Application Pool Use existing application pool:  Application pool name:  SharePoint Service Application Pool   Service Application Proxy Select create proxy for this service application.    &nbsp;  This were all off the service application we will create trough the UI will will have to create the State Service Application with PowerShell.  Create State Service application with PowerShell   Run the “SharePoint 2013 Management Shell” as Administrator.   Type: “New-SPStateServiceApplication”   Next enter the name for the service application “Name : State Service Application”   With all the Service applications in place we will perform a IISReset make sure that all settings are refreshed.  After the IISReset we can start the service by going to the following screen:   Open “Central Administration”   Select “Application Management”   Then select “Manage services on server”   With the Services screen open start the following services:   App Management Service   Business Data Connectivity Service   Claims to Windows Token Service   Machine Translation Service   Managed Metadata Web Service   Request Management   Secure Store Service   User Profile Service   Work Management Service   When these services are started we will make the farm account (svc_spfarm) local administrator. This is because off the same problem with SharePoint 2010.  To make the svc_spfarm account local administrator do the following:  Open Active Directory Users or Computer by doing the following:   Go to the Start Menu by using the Windows button or by going to the low left corner with your mouse.   With the Start Menu open select “Active Directory Users and Computers”.   With Active Directory Users and Computers open open the domain and select the Builtin OU.   Within the Builtin OU find the administrators group and add the farm account to it.   Perform a reset off your VM to reset the security of the farm account. Then go back the “Manage services on server” and start the “User Profile Synchronization Service”.  When you will start this service you will have to connect it to a Service Application. Connect it to the User Profile Service application we created above.  With all these service applications and web applications in place you have a great environment to start testing SharePoint 2013. If you have any further question just leave a comment and I will get back to you!!     ","categories": ["Administration"],
        "tags": ["Configuration","SharePoint 2013","Windows Server 2012"],
        "url": "/2012/08/install-sharepoint-2013-public-beta-on-windows-server-2012-rc-part-vi-configuring-sharepoint-2013-on-windows-server-2012-rc/",
        "teaser": null
      },{
        "title": "Article Published Diwug Magazine 7 &ndash; SharePoint 2010 Solution Life Cycle Management",
        "excerpt":" A while ago I wrote an article about Solution Life Cycle Management for SharePoint 2010. The magazine it was written for was released today.  If you want to read the article you can download it here:  SharePoint 2010 Solution Life Cycle Management  If you want to download the complete Diwug magazine you can download it from the diwug website. The complete magazine contains the following articles:   Using Claims for Authorization in SharePoint 2010 - Niels Loup &amp; Jeffrey Zeeman   Deep Dive into SharePoint Enterprise Search for Developers – Part II - Niels Loup &amp; Jeffrey Zeeman   Organizing SharePoint Development - Ton Stegeman   Office 365 and Identity Federation - Bert Jan van der Steeg   Extreme “New Document”-button Makeover - Jeffrey Paarhuis   Custom Ranking Model Explained - Anita Boerboom   Real World Service Application federation with SharePoint 2010 Part Two - Spencer Harbar   SharePoint 2010 Solution Life Cycle Management - Maik van der Gaag   I just became the governance guy Now what? - Joe Capka  You can also download the complete e-magazine here. Let me know what you think about the article!!   ","categories": ["Article"],
        "tags": ["Article","Diwug","Solution Life Cycle"],
        "url": "/2012/08/article-published-diwug-magazine-7-sharepoint-2010-solution-life-cycle-management-2/",
        "teaser": null
      },{
        "title": "SharePoint 2013 Databases without the GUID",
        "excerpt":" When you configure SharePoint 2013 it will create the SharePoint_AdminContent database with a GUID at the end. To keep my database names clean I searched for a solution.  To be able to set the name of the database you will have to perform the configuration with PowerShell.  Take the following steps:  1. Open SharePoint 2013 Management Shell as Administrator.  2. Type the following:   New-SPConfigurationDatabase -DatabaseName BETA_SP2013_Config –AdministrationContentDatabaseName BETA_SP2013_AdminContent –DatabaseServer SP2013DB -FarmCredentials (get-credential)  3. You will get a pop-up to fill in the farm credentials. Fill in the account you want to use to run central administration under. When you filled in the account the management shell will also ask you to fill in the passphrase for joining servers to your farm.  4. Install the help collection files by running the following script.   Install-SPHelpCollection –All  5. Initialize the SharePoint Security by running the following script.   Initialize-SPResourceSecurity  6. Register the SharePoint Services by running the following script.   Install-SPService  7. Create central administration by running the following script.   New-SPCentralAdministration -Port 5555 -WindowsAuthProvider “ntlm”  8. Install the application content by running the following script.   Install-SPApplicationContent  9. When everything is done. You performed the steps the configuration wizard does for you. To be absolutely sure that everything is configured run the configuration wizard.   &nbsp;  After these steps SharePoint is configured with a clean database name!  Almost all other SharePoint databases can be named trough the UI only the Search Service Application and the Usage service create there own databases. The solution is to also create these services&nbsp; with PowerShell.  In other for you to create the Search Service Application you can use the script below (You only have to change the arguments on the top).  $databaseServer = \"SP2013DB\" $ServiceAppPool = \"SharePoint Services Application Pool\" $IndexLocation = \"C:\\SP2013_Search\" $SearchServiceApplicationName = \"Search Service Application\" $server = \"CLAY\"  Write-Host \"Setting up Search\"  Start-SPEnterpriseSearchServiceInstance $server Start-SPEnterpriseSearchQueryAndSiteSettingsServiceInstance $server  $searchApp = New-SPEnterpriseSearchServiceApplication -Name $SearchServiceApplicationName -ApplicationPool $ServiceAppPool -DatabaseServer $databaseServer -DatabaseName \"BETA_SP2013_Search\" $searchInstance = Get-SPEnterpriseSearchServiceInstance $server  $ssa = Get-SPEnterpriseSearchServiceApplication   Write-Host \"Setup Topology\" $ssa.ActiveTopology $clone = $ssa.ActiveTopology.Clone()  New-SPEnterpriseSearchAdminComponent –SearchTopology $clone -SearchServiceInstance $searchInstance New-SPEnterpriseSearchContentProcessingComponent –SearchTopology $clone -SearchServiceInstance $searchInstance New-SPEnterpriseSearchAnalyticsProcessingComponent –SearchTopology $clone -SearchServiceInstance $searchInstance  New-SPEnterpriseSearchCrawlComponent –SearchTopology $clone -SearchServiceInstance $searchInstance  New-SPEnterpriseSearchIndexComponent –SearchTopology $clone -SearchServiceInstance $searchInstance -RootDirectory $IndexLocation New-SPEnterpriseSearchQueryProcessingComponent –SearchTopology $clone -SearchServiceInstance $searchInstance  $clone.Activate()  $ssa | get-SPEnterpriseSearchAdministrationComponent |  set-SPEnterpriseSearchAdministrationComponent -SearchServiceInstance  $searchInstance   Write-Host \"Setup Proxy\" $searchAppProxy = New-SPEnterpriseSearchServiceApplicationProxy -Name \"$SearchServiceApplicationName Proxy\" -SearchApplication $SearchServiceApplicationName &gt; $null  Write-Host \"Done\" &nbsp;  You need to take the steps below to create the Usage Service Application:   1. Get the Usage Service.   $serviceInstance = Get-SPUsageService ","categories": ["Development"],
        "tags": ["Database","SharePoint 2013"],
        "url": "/2012/08/sharepoint-2013-databases-without-the-guid/",
        "teaser": null
      },{
        "title": "SharePoint 2013 Reaches RTM",
        "excerpt":"The Office engineering team signed off on a new Build. The build they signed off for is RTM. This means SharePoint 2013 will be able to be downloaded in less than a month from now on TechNet or MSDN.    You can read all about it on the Office News site: http://blogs.office.com/b/office-news/archive/2012/10/11/office-reaches-rtm.aspx  Some interesting point from the news article are:  Additionally, we have a number of programs that provide business customers with early access so they can begin testing, piloting and adopting Office within their organizations:    We will begin rolling out new capabilities to Office 365 Enterprise customers in our next service update, starting in November through general availability.   Volume Licensing customers with Software Assurance will be able to download the Office 2013 applications as well as other Office products including SharePoint 2013, Lync 2013 and Exchange 2013 through the Volume Licensing Service Center by mid-November. These products will be available on the Volume Licensing price list on December 1.   IT professionals and developers will be able to download the final version via their TechNet or MSDN subscriptions by mid-November.   Please stay tuned for more specifics on general availability dates and other Office launch news. In the meantime, if you'd like to give the pre-release version a try, you can visit office.com/preview.  Thank you to the millions of people who have been testing early releases of the new Office. We are grateful for your support. Your invaluable feedback has helped us make the new Office the best Office ever.  ","categories": ["Azure DevOps"],
        "tags": ["RTM","SharePoint 2013"],
        "url": "/2012/10/sharepoint-2013-reaches-rtm/",
        "teaser": null
      },{
        "title": "SharePoint 2013 Released on MSDN",
        "excerpt":"12 October I wrote a post about the fact that SharePoint 2013 and Office Reached RTM. Today I noticed that it came available on MSDN. So the final version is THERE!!!!  Happy SharePointing. (I know what I will be doing today ).    ","categories": ["Azure DevOps"],
        "tags": ["Office 2013","SharePoint 2013"],
        "url": "/2012/10/sharepoint-2013-released-on-msdn/",
        "teaser": null
      },{
        "title": "Behold the new SharePoint Splash screen",
        "excerpt":"Last week the new version of SharePoint 2013 was released on MSDN. As a real SharePoint enthusiasts I directly downloaded the version. Information about this version can be read here.   This new version of SharePoint called SharePoint 2013 has a new Installation screen, to give more power to the new Interface.    Today I will start a new fresh installation of a development machine with the following specifications:   Microsoft Office 2013 Professional. Microsoft Office Visio 2013. Microsoft Office Project 2013. Visual Studio 2012. Windows Server 2012. SQL Server 2012.  Every interesting thing I will discover will be posted on this blog, so stay tuned!   ","categories": ["Administration","SharePoint"],
        "tags": ["SharePoint 2013"],
        "url": "/2012/10/behold-the-new-sharepoint-splash-screen/",
        "teaser": null
      },{
        "title": "Retrieve the Friendly URL of a Publishing Page",
        "excerpt":"One of my first projects with SharePoint 2013 is building a SharePoint website. For the website we had to build a functionality that display’s the URL of specific pages.   For the website we are using Managed Navigation. Displaying URLs meant we wanted to display the friendly URLS. After searching for a while with ILSpy and looking at some MSDN articles:   &nbsp;NavigationTerm &nbsp;PublishingPage  I found out that the friendly URL can be retrieved by first getting a list of NavigationTerm items from a SharePoint list item. When you have a NavigationTerm you can retrieve the display URL of that term using the method “GetResolvedDisplayUrl”.  In short the code to retrieve the friendly URLs will look something like this.  public List&lt;string&gt; GetPagesUrls() {          //list for saving the urls     List&lt;string&gt; retVal = new List&lt;string&gt;();      //current web     SPWeb web = SPContext.Current.Web;      //check if the current web is a publishing weg     if (PublishingWeb.IsPublishingWeb(web)) {          //get the pages list id         Guid listId = PublishingWeb.GetPagesListId(web);          //retrieve the pages list         SPList pagesList = web.Lists[listId];          //itterate trough the pages         foreach (SPListItem item in pagesList.Items) {             //retrieve the terms used for the navigation (this can be multiple terms)             IList&lt;NavigationTerm&gt; terms = TaxonomyNavigation.GetFriendlyUrlsForListItem(item, false);              string url = string.Empty;              //check if the pages has terms associated with it             if (terms.Count &gt; 0) {                 //use the GetResolvedDisplayUrl to retrieve the page friendly urls                 url = terms[0].GetResolvedDisplayUrl(string.Empty);             } else {                 //if the page does not have any terms get the normal url                 url = item.File.Url;             } ","categories": ["Development"],
        "tags": ["C#","Publishing","SharePoint 2013"],
        "url": "/2013/01/retrieve-the-friendly-url-of-a-publishing-page/",
        "teaser": null
      },{
        "title": "Minimize JavaScript and CSS &ndash; Web Essentials",
        "excerpt":"When working on a Website or Intranet environment and you are using a lot off JavaScript and CSS files you would like to have the possibility to minimize/bundle the files.  On the web you can find a lot of solutions for this, on my last project when I was working on a website based on SharePoint 2013 (http://www.motion10.nl). I found a great extension for Visual Studio called: Web Essentials.  Key features of this extension are:   Bundling of JavaScript and CSS files. Minimize files.  &nbsp;  The extension does much more for example it also display’s a color swatch next to a specified color in you Stylesheet file  .    For a complete list off the features you can look on the website of Web Essentials.  The extension gives you the option to minimize a JavaScript files by clicking on it with your second mouse button. Once generated it keeps synching your changes to the minimized file (so you will keep working in the normal file).  &nbsp;    All of the settings of the extension can be managed from the default options menu within the “Web Essentials” category that the extension adds.     In my opinion this extension should be added to the list of items you install on a development environment.  ","categories": ["Development"],
        "tags": ["SharePoint","Visual Studio","Website"],
        "url": "/2013/03/minimize-javascript-and-css-web-essentials/",
        "teaser": null
      },{
        "title": "Active Directory Picture Synchronization",
        "excerpt":"When you have stored the pictures of employees in Active Directory you would like to have the option to synchronize these pictures to their SharePoint profile.  This article describes the steps you need to take to import them into the SharePoint profiles.  &nbsp;  1. Change the mapping of the Picture profile field.  Navigate to the User Profile Service Application and got to “Manage User Properties\". Find the picture property and select the edit menu item. Add a new Mapping to the “thumbnailPhoto” attribute and select “Ok” when you are done.  Before:    After:    2. Perform a Full Import  On the User Profile Service Application select “Start Profile Synchronization” to start a Full Synchronization.  3. Run the command Update-SPProfilePhotoStore  This command let will create the profile pictures in the “User Photo” library in the Mysite host site collection. Run this command with the following options:  Update-SPProfilePhotoStore -MySiteHostLocation [Your Mysite Host Location] -CreateThumbnailsForImportedPhotos 1  &nbsp;  4. Check the profiles for their pictures.  &nbsp;  One downside to this is that you need to rerun the Update-SPProfilePhotoStore when you have a new Employee for example.  &nbsp;  When you would like to try this out on your environment you can use the following PowerShell script for importing a picture in Active Directory.  #parameters $username = \"tpicture\" $picture = \"C:\\Pictures\\msftplayground.png\"  #get the active directory information $dom = [System.DirectoryServices.ActiveDirectory.Domain]::GetCurrentDomain() $root = $dom.GetDirectoryEntry() $search = [System.DirectoryServices.DirectorySearcher]$root $search.Filter = \"(&amp;(objectclass=user)(objectcategory=person)(samAccountName=$username))\" $result = $search.FindOne()  #if the result not equal to null if ($result -ne $null){     $user = $result.GetDirectoryEntry()      #get the byte array of the picture     [byte[]]$jpg = Get-Content $picture -encoding byte       #change the active directory property     $user.put(\"thumbnailPhoto\",  $jpg )     $user.setinfo()      Write-Host $user.displayname \" updated\" -ForegroundColor Green } else {     Write-Host $username \" Does not exist\" -ForegroundColor Red } ","categories": ["SharePoint"],
        "tags": ["Active Directory","SharePoint 2010","SharePoint 2013"],
        "url": "/2013/05/active-directory-picture-synchronization/",
        "teaser": null
      },{
        "title": "SharePoint Color Palette Tool (ThemeSlots)",
        "excerpt":"Many off you may have seen it in presentations of Microsoft. The tool was then called ThemeSlots. Today I stumbled on the release and it is now called “SharePoint Color Palette Tool”.  This tool helps you to create your own SPColor file for SharePoint 2013 by giving you a handy interface with a preview window.    &nbsp;  You can download the tool here:  http://www.microsoft.com/en-us/download/details.aspx?id=38182  ","categories": ["Design"],
        "tags": ["Color","Design","SharePoint 2013","SharePoint Color Palette Tool","spcolor","ThemeSlot"],
        "url": "/2013/05/sharepoint-color-palette-tool-themeslots/",
        "teaser": null
      },{
        "title": "Exception: The Execute method of job definition (ID ad81e6cd-51ef-4baa-aedc-d529219368b5) threw an exception",
        "excerpt":"When investigating the errors in the event log today I stumbled upon a strange error:     The Execute method of job definition Microsoft.Office.Server.Search.Administration.QueryClassificationDictionaryUpdateTimerJobDefinition (ID ad81e6cd-51ef-4baa-aedc-d529219368b5) threw an exception. More information is included below.  Expected 1 Managed Metadata Proxy which is default keyword taxonomy for SSA 96233017-f26a-49b7-9950-1b8577beb4f8, but located 2  I new I had configured 2 Managed Metadata Service applications but what is the error about. After thinking about the problem for a hour it hit me that you need to specify a default Metadata service application.     This can be done by selecting the proxy of the Managed Metadata service application and selecting the “Properties” button in the ribbon. The mistake that I made was that I selected “This service application is the default storage location for Keywords” and “This service application is the default storage for column specific term sets” for both of the service applications and neglected the warning message that SharePoint shows you.      This resolved in the error messages that were in the event log. By deselecting the options for one of the service applications the error messages were gone.   ","categories": ["SharePoint","Troubleshooting"],
        "tags": ["Managed Metadata","SharePoint 2013","Troubleshooting"],
        "url": "/2013/05/exception-the-execute-method-of-job-definition-id-ad81e6cd-51ef-4baa-aedc-d529219368b5/",
        "teaser": null
      },{
        "title": "Google Maps Display Template &ndash; SharePoint 2013 &ndash; Part 1",
        "excerpt":"As the most of you will know by now is that SharePoint 2013 has a new web part called the “Content Search” web part. This web part display’s search result by using specific display templates.  For one of our clients we were asked to display location/office information on a Google Maps card based on information they save in a list.&nbsp; We first had the idea of using the new Geo Location fields that are pretty awesome. But we had a hard requirement for using Google Maps because they already had a licence for using that.  If you want more information about the Geo Location field you can read about here:   Display Location Maps within SharePoint Lists using Geolocation  The easiest way to display this information is retrieving the results by its content type with the Content Search web part and using a Google Maps Display Template to display those results.  There are two primary types of display templates:   Control templates determine the overall structure of how the results are presented. Includes lists, lists with paging, and slide shows.    Item templates determine how each result in the set is displayed. Includes images, text, video, and other items.    The control display template defines the HTML structure for the overall layout and the item template defines the HTML structure for the item. If you want to read more information about display templates here is a good article: SharePoint 2013 Design Manager display templates In this Part we will be creating the item template. The easiest way to create a new item display template is using SharePoint Designer and create a copy of another display template. The display templates can be found here:   All Files - _catalogs – masterpage – Display Templates – Content Web Parts   To get a new item display template you will need to copy an existing item template, for this example we will create a copy of the “Item_TwoLines.html” and rename it to “Item_GoogleMarker.html\". When you open the file in edit mode you will see that there is a Title attribute on the top of the page. Change this title to Google Maps Marker because the item we will retrieve will represent markers on the map. The result we will be retrieving will have the following information:  Title   Longitude   Latitude   Description   Address   Site Url (Url to a SharePoint site)  In order to get this information from search you will have to create managed properties for the specific field. In this post I will not tell you how to create those for more information you can read this post or look on MSDN for more information:   Creating Metadata Properties through code – SharePoint 2010  If you look further in the display template you will see the property “ManagedPropertyMapping” this will map the properties to specific variables in the file. To which property they are mapped can be changed in the tool part of the web part     &lt;mso:ManagedPropertyMapping msdt:dt=\"string\"&gt;&amp;#39;Link URL&amp;#39;{Link URL}:&amp;#39;Path&amp;#39;,&amp;#39;Line 1&amp;#39;{Line 1}:&amp;#39;Title&amp;#39;,&amp;#39;Line 2&amp;#39;{Line 2}:&amp;#39;&amp;#39;,&amp;#39;FileExtension&amp;#39;,&amp;#39;SecondaryFileExtension&amp;#39;&lt;/mso:ManagedPropertyMapping&gt;  Each property mapping exists out of three variables separated by &amp;#39;,&amp;#39; and the complete properties are separated by ‘, ‘.   For example “&amp;#39;Line 1&amp;#39;{Line 1}:&amp;#39;Title&amp;#39;”:    The first “Line 1” will represents the value in the display template.  The second “{Line 1}” represents the description and is displayed in the tool part. You can change this to each value you want.  The third “Title” is the default managed property it is mapped to.  For our item template we will extend the property with our own values we need to be able to retrieve the right values from the search engine.  &lt;mso:ManagedPropertyMapping msdt:dt=\"string\"&gt;&amp;#39;Link URL&amp;#39;{Link URL}:&amp;#39;Path&amp;#39;,&amp;#39;Line 1&amp;#39;{Title}:&amp;#39;Title&amp;#39;,&amp;#39;Line 2&amp;#39;{Latitude}:&amp;#39;Title&amp;#39;,&amp;#39;Line 3&amp;#39;{Longtitude}:&amp;#39;Title&amp;#39;,&amp;#39;Line 4&amp;#39;{Description}:&amp;#39;Title&amp;#39;,&amp;#39;Line 5&amp;#39;{Address}:&amp;#39;Title&amp;#39;,&amp;#39;Line 6&amp;#39;{Site Url}:&amp;#39;Title&amp;#39;&lt;/mso:ManagedPropertyMapping&gt;  When this is done you can change the “MasterPageDescription” property to the value of your choice.  When you go further down into the display template the values are retrieved within the div called “TwoLines”.  &lt;!--#_ var encodedId = $htmlEncode(ctx.ClientControl.get_nextUniqueId() + \"_2lines_\");  var linkURL = $getItemValue(ctx, \"Link URL\"); linkURL.overrideValueRenderer($urlHtmlEncode); var iconURL = Srch.ContentBySearch.getIconSourceFromItem(ctx.CurrentItem);  var line1 = $getItemValue(ctx, \"Line 1\"); var line2 = $getItemValue(ctx, \"Line 2\"); line1.overrideValueRenderer($contentLineText); line2.overrideValueRenderer($contentLineText);  var containerId = encodedId + \"container\"; var pictureLinkId = encodedId + \"pictureLink\"; var pictureId = encodedId + \"picture\"; var dataContainerId = encodedId + \"dataContainer\"; var line1LinkId = encodedId + \"line1Link\"; var line1Id = encodedId + \"line1\"; var line2Id = encodedId + \"line2\"; _#--&gt;  Replace this by the following code in which we get all of the values from our properties and use basically the same code the was in the original template:  &lt;!--#_   var encodedId = $htmlEncode(ctx.ClientControl.get_nextUniqueId() + \"_2lines_\");   var linkURL = $getItemValue(ctx, \"Link URL\");   linkURL.overrideValueRenderer($urlHtmlEncode);   var iconURL = Srch.ContentBySearch.getIconSourceFromItem(ctx.CurrentItem);           var title = $getItemValue(ctx, \"Line 1\");   var latitude = $getItemValue(ctx, \"Line 2\");   var longtitude = $getItemValue(ctx, \"Line 3\");   var description = $getItemValue(ctx, \"Line 4\");   var address = $getItemValue(ctx, \"Line 5\");   var siteUrl = $getItemValue(ctx, \"Line 6\");   title.overrideValueRenderer($contentLineText);   latitude.overrideValueRenderer($contentLineText);   longtitude.overrideValueRenderer($contentLineText);   siteUrl.overrideValueRenderer($contentLineText);          var itemId = ctx.CurrentItemIdx; _#--&gt;  Now that we have all of the values for a item. It is time to write the HTML we want for the display template. The easiest way is to place all of the information about the marker in a hidden input field. In the control template we will then retrieve the information and display it on the map.  The complete display template will then look like this:  &lt;html xmlns:mso=\"urn:schemas-microsoft-com:office:office\" xmlns:msdt=\"uuid:C2F41010-65B3-11d1-A29F-00AA00C14882\"&gt;  &lt;head&gt; &lt;title&gt;Google Map Marker&lt;/title&gt;  &lt;!--[if gte mso 9]&gt;&lt;xml&gt; &lt;mso:CustomDocumentProperties&gt; &lt;mso:TemplateHidden msdt:dt=\"string\"&gt;0&lt;/mso:TemplateHidden&gt; &lt;mso:ManagedPropertyMapping msdt:dt=\"string\"&gt;&amp;#39;Link URL&amp;#39;{Link URL}:&amp;#39;Path&amp;#39;,&amp;#39;Line 1&amp;#39;{Title}:&amp;#39;Title&amp;#39;,&amp;#39;Line 2&amp;#39;{Latitude}:&amp;#39;Title&amp;#39;,&amp;#39;Line 3&amp;#39;{Longtitude}:&amp;#39;Title&amp;#39;,&amp;#39;Line 4&amp;#39;{Description}:&amp;#39;Title&amp;#39;,&amp;#39;Line 5&amp;#39;{Address}:&amp;#39;Title&amp;#39;,&amp;#39;Line 6&amp;#39;{Site Url}:&amp;#39;Title&amp;#39;&lt;/mso:ManagedPropertyMapping&gt; &lt;mso:MasterPageDescription msdt:dt=\"string\"&gt;This Item Display Template will show a marker when used with the the Google Maps control.&lt;/mso:MasterPageDescription&gt; &lt;mso:ContentTypeId msdt:dt=\"string\"&gt;0x0101002039C03B61C64EC4A04F5361F385106603&lt;/mso:ContentTypeId&gt; &lt;mso:TargetControlType msdt:dt=\"string\"&gt;;#Content Web Parts;#&lt;/mso:TargetControlType&gt; &lt;mso:HtmlDesignAssociated msdt:dt=\"string\"&gt;1&lt;/mso:HtmlDesignAssociated&gt; &lt;mso:HtmlDesignConversionSucceeded msdt:dt=\"string\"&gt;True&lt;/mso:HtmlDesignConversionSucceeded&gt; &lt;mso:HtmlDesignStatusAndPreview msdt:dt=\"string\"&gt;&lt;/mso:HtmlDesignStatusAndPreview&gt; &lt;/mso:CustomDocumentProperties&gt; &lt;/xml&gt;&lt;![endif]--&gt; &lt;/head&gt;  &lt;body&gt;     &lt;script&gt;         $includeLanguageScript(this.url, \"~sitecollection/_catalogs/masterpage/Display Templates/Language Files/{Locale}/CustomStrings.js\");     &lt;/script&gt;      &lt;div id=\"TwoLines\"&gt;         &lt;!--#_         var encodedId = $htmlEncode(ctx.ClientControl.get_nextUniqueId() + \"_2lines_\");         var linkURL = $getItemValue(ctx, \"Link URL\");         linkURL.overrideValueRenderer($urlHtmlEncode);         var iconURL = Srch.ContentBySearch.getIconSourceFromItem(ctx.CurrentItem);                  var title = $getItemValue(ctx, \"Line 1\");         var latitude = $getItemValue(ctx, \"Line 2\");         var longtitude = $getItemValue(ctx, \"Line 3\");         var description = $getItemValue(ctx, \"Line 4\");         var address = $getItemValue(ctx, \"Line 5\");         var siteUrl = $getItemValue(ctx, \"Line 6\");         title.overrideValueRenderer($contentLineText);         latitude.overrideValueRenderer($contentLineText);         longtitude.overrideValueRenderer($contentLineText);         siteUrl.overrideValueRenderer($contentLineText);                  var itemId = ctx.CurrentItemIdx;         _#--&gt;         &lt;input type=\"hidden\" id=\"_#= itemId =#_-Location\" value=\"_#= title =#_;_#= latitude =#_;_#= longtitude =#_;_#= description =#_;_#= address =#_;_#= siteUrl =#_\"&gt;     &lt;/div&gt; &lt;/body&gt; ","categories": ["Development","Javascript","SharePoint"],
        "tags": ["Display Tempaltes","Google","SharePoint 2013","SharePoint Designer"],
        "url": "/2013/05/google-maps-display-template-sharepoint-2013-part-1/",
        "teaser": null
      },{
        "title": "Google Maps Display Template &ndash; SharePoint 2013 &ndash; Part 2",
        "excerpt":"As described in my previous post we will be creating a display template for Google maps. When you did not have the change to read part 1 you can do this by using the below link:  Google Maps Display Template – SharePoint 2013 – Part 1&nbsp;  In part one we created the item display template and because we could not insert the JavaScript of adding markers to the map in the display template we have inserted the information in hidden inputs.  First of there is 1 important thing to notice. If you would like to use Google maps on you page you have to add a JavaScript file on you page. Within display control template you can use a specific method called “$includeScript();”. This method will include the script to the page for you. But there is a catch for as far as I could investigate if your JavaScript file doesn’t end&nbsp; with “.js” it will not be included on the page. The Google maps API is a file that does not end with “.js”. The only option you have is to add the Google API file by using a script editor.        &lt;script src=\"https://maps.googleapis.com/maps/api/js?v=3.exp&amp;sensor=false\"&gt;&lt;/script&gt;   In the control template we will begin by reading out that information and placing them on the map.  To get a new control template you will need to copy an existing control template, for this example we will create a copy of the “Control_List.html” and rename it to “Control_GoogleMaps.html”.  On each hidden input we have specified an id in order to retrieve the values, the id consist of a text value and the number of the result. In the Control Display Template we can retrieve the number of the result by using the “ctx” object and the property “CurrentItemIdx”. Each marker will become an Item in the “markers” array and will be a array itself. In the iteration we also construct the information window for each marker.  var markers = new Array();  for(var i=0;i&lt;=ctx.CurrentItemIdx;i++){     var inputValue = document.getElementById(i+'-Location').value     var itemArray = inputValue.split(\";\");       var contentString = '&lt;div style=\"height:100px;\" id=\"content\"&gt;'+       '&lt;b&gt;' + itemArray[0] + '&lt;/b&gt;'+      '&lt;hr&gt;&lt;div id=\"bodyDescription\" style=\"margin-top:2px;\"&gt;'+ itemArray[3] + '&lt;/div&gt;' +      '&lt;div id=\"bodyAddress\" style=\"margin-top:2px;\"&gt;'+ itemArray[4] + '&lt;/div&gt;' +      '&lt;a href=\"'+ itemArray[5] + '\"&gt;&lt;div style=\"font-weight:bold;margin-top:5px;\"&gt;Office Information&lt;/div&gt;&lt;/a&gt;&lt;/div&gt;';       itemArray[3] = contentString;      markers[i] = itemArray;              }   When we have the information of the markers we can start by placing the markers on the map. For more information on how to use the Google Maps API go here.  var mapOptions = {      zoom: 4,      mapTypeId: google.maps.MapTypeId.HYBRID }  var map = new google.maps.Map(document.getElementById('map-canvas'), mapOptions); var bounds = new google.maps.LatLngBounds();  var infowindow = new google.maps.InfoWindow(), marker, i; for (i = 0; i &lt; markers.length; i++) {            var title = markers[i][0];     var latitude = markers[i][1];     var longitude = markers[i][2];     var pointLatlng = new google.maps.LatLng(latitude,longitude);     bounds.extend(pointLatlng);              marker = new google.maps.Marker({                 position: pointLatlng ,                 map: map,                 title:title,                 icon:'/PublishingImages/point.png'     });          google.maps.event.addListener(marker, 'click', (function(marker, i) {       return function() {         infowindow.setContent(markers[i][3]);         infowindow.open(map, marker);       }     })(marker, i)); }  map.fitBounds(bounds);   By saving each point within a LatLngBounds object in the example above the bounds object we can specify on the map that it has to load all point within the first view by using the method “map.fitBounds(bounds)”.&nbsp;   When tying this all together you will get a control display template like below:  &lt;html xmlns:mso=\"urn:schemas-microsoft-com:office:office\" xmlns:msdt=\"uuid:C2F41010-65B3-11d1-A29F-00AA00C14882\"&gt;  &lt;head&gt; &lt;title&gt;Google Maps&lt;/title&gt;  &lt;!--[if gte mso 9]&gt;&lt;xml&gt; &lt;mso:CustomDocumentProperties&gt; &lt;mso:TemplateHidden msdt:dt=\"string\"&gt;0&lt;/mso:TemplateHidden&gt; &lt;mso:MasterPageDescription msdt:dt=\"string\"&gt;This is the default Maps Display Template that will show a map with all of the items. For this map you should have managed properties with the longtitude and latitude of locations. Also make sure you reference the google map api: (https://maps.googleapis.com/maps/api/js?v=3.exp)&lt;/mso:MasterPageDescription&gt; &lt;mso:ContentTypeId msdt:dt=\"string\"&gt;0x0101002039C03B61C64EC4A04F5361F385106601&lt;/mso:ContentTypeId&gt; &lt;mso:TargetControlType msdt:dt=\"string\"&gt;;#Content Web Parts;#&lt;/mso:TargetControlType&gt; &lt;mso:HtmlDesignAssociated msdt:dt=\"string\"&gt;1&lt;/mso:HtmlDesignAssociated&gt; &lt;/mso:CustomDocumentProperties&gt; &lt;/xml&gt;&lt;![endif]--&gt; &lt;/head&gt;  &lt;body&gt;          &lt;script&gt;         $includeLanguageScript(this.url, \"~sitecollection/_catalogs/masterpage/Display Templates/Language Files/{Locale}/CustomStrings.js\");     &lt;/script&gt;      &lt;div id=\"Control_List\"&gt;     &lt;!--#_      if (!$isNull(ctx.ClientControl) &amp;&amp; !$isNull(ctx.ClientControl.shouldRenderControl) &amp;&amp; !ctx.ClientControl.shouldRenderControl()){         return \"\";     }      ctx.ListDataJSONGroupsKey = \"ResultTables\";     var $noResults = Srch.ContentBySearch.getControlTemplateEncodedNoResultsMessage(ctx.ClientControl);     var noResultsClassName = \"ms-srch-result-noResults\";     _#--&gt;               _#= ctx.RenderGroups(ctx) =#_              &lt;!--#_      function initialize() {            var mapOptions = {                 zoom: 4,               mapTypeId: google.maps.MapTypeId.HYBRID           }            var markers = new Array();           var map = new google.maps.Map(document.getElementById('map-canvas'), mapOptions);           var bounds = new google.maps.LatLngBounds();            for(var i=0;i&lt;=ctx.CurrentItemIdx;i++){               var inputValue = document.getElementById(i+'-Location').value               var itemArray = inputValue.split(\";\");                var contentString = '&lt;div style=\"height:100px;\" id=\"content\"&gt;'+                   '&lt;b&gt;' + itemArray[0] + '&lt;/b&gt;'+                  '&lt;hr&gt;&lt;div id=\"bodyDescription\" style=\"margin-top:2px;\"&gt;'+ itemArray[3] + '&lt;/div&gt;' +                  '&lt;div id=\"bodyAddress\" style=\"margin-top:2px;\"&gt;'+ itemArray[4] + '&lt;/div&gt;' +                  '&lt;a href=\"'+ itemArray[5] + '\"&gt;&lt;div style=\"font-weight:bold;margin-top:5px;\"&gt;Office Information&lt;/div&gt;&lt;/a&gt;&lt;/div&gt;';              itemArray[3] = contentString;             markers[i] = itemArray;                        }                var infowindow = new google.maps.InfoWindow(), marker, i;                for (i = 0; i &lt; markers.length; i++) {                   var title = markers[i][0];                var latitude = markers[i][1];                var longitude = markers[i][2];                var pointLatlng = new google.maps.LatLng(latitude,longitude);                bounds.extend(pointLatlng);                         marker = new google.maps.Marker({                   position: pointLatlng ,                   map: map,                   title:title,                   icon:'/PublishingImages/point.png'                }); ","categories": ["Design","Development","Javascript","SharePoint"],
        "tags": ["Display Tempaltes","Google","SharePoint 2013","SharePoint Designer"],
        "url": "/2013/05/google-maps-display-template-sharepoint-2013-part-2/",
        "teaser": null
      },{
        "title": "SkyDrive Pro Client Application",
        "excerpt":"When using SharePoint 2013 you are able to Sync your document libraries to your local system when using SkyDrive Pro. SkyDrive Pro was only available on:  Office 365 Pro Plus Office 365 Small Business Premium Office Professional Plus 2013 &nbsp;  SkyDrive Pro is now also available as standalone client application and can be downloaded here:  http://www.microsoft.com/en-US/download/details.aspx?id=39050  “The standalone SkyDrive Pro sync client allows users of SharePoint 2013 and SharePoint Online in Office 365 to sync their personal SkyDrive Pro and any SharePoint 2013 or Office 365 team site document libraries to their local machine for access to important content on and offline. The SkyDrive Pro client can be installed side-by-side with previous versions of Office (Office 2010, Office 2007, etc)”      ","categories": ["Administration"],
        "tags": ["Office","SharePoint 2013","SkyDrive Pro"],
        "url": "/2013/06/skydrive-pro-client-application/",
        "teaser": null
      },{
        "title": "SharePoint 2013 Workflow with Visual Studio 2012",
        "excerpt":"Today I have been working on a project that required a workflow in order to keep track of a status. Because we were working with SharePoint 2013 I thought lets build a SharePoint 2013 Workflow.  First of all when you would like to begin using SharePoint 2013 Workflow you have to install some services. When using the Web Platform Installer it is quit easy. When you still need to install SharePoint 2013 workflow following the below article from Microsoft:   Install and configure workflow for SharePoint Server 2013  When you want to start with developing SharePoint 2013 workflows you will also have to install a Extension for Visual Studio:   Workflow Manager Tools 1.0 for Visual Studio 2012  When creating my first workflow for Visual Studio 2012 I noticed that I was missing a .cs file so I started to read some articles on MSDN:   Develop SharePoint 2013 workflows using Visual Studio  The following line got my attention.  “As with previous versions, Microsoft SharePoint 2013 provides two primary workflow development environments for authoring workflows: Microsoft SharePoint Designer and Microsoft Visual Studio. However, what differs from previous versions is that using Visual Studio no longer provides a code-based authoring strategy. Instead, both SharePoint Designer and Visual Studio provide a fully declarative, no-code authoring environment regardless of the development tool you select.”  Basically this will mean you can only create your workflow declarative. When you need to write custom code you will have to build a custom action:   How to: Build and deploy workflow custom actions  When I was creating my first workflow the second thing I noticed was that there are a lot of actions you use and that it is quite easy to extend:     All together I think this new approach is very powerful and does also mean you will be able to create APP workflow.  Deploying my first workflow directly resulted in a error when activating the feature:      Server Error in '/' Application. --------------------------------------------------------------------------------    Microsoft.SharePoint.SPException: App Management Shared Service Proxy is not installed. &nbsp;&nbsp; at Microsoft.SharePoint.AppRegistration.GetProxy(SPServiceContext serviceContext) &nbsp;&nbsp; at Microsoft.SharePoint.AppRegistration.AddOrUpdateAppNoPermissionCheck(SPAppPrincipalInfo appInfo)      You will have to create a App Management Service Application if you want to run SharePoint 2013 workflow and off course also start the “App Management Service“. Besides that the “Microsoft SharePoint Foundation Subscription Settings Service” should be started and you also have to make the Service Application by using PowerShell.  if ((Get-PSSnapin \"Microsoft.SharePoint.PowerShell\" -ErrorAction SilentlyContinue) -eq $null) {     Add-PSSnapin \"Microsoft.SharePoint.PowerShell\" }  $account = Get-SPManagedAccount devdevservice $appPool = New-SPServiceApplicationPool -Name SubscriptionServiceAppPool -Account $account  $serviceApp = New-SPSubscriptionSettingsServiceApplication -ApplicationPool $appPool -name \"Subscription Settings Service Application\" -DatabaseName \"SP2013_DEV_SubscriptionSettingsDB\"  $serviceAppProxy = New-SPSubscriptionSettingsServiceApplicationProxy -ServiceApplication $serviceApp  One other thing I noticed is that the workflow feature will have to be a Web scoped features. You should keep this in mind when designing your solutions.  ","categories": ["Development"],
        "tags": ["SharePoint 2013","Visual Studio","Worklow"],
        "url": "/2013/07/sharepoint-2013-workflow-with-visual-studio-2012/",
        "teaser": null
      },{
        "title": "Book Reviews",
        "excerpt":"As a IT consultant you always have to renew your skills and start to learn new technologies. In the last couple of months I have been reading a couple off books in order to learn new stuff and get the new certificates there are. To help my fellow IT consultants I will try the option of writing book reviews to help others to select books for their certifications paths or just to learn new technologies. I hope everyone will enjoy my book reviews. Please let me know what you think of this new step.  You can read all of the book reviews here: http://msftplayground.com/book-review/ or you can select one off my latest ones at the right side.   ","categories": ["General"],
        "tags": ["Book"],
        "url": "/2013/09/book-reviews/",
        "teaser": null
      },{
        "title": "TFS and SharePoint Builds including Automatic deployments &ndash; Part 1",
        "excerpt":"Part 1 of this series will instruct how to configure your build server for building SharePoint projects. In Part 2 we will discuss the automatic deploy options. Part 1 of this series is based on the following MSDN article How to Build Office Developer Tools Projects with TFS Team Build 2012. In this article there are some mistakes and are corrected here.  When user are using Frameworks like SCRUM you also have to look into automatic builds and automatic deployments to your test and acceptance environments in order to not lose a lot of times with you deployments.  Setting this up is quite some work. First off all you need to configure your build server in order to even be ready to build SharePoint projects. In this article we will go trough each step to configure your build server to build your SharePoint projects.   Install TFS Team Build If your build system already has TFS Team Build installed on it, you can skip this step. Otherwise, install and configure Team Build on your build system.   Install Windows SDK The Windows SDK must be installed on your build system because it includes the GACUtil tool. GACUtil is used to install the SharePoint Tool assemblies on the build system. You can download the Windows SDK from this location: http://msdn.microsoft.com/en-us/windows/bb980924.aspx. After you download the Windows SDK, install it.   When installing the Windows SDK you can encounter the following error:  Installation of the “Microsoft Windows SDK for Windows 7″ product has reported the following error: Please refer to Samples\\Setup\\HTML\\ConfigDetails.htm document for further information  Looking at the log files you will see these two entries:  DDSet_Error: Patch Hooks: Missing required property 'ProductFamily': Setup cannot continue.  DDSet_Warning: Setup failed while calling 'getDLLName'. System error: Cannot create a file  This is because the Windows SDK installer is having problems with a currently installed component. Remove this component and it will work fine:   Visual C++ 2010 runtime  After that you can also install the Visual C++ 2010 Service Pack 1 Compiler Update   Install Build Support for Apps for Office and Apps for SharePoint To build apps for Office and apps for SharePoint projects, several components must be installed on your system.   Install Windows Identity Foundation (WIF)     Install WIF Extensions x64 or x86   Install SharePoint Client Component SDK  Install Workflow Client 1.0 and Workflow Manager 1.0 by using the Web Platform Installer.        Install SharePoint Farm and Sandboxed Solution Build Support Since SharePoint farm and sandboxed solution projects reference SharePoint Server assemblies, those SharePoint assemblies must be present on the build system.  One way to do this is to install the full version of SharePoint Server 2013 on your build system. An advantage of having SharePoint installed is that you can use it to deploy and test your SharePoint application immediately after generating the WSP file. Note, however, that SharePoint Server 2013 can degrade your system performance, and it has increased system requirements (such as requiring a 64-bit CPU).  As an alternative, you can download the required SharePoint assemblies onto your build system. For either method, you will need the proper license of SharePoint on the build system. Copy the following assemblies from the development system to the build system and put them in a Reference Assembly folder:    Microsoft.SharePoint.dll  Microsoft.SharePoint.Security.dll  Microsoft.SharePoint.WorkflowActions.dll  Microsoft.Office.Server.dll  Microsoft.Office.Server.UserProfiles.dll  Microsoft.SharePoint.Client.dll  Microsoft.SharePoint.Client.Runtime.dll  Microsoft.SharePoint.Client.ServerRuntime.dll  Microsoft.SharePoint.Linq.dll  Microsoft.SharePoint.Portal.dll  Microsoft.SharePoint.Publishing.dll  Microsoft.SharePoint.Taxonomy.dll  Microsoft.Office.SharePoint.Tools.dll  Microsoft.SharePoint.WorkflowActions.dll  Microsoft.Web.CommandUI.dll  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; These files are located in the following folder on the development system:  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C:\\Program Files\\Common Files\\Microsoft Shared\\Web Server Extensions\\15\\ISAPI &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; It is recommended that you copy the SharePoint Server assemblies to the folder: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .. \\Program Files\\Reference Assemblies\\SharePoint\\ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; And then add one of the following registry entries:   The Microsoft articles tells you to place the following registery key but it should be the one below.    Wrong: HKEY_LOCAL_SYSTEM\\SOFTWARE\\Microsoft\\Wow6432Node\\.NETFramework\\v4.0.30319\\AssemblyFoldersEx\\SharePoint15]@=\"&lt;AssemblyFolderLocation&gt;\"  Good: HKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\Microsoft\\.NETFramework\\v2.0.50727\\AssemblyFoldersEx\\SharePoint14]@=\"&lt;AssemblyFolderLocation&gt;\"  You could also use the following “reg” file to import the setting to the following location: \"C:\\\\Program Files\\\\Reference Assemblies\\\\SharePoint\\\\\".   SharePoint Registry  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If your SharePoint Projects refer to other SharePoint Server assemblies, copy them to the build system as well.    Install Office Developer Tools Assemblies to the GAC  In order accomplish this step you can do two things. When you already have Visual Studio 2012 installed install de developer tools or add all the right assemblies to the system.  Install developer tools  Open the Web Platform Installer and install the Microsoft Office Developer Tools for Visual Studio 2012. You will need to make sure you have installed the Service Bus 1.0 Cumulative Update before installing the developer tools.  Right Assemblies for the system  If you do not have Visual Studio Installed on the system you should follow the following steps:  The following assemblies must be copied to the GAC of the build system:   Microsoft.VisualStudio.SharePoint.Designers.Models.dll  Microsoft.VisualStudio.SharePoint.Designers.Models.Features.dll  Microsoft.VisualStudio.SharePoint.Designers.Models.Packages.dll  Microsoft.VisualStudio.SharePoint.dll  These files are located in the following folder on the development system: C:\\Windows\\Microsoft.NET\\assembly\\GAC_MSIL\\  If you also need localized versions of the files, you should also copy and install the corresponding resource DLLs to the GAC of the build system:    Microsoft.VisualStudio.SharePoint.Designers.Models.Features.resources.dll  Microsoft.VisualStudio.SharePoint.Designers.Models.Packages.resources.dll  Microsoft.VisualStudio.SharePoint.Designers.Models.resources.dll  Microsoft.VisualStudio.SharePoint.resources.dll  Next up is installing the required MSBuild dependencies. Copy and install the following custom targets and tasks folders to the corresponding folders on the build machine:    .. \\Program Files\\MSBuild\\Microsoft\\VisualStudio\\v11.0\\SharePointTools  .. \\Program Files\\MSBuild\\Microsoft\\VisualStudio\\v11.0\\WebApplications  .. \\Program Files\\MSBuild\\Microsoft\\VisualStudio\\v11.0\\Web  .. \\Program Files\\MSBuild\\Microsoft\\Workflow Manager\\1.0  (For 64-bit systems, use \"\\Program Files (x86)\" in the path above.)   As last Install Workflow Dependencies to the GAC   Microsoft.Activities.Design.dll  The file is located in the following folder on the development system:  C:\\Windows\\Microsoft.NET\\assembly\\GAC_MSIL\\     Create a package for the Office Developer Tools Project    When you also want your project to be packages you need to supply the following MSBuild argument to your build Definition:   /p:IsPackaging=true      ","categories": ["Development"],
        "tags": ["Deployment","SharePoint","Team Foundation Server","TFS"],
        "url": "/2013/10/tfs-and-sharepoint-builds-including-automatic-deployments-part-1/",
        "teaser": null
      },{
        "title": "Host-Named Site Collections (Provisioning by Code and in Specific Content Database)",
        "excerpt":"Host-named site collections are the preferred method to deploy sites in SharePoint 2013.   Because the Office 365 environment uses host-named site collections, new features are optimized for these site collections and they are expected to be more reliable.  Creating Host Named site collection in a on premise environment still needs to be done with the use of PowerShell.   New-SPSite 'http://dev.sharepoint.local' -HostHeaderWebApplication 'http://webapplication.sharepoint.local’ -Name 'Host-Named Site Collection' -Description 'My First Host-Name Site Collection' -OwnerAlias 'SharePoint\\spadmin' -language 1033 -Template 'STS#0'  When you would like to create the specific site collection in a separate content database you could use the following command.  New-SPSite 'http://dev.sharepoint.local' -HostHeaderWebApplication 'http://webapplication.sharepoint.local’ -Name 'Host-Named Site Collection' -Description 'My First Host-Name Site Collection' -OwnerAlias 'SharePoint\\spadmin' -language 1033 -Template 'STS#0' -ContentDatabase 'SP2013_Content_HostNamed'  For more information on the out of the box functionalities:   Host-named site collection architecture and deployment (SharePoint 2013)  If you work in a environment were site need to be provisioned&nbsp; you will have to create a solution to create host-named site collections.  But how do you create host-named site collection by using code (C#).  At first you will have to get a reference to the web application were in you would like to create the host-named site collection. After that you can just use the ‘Add’ method on the ‘Sites’ collection.  SPWebApplication webApp = SPWebApplication.Lookup(new Uri(\"http://webapplication.sharepoint.local\")); SPSite Site = webApp.Sites.Add(\"http://dev.sharepoint.local\", \"Host-Named Site Collection\",\"My First Host-Name Site Collection\", 1033, \"STS#0\", \"SharePoint\\spadmin\", \"SharePoint Administrator\", \"spadmin@sharepoint.local\", \"SharePoint\\poweruser”,\"Power User\", \"poweruser@sharepoint.local\", true);  But what should you do if you would like to create a host-named site collection in a specific content database in C#.  bool retVal = false; SPWebApplication webApp = SPWebApplication.Lookup(new Uri(hostheaderApplication)); SPContentDatabase newDatabase = null;  if (webApp != null) {      bool continueValue = false;     if (!string.IsNullOrEmpty(contentDatabase)) {         //check if database exists         var query = from SPContentDatabase database in webApp.ContentDatabases                     where database.Name == contentDatabase                     select database;          newDatabase = query.FirstOrDefault();         continueValue = newDatabase == null ? false : true;     } else {         continueValue = true;     }      if (continueValue) {         SPSite createdSite = webApp.Sites.Add(url, name, description, languageId, template, ownerAlias, string.Empty, string.Empty, ownerAlias, string.Empty, string.Empty, true);          SPContentDatabase database = createdSite.ContentDatabase;         if (database.Name != contentDatabase) {             Dictionary&lt;SPSite, string&gt; failed = new Dictionary&lt;SPSite, string&gt;();              database.Move(newDatabase, new List&lt;SPSite&gt;() { createdSite }, out failed);              if (failed.Count &gt; 0) {                 createdSite.Delete();             } else {                 retVal = true;             }         } else { ","categories": ["Development"],
        "tags": ["C#","Host-Names site collection","SharePoint 2013"],
        "url": "/2013/10/host-named-site-collections-provisioning-by-code-and-in-specific-content-database/",
        "teaser": null
      },{
        "title": "SharePoint 2013 and Visual Studio 2013",
        "excerpt":"For a few weeks I have been using Visual Studio on my normal development environment. Yesterday I decided to install it on a SharePoint 2013 development environment we have at a client.  This development machine already had a Visual Studio 2012 installation so we first uninstalled the 2012 installation. After the uninstall we installed Visual Studio 2013. After rebooting machine strange things started to happen.   SharePoint Designer could not connect any more to my SharePoint sites.  SharePoint Controls did not work anymore. The content search web part always returned a error:  Unable to load one or more of the requested types. Retrieve the LoaderExceptions property for more information.   Errors in the ULS log:  11/21/2013 14:35:27.81&nbsp;&nbsp;&nbsp;&nbsp; w3wp.exe (0x0DA0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0x23B4&nbsp;&nbsp;&nbsp; SharePoint Server Search&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Query&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; afpkb&nbsp;&nbsp;&nbsp; Unexpected&nbsp;&nbsp;&nbsp; Getting results failed: System.Reflection.ReflectionTypeLoadException: Unable to load one or more of the requested types. Retrieve the LoaderExceptions property for more information.&nbsp;&nbsp;&nbsp;&nbsp; at System.Reflection.RuntimeModule.GetTypes(RuntimeModule module)&nbsp;&nbsp;&nbsp;&nbsp; at System.Reflection.RuntimeModule.GetTypes()&nbsp;&nbsp;&nbsp;&nbsp; at System.Reflection.Assembly.GetTypes()&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.ProxyMap.ProcessOneAssembly(Assembly azzembly, ClientServiceHost processorSurrogate)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.ProxyMap.Init(ClientServiceHost processorSurrogate)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.ProxyMap.EnsureInited(ClientServiceHost processorSurrogate)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.ProxyMap.GetServerStub(Type type, ClientServiceHost processorSurrogate)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.DataConv...&nbsp;&nbsp;&nbsp;&nbsp;  11/21/2013 14:35:27.81*&nbsp;&nbsp;&nbsp; w3wp.exe (0x0DA0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0x23B4&nbsp;&nbsp;&nbsp; SharePoint Server Search&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Query&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; afpkb&nbsp;&nbsp;&nbsp; Unexpected&nbsp;&nbsp;&nbsp; ...erter.WriteAsJson(JsonWriter writer, Object obj, ProxyContext proxyContext, JsonSerializationOptions options)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.JsonUtility.SerializeToJson(Object value, TextWriter writer, JsonSerializationOptions options)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.JsonUtility.SerializeToJson(Object value)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.Office.Server.Search.WebControls.DataProviderScriptWebPart.GetInitialResult()&nbsp;&nbsp;&nbsp;&nbsp;  11/21/2013 14:35:27.81&nbsp;&nbsp;&nbsp;&nbsp; w3wp.exe (0x0DA0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0x23B4&nbsp;&nbsp;&nbsp; SharePoint Server Search&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Query&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; afpkb&nbsp;&nbsp;&nbsp; Unexpected&nbsp;&nbsp;&nbsp; Getting results failed: System.Reflection.ReflectionTypeLoadException: Unable to load one or more of the requested types. Retrieve the LoaderExceptions property for more information.&nbsp;&nbsp;&nbsp;&nbsp; at System.Reflection.RuntimeModule.GetTypes(RuntimeModule module)&nbsp;&nbsp;&nbsp;&nbsp; at System.Reflection.RuntimeModule.GetTypes()&nbsp;&nbsp;&nbsp;&nbsp; at System.Reflection.Assembly.GetTypes()&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.ProxyMap.ProcessOneAssembly(Assembly azzembly, ClientServiceHost processorSurrogate)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.ProxyMap.Init(ClientServiceHost processorSurrogate)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.ProxyMap.EnsureInited(ClientServiceHost processorSurrogate)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.ProxyMap.GetServerStub(Type type, ClientServiceHost processorSurrogate)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.DataConv...&nbsp;&nbsp;&nbsp;&nbsp;  11/21/2013 14:35:27.81*&nbsp;&nbsp;&nbsp; w3wp.exe (0x0DA0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0x23B4&nbsp;&nbsp;&nbsp; SharePoint Server Search&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Query&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; afpkb&nbsp;&nbsp;&nbsp; Unexpected&nbsp;&nbsp;&nbsp; ...erter.WriteAsJson(JsonWriter writer, Object obj, ProxyContext proxyContext, JsonSerializationOptions options)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.JsonUtility.SerializeToJson(Object value, TextWriter writer, JsonSerializationOptions options)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.JsonUtility.SerializeToJson(Object value)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.Office.Server.Search.WebControls.DataProviderScriptWebPart.GetInitialResult()&nbsp;&nbsp;&nbsp;&nbsp;  11/21/2013 14:35:27.81&nbsp;&nbsp;&nbsp;&nbsp; w3wp.exe (0x0DA0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0x23B4&nbsp;&nbsp;&nbsp; SharePoint Server Search&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Query&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; afpkb&nbsp;&nbsp;&nbsp; Unexpected&nbsp;&nbsp;&nbsp; Getting results failed: System.Reflection.ReflectionTypeLoadException: Unable to load one or more of the requested types. Retrieve the LoaderExceptions property for more information.&nbsp;&nbsp;&nbsp;&nbsp; at System.Reflection.RuntimeModule.GetTypes(RuntimeModule module)&nbsp;&nbsp;&nbsp;&nbsp; at System.Reflection.RuntimeModule.GetTypes()&nbsp;&nbsp;&nbsp;&nbsp; at System.Reflection.Assembly.GetTypes()&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.ProxyMap.ProcessOneAssembly(Assembly azzembly, ClientServiceHost processorSurrogate)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.ProxyMap.Init(ClientServiceHost processorSurrogate)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.ProxyMap.EnsureInited(ClientServiceHost processorSurrogate)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.ProxyMap.GetServerStub(Type type, ClientServiceHost processorSurrogate)&nbsp;&nbsp;&nbsp;&nbsp; at Microsoft.SharePoint.Client.DataConv...&nbsp;&nbsp;&nbsp;&nbsp; We started looking for the problem and tried the following list already:   Uninstall the Office developer tools for Visual Studio 2012. Repair the SharePoint 2013 installation. Run the Configuration wizard of SharePoint. This also returned a error. Looking in the upgrade log we could not find a real exception.  After all this work we decided to reinstall Visual Studio 2012 to check if that would work and it did!! For now we choose to run both version on our development system. If there is someone that knows what is going wrong please let us know.   ","categories": ["Development"],
        "tags": ["SharePoint 2013","Visual Studio 2013"],
        "url": "/2013/11/sharepoint-2013-and-visual-studio-2013/",
        "teaser": null
      },{
        "title": "Event ID 1008 Event log message of the &ldquo;Perflib&rdquo; Source",
        "excerpt":"When I was setting up a new SharePoint environment this week I was getting very annoyed about perflib error messages within the event log:  “The Open Procedure for service \"WmiApRpl\" in DLL \"C:\\Windows\\system32\\wbem\\wmiaprpl.dll\" failed. Performance data for this service will not be available. The first four bytes (DWORD) of the Data section contains the error code.”  Looking on TechNet I found the below article for fixing the problem.   http://technet.microsoft.com/en-us/library/cc774913(v=ws.10).aspx  Taking these steps made no change. After that I was getting more annoyed and called a colleague for advice. He pointed to me that another colleague had already blogged about this a few months ago.  Take a look at his blog post to fix this issue:   Wesley Bakker - Solving WmiApRpl and BITS errors with SharePoint 2013 on Windows Server 2012  ","categories": ["Administration"],
        "tags": ["Error","Issues","SharePoint 2013","Windows Server 2012"],
        "url": "/2013/11/event-id-1008-event-log-message-of-the-perflib-source/",
        "teaser": null
      },{
        "title": "Setting up navigation based on search for Site collections",
        "excerpt":"At the moment we are creating a portal based on host named site collections. The root site collection needs to display the sub site collections you have access to.  To give you a better understanding of what we created take a look at the following image:     Every site in the above image is a host named site collection. We are using separate site collections to be able to move sites between databases because the sites them selves will contain a large amount of data.   For the creation of the sub site collection we have created a custom provisioning method. How to do this provisioning can be read in my other article:   Host named site collections provisioning by code and in specific content databases  But now we had the challenge to display all the sites the current user has access to on the root site.  When we had setup the environment with the host named site collection it was time to create the representation of the sites on the root site (http://root).  First of all we started by adding the “Content Search” web part to the page that can be found in the “Content Rollup” category.     This web part will display several results already based on a empty search query. By editing the web part you will be able to define your own query.      Select the “Change Query” button to start defining your own query. Right away click on the “Switch to Advanced Mode” to start writing your own query. In the advanced window you will have a textbox to define a custom query.      Take a look at the following query we defined:  contentclass:STS_Site Path:{Site.Url}* Site&lt;&gt;{Site.Url}  The explanation of the query is the following:   contentclass:STS_Site : This will return every object within SharePoint that is of the type STS_Site. STS_Site stands for Site Collection. You also have STS_Web that stands for Web Site. Path:{Site.Url}* : This will return only the sites that start with the current URL or have the current URL in the path variable. This means you will also retrieve the current site as a result. Site&lt;&gt;{Site.Url} : This will return everything but the current site.  So by using this query you will retrieve every site collection under (URLwise) the current site collection that you have access to (Because search results are security trimmed).  For displaying these sites you can use several out of the box managed properties. Save the query by clicking “OK”. Then in the edit properties window extend the “Property Mappings” section.     In the screen shot above you can see several properties of a site that then result in the following:     The title’s links to the correct site. The logo’s of the sites are displayed when they have a custom logo attached. The description is also displayed and because we do not have any owners specified in this demo you can not see the result of this.  Within a custom portal you also have the option of using property bag properties within your search queries or your results, how this can be accomplished can be read here:   using SharePoint property bag in the context of search.  ","categories": ["SharePoint"],
        "tags": ["Host Named Site Collections","Search","SharePoint 2013"],
        "url": "/2013/12/setting-up-navigation-based-on-search-for-site-collections/",
        "teaser": null
      },{
        "title": "SharePoint 2013 and Biztalk 2013 (On-Premise)",
        "excerpt":"For a couple of days we have been trying to connect our On-Premise SharePoint 2013 environment with our On-Premise BizTalk 2013 environment using the SharePoint adapter.  During this process we encountered several errors that I would like to share with you.  After configuring the SharePoint Adapter (you can read about it on MSDN here) and trying to send our first document we encountered the first error message:  The adapter failed to transmit message going to send port \"SendPort-SP\" with URL \"wss://sharepointsite.com:80/sites/data/DropOffLibrary\". It will be retransmitted after the retry interval specified for this Send Port. Details:\"The Windows SharePoint Services site was not found. The URL \"http://sharepointsite.com:80/sites/data/DropOffLibrary\" points to a SharePoint object for which there is no Windows SharePoint Services site. Resolving this error is relatively easy. By enabling the “Windows Identity Framework” (WIF) within Windows Server 2012 this error will be resolved (Thanks Vincent Rouet). The second error that we received look like this: A message sent to adapter \"Windows SharePoint Services\" on send port \"SharePoint\" with URI \"wsss://sharepointsite.com:80/sites/data/DropOffLibrary\" is suspended.   Error details: [System.ServiceModel.CommunicationObjectFaultedException] The communication object, System.ServiceModel.Channels.ServiceChannel, cannot be used for communication because it is in the Faulted state. Reading about this error concluded that this had to be some kind of security issue. We had filled in a domain user in the “SharePoint Online Password”&nbsp; and “SharePoint Online Username“ field of the adapter. When we changed the right properties to a Office 365 location using a Online login account everything seemed to be working fine. Leaving the “SharePoint Online Password”&nbsp; and “SharePoint Online Username“ field of the adapter empty raised a security exception. After some logical thinking we figured out the the account running the host should have access to the SharePoint. So what we did is creating a separate Host within BizTalk with a specific domain account and attach it to the SharePoint adapter. When you then leave the “SharePoint Online Password”&nbsp; and “SharePoint Online Username“ fields empty it will use the account that is specified for the host to add the documents to SharePoint.      ","categories": ["Azure DevOps"],
        "tags": ["BizTalk 2013","SharePoint 2013","SharePoint Adapter"],
        "url": "/2013/12/sharepoint-2013-and-biztalk-2013-on-premise/",
        "teaser": null
      },{
        "title": "SharePoint License Versions versus Versions",
        "excerpt":"As a SharePoint developer you do not often get the question what SharePoint version do we need to install. In many situations we see that the Enterprise license is installed.  For a project I’m currently working on I got the question to stick to the SharePoint 2013 Standard license. Having this requirement we were looking for a list of features we could use within a Standard license.  Searching on the internet supplied as a lot of resources but all these resources were all very different.  Finally we found a link that pointed us to a TechNet article:   SharePoint Online Service Description  This article contains a lot of information regarding the following things:   Differences in features across Office 365 plans. Subscription options. SharePoint Online boundaries and limits. Differences in features across standalone plans. Differences in features across on-premise solutions (Differences between: SharePoint Foundation, Standard and Enterprise).  ","categories": ["Administration","SharePoint"],
        "tags": ["license","SharePoint 2013"],
        "url": "/2013/12/sharepoint-license-versions-versus-versions/",
        "teaser": null
      },{
        "title": "PowerShell Scripts",
        "excerpt":"During my current project we are using a lot of PowerShell scripts to administrate the environment and make development easier for us.  In this post that I will try to update frequently, I will share some of our useful PowerShell scripts.  Disabling or Enabling Access Request  Within our project we had an application with about 400 sites. All of these sites still had the Access Request enabled. We needed to disable these access requests. For this we created a PowerShell function you can use to enable and disable access requests.  ### # Function for enabling and disabling site access requests ### function Set-AccessRequest{     [CmdletBinding()] \tparam ( \t\t[string] $siteUrl = $(Read-Host -prompt \"Site Url\"),         [bool] $enable  =[System.Convert]::ToBoolean($(Read-Host -prompt \"Enable Access Requests\")),         [string] $email = $(Read-Host -prompt \"Email\") \t)         $web = Get-SPWeb $SiteUrl;      if($enabled){         Write-Host \"Enabling Site Access Requests for site: \" $web.Url \" for email: \" $email -ForegroundColor Green;          $web.RequestAccessEmail = $email;     }else{         Write-Host \"Disabling Site Access Requests for site: \" $web.Url -ForegroundColor Green;          $web.RequestAccessEmail = \"\";     }      $web.Update(); }  &nbsp;  Publish files within a site collection  In some situations you would like to have the ability to publish all files within a specific site collection. For example right before going live with a SharePoint publishing portal you need to be sure that every file is published or else end users could  get a access denied on certain resources.  ### # Function for Publishing files in SharePoint ### function Publish-FilesInList(){     param     (         [string] $siteUrl = $(Read-Host -prompt \"Site Url\"),         [bool] $checkIn = $(Read-Host -prompt \"CheckIn when file is CheckedOut\"),         [bool] $debug = $(Read-Host -prompt \"Run Debug mode\")     )       Start-SPAssignment -Global     $site = Get-SPSite $siteUrl     Write-Host \"- Publishing files for the Site Collection:\" $site.Url -ForegroundColor Cyan      foreach($web in $site.AllWebs){              Write-Host \" - Publishing files for the Web:\" $web.Url -ForegroundColor Cyan                  foreach($list in $web.Lists){             if(!$list.Hidden -And !$list.IsPrivate -AND !$list.IsApplicationList -AND $list.BaseType -eq 1){                 Write-Host \"  - Publishing files for the List:\" $list.Title -ForegroundColor Cyan                 ProcessFolderGetFiles $list.RootFolder $debug             }else{                 Write-Host \"List is Private, ApplicationList, Hidden or no document library: \" $list.Title -ForegroundColor Magenta             }         }     }      Stop-SPAssignment -Global }  function ProcessFolderGetFiles(){     param(         $folder,         $debug     )     if($debug){Write-Host \"    - Processing Folder:\" $folder.Name -ForegroundColor Cyan}      foreach($file in $list.RootFolder.Files){         if($debug){Write-Host \"     - Publishing file:\" $file.Name -ForegroundColor Cyan}         if ($file.Level -ne \"Published\"){             if (($file.Level -eq \"Checkout\" -or $file.CheckOutStatus -ne \"None\")-and $checkIn){                 Write-Host \"      - File is checkedout\" -ForegroundColor Yellow                 if($checkIn){                     Write-Host \"     - CheckingIn File\" -ForegroundColor Yellow                     $file.CheckIn(\"CheckingIn\", [Microsoft.SharePoint.SPCheckinType]::MajorCheckIn);                     $file.Publish('Published using Motion10 Powershell Module');                     Write-Host \"      - File Published\" -ForegroundColor Yellow                 }             }else{                 $file.Publish('Published using Motion10 Powershell Module');                 Write-Host \"      - File Published\" -ForegroundColor Yellow             }              $file.Update();         }else{             if($debug){Write-Host \"      - File already published\" -ForegroundColor Cyan}         }     }     # Use recursion to loop through all subfolders.     foreach ($subFolder in $folder.SubFolders){        ProcessFolderGetFiles($Subfolder)     } }  Unregister a Custom Claim Provider for a specific zone in a web application  There are situations were for you will develop custom claim providers. These claim providers can be installed globally on the farm but can also be installed for a specific zone in a web application. The following script will allow you to unregister a custom claim provider for a zone.  function Unregister-CustomClaimProvider{ \tparam( \t\t  [string] $DisplayName = $(Read-Host -prompt \"Claim Provider Display Name\"), \t\t  [string] $Url = $(Read-Host -prompt \"Web Application Identity\"), \t\t  [string] $InternalName = $(Read-Host -prompt \"Claim Provider Internal Name\"), \t\t  [string] $Zone = $(Read-Host -prompt \"Zone\") \t)  \t \t$WebApplication = Get-SPWebApplication $Url  \tif ($WebApplication.IisSettings.ContainsKey($Zone)){ \t\t$settings = $WebApplication.GetIisSettingsWithFallback($zone) \t\t$providers = $settings.ClaimsProviders; \t\tif($providers.Contains($InternalName)) { \t\t\tWrite-Host \"Claim Provider is registerd in this zone trying to remove...\" -ForegroundColor Green \t\t\t$providers.Remove($InternalName); \t\t\tSet-SPWebApplication -Identity $WebApplication -Zone $Zone -AdditionalClaimProvider $providers \t\t}else{ \t\t\tWrite-Host \"$DisplayName is not registered on $($WebApplication.Url) in zone $Zone\" -ForegroundColor Yellow \t\t} \t} }  Disable Browser Based Editing for Sites under a Specific Url  In certain situations you would like to disable all browser based editing for specific sites within SharePoint. With the below PowerShell script you can disable browser based editing.  Get-SPSite -WebApplication [Web Application] -Limit ALL | where {$_.Url.StartsWith(\"[Url start With]\") } | Foreach-Object {      Write-Host \"Activating Feature Client Based Editing for site\" $_.Url     Enable-SPFeature –identity 8a4b8de2-6fd8-41e9-923c-c7c3c00f8295 -URL $_.Url }  Warm Up SharePoint 2013 Site Collections  When you maintain SharePoint environments you would like to warm up the environments for example once a day. The first load on SharePoint still takes a lot of time. For this situation I have developed a warm up script that also take in account your extended Host Named Site Collections.  function WarmUp-Url([string]$url){     Write-Host \"Warming up \" $url -ForegroundColor DarkYellow;     Invoke-WebRequest $url -UseDefaultCredentials -UseBasicParsing;  }  foreach ($WebApp in (Get-SPWebApplication -IncludeCentralAdministration)){     $sites = $WebApp | Get-SPSite -Limit All | Get-SPWeb -Limit All         foreach($site in $sites){             $html= WarmUp-Url -url $site.Url                       if($site.IsRootWeb){                 $extendedurls = Get-SPSiteURL $site.Url  -ErrorAction SilentlyContinue;                  foreach($url in $extendedurls){                     if($url.Url -ne $site.Url){                                                 $html = WarmUp-Url -url $url.Url;                     }                 }             }         $site.Dispose();     }    }  Publish Content Types for Specific group  When you start working with the Content Type Hub in SharePoint you will get in situations were you would like to publish multiple content types at once. In these situations you could use the below function to publish all the content types in a specific group.  Add-PSSnapin Microsoft.SharePoint.PowerShell -ErrorAction SilentlyContinue function Publish-ContentTypesForGroup {     param     (         [parameter(mandatory=$true)][string]$hubUrl,         [parameter(mandatory=$true)][string]$group     )       $site = Get-SPSite $hubUrl      if(!($site -eq $null))     {         $contentTypePublisher = New-Object Microsoft.SharePoint.Taxonomy.ContentTypeSync.ContentTypePublisher ($site)         $site.RootWeb.ContentTypes | ? {$_.Group -match $group} | % {              if($_.ReadOnly){                 Write-Host \"Content Type\" $_Name \" is Read Only\" -ForegroundColor Yellow;                 $_.ReadOnly = $false;                 $_.Update();             }             $contentTypePublisher.Publish($_)             write-host \"Content type\" $_.Name \"has been republished\" -foregroundcolor Green         }     } } Starting the Content Type Subscriber Job  When you work a lot with the Content Type Hub in SharePoint you would also like to start the Content Type Subscriber job by using PowerShell. The below script does the job the web application you specify.  Add-PSSnapin Microsoft.SharePoint.PowerShell -ErrorAction SilentlyContinue function Run-SPContentTypeHubJob([string]$url)  {      $job = Get-SPTimerJob -WebApplication $url | ?{ $_.Name -like \"MetadataSubscriberTimerJob\"}       if ($job -ne $null)       {            Write-host \"The timerjob will be started.....\" -ForeGroundColor Green         $job | Start-SPTimerJob           Write-host  \"Run the admin job...\" -ForeGroundColor Green         Start-SPAdminJob -ErrorAction SilentlyContinue                   }     else{ \tWrite-Host \"The Job could not be found\" -ForeGroundColor Red     } } Delete all items in a List  During many projects I had situations were I had to remove all items in a list. In this situation you can choose for removing the complete list en recreating it or you could delete every single item. With the below function you can delete every item in the list.  Add-PSSnapin Microsoft.SharePoint.PowerShell -ErrorAction SilentlyContinue function Clear-SPList {     param     (         [parameter(mandatory=$false)][string]$siteUrl,         [parameter(mandatory=$false)][string]$listname     )      $site = Get-SPSite $siteUrl;     $web = $site.OpenWeb();     $list = $web.Lists[$listname];      write-host \"Clearing SharePoint list: \" $list.Title -ForeGroundColor Green      $collListItems = $list.Items;     $count = $collListItems.Count - 1;     for($intIndex = $count; $intIndex -gt -1; $intIndex--)     {             $collListItems.Delete($intIndex);     }  } Finding content types that use a specific field  It ofton occurs that you would like to delete a site column but can't because it is used by a specific content type. It really takes a long time then to find the right content type. Using the below PowerShell function it is easy and fast to find your content type.  Add-PSSnapin Microsoft.SharePoint.PowerShell -ErrorAction SilentlyContinue function Find-ContenttypesByField {     param     (         [parameter(mandatory=$false)][string]$siteUrl,         [parameter(mandatory=$false)][string]$fieldname     )    $site = Get-SPSite $siteUrl   $web = $site.RootWeb;   $fields = $web.Fields;    $guid = $fields[$fieldname].id;    $ct = $web.AvailableContentTypes    for ($i=0; $i -lt $ct.count; $i++)    {     for ($j=0; $j -lt $ct[$i].fields.count; $j++)      {       if ($ct[$i].fields[$j].id -eq $guid)       {         Write-host $ct[$i].Name \" has column\";       }     }   } } Setting up Ping and SharePoint Trust Federation  For a environment we wanted to setup a SSO environment with for example K2. The most know situation is using ADFS. But at our client we were required to use Ping. The configuration an the SharePoint site looks very much the same as ADFS. For a reference I have included the PowerShell script in this post.  Add-PSSnapin Microsoft.SharePoint.PowerShell -ErrorAction SilentlyContinue  $certpath  = \"[Ping Certification Path]\" $stsname   = \"Auth-iLink\" $stsdesc   = \"PingFederate Claims Provider SharePoint\" $stsrealm  = \"sharepoint:ping:con\"  #realm configured on Ping $signinurl = \"[URL Ping]/idp/prp.wsf\"  Write-host \"Importing Certification on SharePoint Server (Trust relation).\" -ForegroundColor Green New-SPTrustedRootAuthority -Name \"PingFederate IP-STS Sharepoint\" -Certificate $certpath  Write-Host \"Getting the Certification Object.\" -ForegroundColor Green $cert = New-Object System.Security.Cryptography.X509Certificates.X509Certificate2($certpath)  Write-Host \"Setting up Claim Mapping for SharePoint Authentication\" -ForegroundColor Green $mappingRole = New-SPClaimTypeMapping -IncomingClaimType \"http://schemas.xmlsoap.org/claims/Group\" -IncomingClaimTypeDisplayName \"Role\" -LocalClaimType \"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\" $mappingName = New-SPClaimTypeMapping -IncomingClaimType \"http://schemas.xmlsoap.org/claims/CommonName\" -IncomingClaimTypeDisplayName \"GivenName\" -LocalClaimType \"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname\"  Write-Host \"Setting Up Token Issuer\" New-SPTrustedIdentityTokenIssuer -Name $stsname -Description $stsdesc -Realm $stsrealm -ImportTrustCertificate $cert -ClaimsMappings $mappingRole,$mappingName -SignInUrl $signinurl -IdentifierClaim $m2.InputClaimType  Write-Host \"Trusted Identity Token Issuer is set up.\" Delete empty sub folders  For a migration we had a large set of folder with also empty folders in it. During the migration we iterated trough the folders also going trough the empty folders took a lot of time for the migration process. That way we wanted a PowerShell script to delete the empty sub folders. (Remove the –whatif to actually perform the script).  Get-ChildItem -recurse | Where {$_.PSIsContainer -and ` @(Get-ChildItem -Lit $_.Fullname -r | Where {!$_.PSIsContainer}).Length -eq 0} | Remove-Item -recurse -whatif &nbsp;  Delete site that start with a specific URL.  When you are using host named site collections within your environment and also build some kind provisioning mechanism (how this can be done with host named site collections can be read here). You will get in situations that you would like to remove a huge number of site collections that start with a specific URL. By using this script you can:  Get-SPSite -Limit ALL | where {$_.Url.StartsWith(\"http://spdev.com/sites\") } | Foreach-Object { Remove-SPSite $_.Url –Confirm:$False }  Flushing the blob cache  Because we are using the blob cache within our environment, we also need a easy option to clear the blob cache when we have update our style sheets or maybe JavaScript files.  Using this function you can easily clear the blob cache for a specific web application.  function Flush-SPBlobCache{         param(           [string] $Url = $(Read-Host -prompt \"Web Application Identity\")         )                 $webApp = Get-SPWebApplication $Url         [Microsoft.SharePoint.Publishing.PublishingCache]::FlushBlobCache($webApp)         Write-Host \"Flushed the BLOB cache for:\" $webApp } &nbsp;  Feature upgrading  For upgrades on our solutions we use the standard upgrading framework of SharePoint. You can read about it here this is an article on the upgrading framework for SharePoint 2010 but it is also something for SharePoint 2013.  Using the following function you are able to upgrade a specific feature within a web application.  function Upgrade-SpecificFeatures{         param(           [string] $Url = $(Read-Host -prompt \"Web Application Identity\"),           [Guid] $ID = $(Read-Host -prompt \"Feature ID\") ","categories": ["Development"],
        "tags": ["blob cache","Hub","Powershell","SharePoint","SharePoint 2013","Sites","Upgrading"],
        "url": "/2014/01/powershell-scripts-2/",
        "teaser": null
      },{
        "title": "Filtering SharePoint Search Results by Content Source",
        "excerpt":"Working for one my projects we were looking for a solution to only retrieve search results for a specific Content Source. In our scenario we had a External Content Type that was linked to a “FilteredEMail” view of a CRM database.  We connected CRM to SharePoint in this scenario because searching in CRM for a specific email was a big problem and slowed down the complete CRM system.   What there was implemented on CRM was a small button to open a window&nbsp; / frame to a SharePoint Search page. On this SharePoint search page you could search for the specific Mail and then the result would link you back to CRM.  Creating the External Content Type wasn’t that hard. The main thing here If you would also like to search trough your external content type is to define a field as a Timestamp field in order to enable incremental search.     For us this was very important because we were dealing with more than a million items. After we had the external content type in place and created the profile pages we crawled the complete set of emails.   We than needed to create a result source in order to retrieve only the information from the external content type. On the Search Service Application page we clicked on the ‘Result Sources’ menu option and created a new Search Result Source.      After setting these properties we opened the Query Builder. Within the query builder you do not have a default option of some sort to start filtering the results by a specific content source.   When I took a closer look at the property filter drop down after selecting “--- Show All Managed properties ---“ you have a property called “ContentSource”. We selected this and filled in the value of our content source.     After that we used the default options for the rest of the properties and saved the result source. Following this hunch resulted in a result source that only returns items of a specific content source.     After that we created a “Display Template” to refer back to CRM if you are curious on how we accomplished that please let me know.  ","categories": ["SharePoint"],
        "tags": ["Content Source","CRM","Result Source","Search","SharePoint 2013"],
        "url": "/2014/01/filtering-sharepoint-search-results-by-content-source/",
        "teaser": null
      },{
        "title": "Load modules by default when opening PowerShell",
        "excerpt":"At the moment we are working a lot with PowerShell in combination with SharePoint. Over time we created a lot of usable functions that were not availible within the default SharePoint Module.  With these functions we created our own PowerShell Module in order to easily load the function within our PowerShell instance. When you would like to know more on how to create a PowerShell Module you can read this TechNet article:   Writing a Windows PowerShell Module  When you have written a module it is relatively easy to load it in the PowerShell instance by using the import-module comment.  import-module [Path To Your Module]\\Module.psm1 -WarningAction Ignore As you can see in the above script we have added the \"–WarningAction ignore\" because loading a module mostly generate a warning for unapproved Verbs:  “WARNING: The names of some imported commands from the module 'Module' include unapproved verbs that might make them less discoverable. To find the commands with unapproved verbs, run the Import-Module command again with the Verbose parameter. For a list of approved verbs, type Get-Verb.”  If you want to stick with the approved Verbs you can take a look at the approved verb list on TechNet:   Approved Verbs for Windows PowerShell Commands  Now the question still is how do we load it in every PowerShell instance by default. You can accomplish this by creating a “profile.ps1” file. This file needs to be added in the following directory:  C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\  In the directory “C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\Example” there is a example profile.ps1 file. This file is empty. For our module we had created the following profile.ps1 file:  #  Copyright (c) Microsoft Corporation.  All rights reserved. #   # THIS SAMPLE CODE AND INFORMATION IS PROVIDED \"AS IS\" WITHOUT WARRANTY OF ANY KIND, # WHETHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE IMPLIED # WARRANTIES OF MERCHANTABILITY AND/OR FITNESS FOR A PARTICULAR PURPOSE. # IF THIS CODE AND INFORMATION IS MODIFIED, THE ENTIRE RISK OF USE OR RESULTS IN # CONNECTION WITH THE USE OF THIS CODE AND INFORMATION REMAINS WITH THE USER.  if ((Get-PSSnapin \"Microsoft.SharePoint.PowerShell\" -ErrorAction SilentlyContinue) -eq $null) {     Write-Host \"Loading Microsoft SharePoint PowerShell\" -ForegroundColor Green     Add-PSSnapin \"Microsoft.SharePoint.PowerShell\" }  Write-Host \"Loading Custom SharePoint PowerShell Module\" -ForeGroundColor Green import-module D:\\SCRIPTMODULE\\Module.psm1 -WarningAction Ignore Using the profile.ps1 file make sure that the profile is loaden in every powershell window. You could also use this filename: \"Microsoft.PowerShellISE_profile.ps1\" to only load within the PowerShell ISE window.  ","categories": ["Development"],
        "tags": ["Module","Powershell","SharePoint"],
        "url": "/2014/01/load-modules-by-default-when-openening-powershell/",
        "teaser": null
      },{
        "title": "Unable to start User Profile Synchronization Service",
        "excerpt":"A few days ago I wrote a post about loading modules in PowerShell by default: “Load modules by default when opening PowerShell”. Today we had to setup a new SharePoint Farm and to perform some default operations we created the PowerShell profile described in that post.  The complete configuration of the server went well until we wanted to start the “User Profile Synchronization Service”. The Service moves to starting and the timer job that configures the service also starts but after 5 seconds it already stops.  Looking at the ULS log we found this error message:  “UserProfileApplication.SynchronizeMIIS: Failed to configure MIIS post database, will attempt during next rerun. Exception: System.Runtime.InteropServices.SEHException (0x80004005): External component has thrown an exception.  at Microsoft.Office.Server.UserProfiles.Synchronization.ILMPostSetupConfiguration.ConfigureMiisStage2()  at Microsoft.Office.Server.Administration.UserProfileApplication.SetupSynchronizationService(ProfileSynchronizationServiceInstance profileSyncInstance).”  This message did not gave us a clue at all. At the same time we were working on the PowerShell Profiles and had temporary removed our custom profile from the service.  Provisioning the “User Profile Synchronization Service” without the custom profile resulted in success. So remember when you want to provision the “User Profile Synchronization Service” make sure you are not using a custom PowerShell profile.  ","categories": ["SharePoint"],
        "tags": ["Administration","Error","Exception","SharePoint 2013","Userprofile Synchronization"],
        "url": "/2014/02/unable-to-start-user-profile-synchronization-service/",
        "teaser": null
      },{
        "title": "My daughter Fiene Vlinder van der Gaag",
        "excerpt":" The last couple of days I haven't been blogging as much as I would like. I also think in the upcoming month I will not have much time to blog.  This is all because my amazing girlfriend gave birth to a really beautifull daughter on 24-02-2014, Fiene Vlinder. As soon as I have some more time I will pick up blogging again and will be posting some interesting Microsoft stuff.  [caption id=\"attachment_870\" align=\"alignnone\" width=\"300\"] Fiene Vlinder[/caption]   ","categories": ["General"],
        "tags": ["Daughter"],
        "url": "/2014/03/my-daughter-fiene-vlinder-van-der-gaag/",
        "teaser": null
      },{
        "title": "Data Deduplication in Windows Server 2012 R2",
        "excerpt":"After reading a lot of information about Windows Server 2012 R2 I found out that it has a really nice feature if you are into virtualization as much as me. When using virtualization you may have a lot of virtual disks that contain a lot of the same data. For example you have four machines that have an installation of Windows Server 2012.   All together this will take up a lot off disk space. I this situation you could also create a base image and use this base image for every virtual machine. But you will then still have the updates and you will have to maintain the base image.  In Windows Server 2012 you already had the feature off Data Deduplication for File and Storage Servers. What it does is the following: It finds and removes duplication within data on a volume while ensuring that the data remains correct and complete. This makes it possible to store more file data in less space on the volume.  A nice image created by Guido van Brakel in the following blog post: Data Deduplication in Windows Server 2012 really explains it:  [caption id=\"attachment_876\" align=\"alignnone\" width=\"300\"] Data Deduplication[/caption]  In Windows Server 2012 R2 they have added a new functionality called:   Data deduplication for remote storage of Virtual Desktop Infrastructure (VDI) workloads  This means you can use the Data Deduplication for your virtual machines when you are running Windows Server 2012 R2 meaning that when you have four virtual machines running the same operating system you will not have to take up all that amount off disk space.  Take a look at the following articles if you want to get started with Data Deduplication:   What's New in Data Deduplication in Windows Server 2012 R2 About Data Deduplication Plan to Deploy Data Deduplication  ","categories": ["Administration"],
        "tags": ["Data","Deduplication"],
        "url": "/2014/03/data-deduplication-in-windows-server-2012-r2/",
        "teaser": null
      },{
        "title": "Limitations of a Public Facing Website in Office 365",
        "excerpt":"Update :Microsoft is stopping with public facing websites for new Office 365 clients, existing ones will have another 2 year support. http://www.zdnet.com/article/microsoft-confirms-it-is-dropping-public-website-feature-from-sharepoint-online/  When you buy an Office 365 tenant you get the option to build a Public website within the SharePoint administration panel.  Creating a website will give you the following URL by default:   http://[tenant]-public.sharepoint.com.  Within SharePoint 2013 they have extended the options to build massive websites where you had to do a lot of customizations in SharePoint 2010. There are already many site available for example (http://www.motion10.nl ) but most off these websites are build on SharePoint 2013 on premise. This is all because building a public website within Office 365 has a lot of limitations. In this blog post I will try to list the limitations I know. If you find any other limitations or they have changed something on the Office 365 platform to make it work and I haven’t alter the blog post please add a comment or send me a email.  Site Settings  Multiple site settings screens are missing from the administration menu. Some of these pages can be accessed by the Url and some of them don’t even exist.     Manage Site Collection Features: The link and the page are missing for a public website. Manage Site Features: The link and the page are missing for a public website. Site Content Types: The link and the page are missing for a public website. Site Columns: The links is missing for a public website you can access the page by using the following URL: [URL]/_layouts/15/mngfield.aspx. Solution Gallery: The links is missing for a public website you can access the page by using the following URL: [URL]/_catalogs/solutions/Forms/AllItems.aspx. Web Part Gallery: The links is missing for a public website you can access the page by using the following URL: [URL]/_catalogs/wp/Forms/AllItems.aspx. List Template Gallery: The links is missing for a public website you can access the page by using the following URL: [URL]/_catalogs/lt/Forms/AllItems.aspx. Term store management: The link and the page are missing for a public website.  If you would like to activate or see which features are activated on the SharePoint Public site take a look at this blog post:   Feature Manager Web Part for Office 365 Public Facing SharePoint 2013 Site  Variations  Many website on the internet are created in multiple languages. Looking at SharePoint you would then maybe would like to use Variations. In Office 365 variations cannot be used.  Managed Metadata  As you can see on point 8 within the site settings section you are not able to access the Term Store Management. This also makes it that one off the amazing new features off SharePoint 2013 taxonomy-based navigation cannot be used on a Office 365 public website.  Besides that more taxonomy based features do not work as for example the product catalog functionality.  Sub Sites  From the UI you cannot create a sub site.  You will have to use SharePoint Designer in order to create a sub site. But even then you are not able to use every site template. In my situation I tried to create a Publishing Site and it got immediately deleted from my tenant.  The only templates that can be used on the online version are:   Team Site Basic Search Site Content Search Web Part:One of the web parts you had to develop for previous version of SharePoint (At least I did) now came to SharePoint 2013 and that is the “Content Search Web Part”. This web part is not available on the public web site of Office 365.  Online you can find great tips to deal with the limitations of the Public website:   Webbrewers on Office 365 Public Website help for Office 365  ","categories": ["Development"],
        "tags": ["Office 365","Pubilc website","SharePoint 2013"],
        "url": "/2014/03/limitations-of-a-public-facing-website-in-office-365/",
        "teaser": null
      },{
        "title": "Warm Up Form Based Web Application",
        "excerpt":"Normal warm up scripts for SharePoint open a site object or web object and make a request to that specific site. When you use form based authentication and do not allow anonymous access, you would only warm up the login page.  In order to warm up a application form based SharePoint site you would have to apply your credentials. All off this can be done by using PowerShell and specific COM objects. This script can then be scheduled with “Task Scheduler” that is included in windows.  function Warm-FBASite{     [CmdletBinding()]     param (         [string] $RootUrl = $(Read-Host -prompt \"Root Url\"),         [string] $LoginUrl = $(Read-Host -prompt \"Login Url\"),         [string] $SiteUrl = $(Read-Host -prompt \"Site Url\"),         [string] $UserName = $(Read-Host -prompt \"Username\"),         [string] $Password = $(Read-Host -prompt \"Password\"),         [bool] $loggedIn = $(Read-Host -prompt \"Already logged in\")     )     $ieMain=New-Object -ComObject \"InternetExplorer.Application\";     if($loggedIn -ne $true){         Login-FBA -RootUrl $RootUrl -LoginUrl $LoginUrl -UserName $UserName -Password $Password -Browser $ieMain;     }     Navigate-Site -RootUrl $RootUrl -SiteUrl $SiteUrl;     $ieMain.Quit();  }  The Warm-FBASite function is the main function that calls other functions. This function needs the following parameters:   RootUrl: The root URL of the application you would like to warm up.  LoginUrl: The relative URL to the login page.  SiteUrl: The relative URL to the site you would like to warm up.  UserName: The username off the forms user.  Password: The password off the forms user.  LogginIn: Boolean value whether the user has already been logged in.  In this function we open a Com object of internet explorer and provide it as a parameter to the function “Login-FBA”.  function Login-FBA{     [CmdletBinding()]     param (         [string] $RootUrl = $(Read-Host -prompt \"Root Url\"),         [string] $LoginUrl = $(Read-Host -prompt \"Login Url\"),         [string] $UserName = $(Read-Host -prompt \"Username\"),         [string] $Password = $(Read-Host -prompt \"Password\"),         $Browser = $(Read-Host -prompt \"Browser\")     )      $url = $RootUrl + $LoginUrl;      Write-Host \"Trying to login the user:\" $UserName \" on the Url: \" $url -ForegroundColor Green     $Browser.navigate($url);     $Browser.visible=$false;      if ((Wait-Browser $Browser -Url $url ) -eq $false){         Write-Host \"Something went wrong with requesting the page\";         return     }else{         $doc=$Browser.Document     }      $txtUsername=$doc.getElementByID(\"ctl00_PlaceHolderMain_signInControl_UserName\"); #Replace by your own ID     $txtPassword=$doc.getElementByID(\"ctl00_PlaceHolderMain_signInControl_password\"); #Replace by your own ID     $btnSubmit=$doc.getElementByID(\"ctl00_PlaceHolderMain_signInControl_login\"); #Replace by your own ID          $txtUsername.value=$UserName;     $txtPassword.value=$Password;     $btnSubmit.click();     if ((Wait-Browser $Browser -Url $url ) -eq $false){         return;     } }  This function tries to open the login page in the com object you send in the parameters. When this page is loaded it inserts the username and password in the login controls by finding these controls by there id.  In this function you will have to insert your own Id’s of the username textbox, password textbox and submit button. In this function you also see a reference to the “Wait-Browser” function. This function is created to make sure the site you are requesting in the COM object is fully loaded.  function Wait-Browser{     [CmdletBinding()]     param (             $Browser = $(Read-Host -prompt \"Browser\"),             $Url = $(Read-Host -prompt \"Url\")     )      $maxRetries=50;     $retry = 2;     $retryCount = 0;      while ($Browser.Busy -eq $true){           Write-Host \"Waiting for browser: \" $URl  -ForeGroundColor Yellow;                  if ($retryCount -gt $maxRetries){             return $false;         }                  $retryCount++;         start-sleep $retry ;     }       return $true; }  When the login request succeeds it is time to open the site and let that document load within the COM object. When the site is loaded it will write the page title to the PowerShell window.  function Navigate-Site{     [CmdletBinding()]     param (         [string] $RootUrl = $(Read-Host -prompt \"Root Url\"),         [string] $SiteUrl = $(Read-Host -prompt \"Site Url\")     )      $url = $RootUrl + $SiteUrl;      $iePage=New-Object -ComObject \"InternetExplorer.Application\";     $iePage.navigate($url);     $iePage.visible=$false;     $retryCounter=0           if ((Wait-Browser $iePage -Url $url ) -eq $false){         Write-Host \"Something went wrong with requesting the page\" -ForegroundColor Red;         return     }else{ ","categories": ["Development"],
        "tags": ["FBA","Powershell","SharePoint","Warm Up"],
        "url": "/2014/04/warm-up-form-based-web-application/",
        "teaser": null
      },{
        "title": "User automatically removed from SharePoint Group",
        "excerpt":"During my current project we received an access request from a user. We did what we normally do, we checked the request and added the user in the appropriate SharePoint Group and notified the user.   After 15 minutes the user told us that it wasn’t working. We looked at the User Group and found out that the user was not in the group. So we added the user again because we thought that we forgot this step. After some time we received another mail from the user telling us that it still wasn’t working.  We requested the account details from the user and tried it our self, we found out that the user was missing again and added the account again. We logged in with the user opened a document with Office Web Applications and took another look at the members of the specific SharePoint group. WTF the user was gone!  After digging into the ULS files and looking in the Event log we did not found a clue of what was happening. After a small conversation with the user she told us that everything worked fine before and that she had several problems since see re-joined the company.  What happened was the following the user was removed from the active directory and when she got back a new account was created with the same login name.  When the user re-joined the account had the same login name but did not have the same SID. SharePoint saves information about an account by using the login name and the SID (Security Identifier) of a user. Because of the mismatch SharePoint was removing the applied security rights.  This issue can be fixed by using different commands:   Move-SPUser : http://technet.microsoft.com/en-us/library/ff607729(v=office.15).aspx  STSADM Migrate User: http://technet.microsoft.com/en-us/library/cc262141(v=office.12).aspx  The commands look as followed:  $user = Get-SPUser -Web [SiteUrl] -Identity [Login Name] Move-SPUser -Identity $user -NewAlias [Login Name] -IgnoreSID  For the PowerShell commando you will first need to retrieve the specific user.When retrieving the user and setting the new alias you should always use the claim login name: “i:0#.w|Domain\\user”.  stsadm -o migrateuser -oldlogin [Login Name] -newlogin [Login Name] -ignoresidhistory  The STSADM command looks almost the same as the PowerShell command if you also look at the parameters they both have something to Ignore the SID history. The parameter is included because if you do not include the parameter it will check the SID references and as we all know they do not match.  “Possible Problem: When performing one of these options you can receive a “Object reference not set to an instance of an object”. The solution to this problem is pretty simple, When it happens your user does not have enough rights. Try it with another account or give your current user rights to the User Profile Service Application.”  ","categories": ["Administration"],
        "tags": ["Active Directory","SharePoint","SharePoint 2013","SID","Users"],
        "url": "/2014/05/user-automatically-removed-from-sharepoint-group/",
        "teaser": null
      },{
        "title": "Custom list view by using the &ldquo;JS Link&rdquo; property.",
        "excerpt":"Customizing a list view could be done by using XSLT in previous version of SharePoint. In SharePoint 2013 this van be done by using the “JS Link” functionality. The “JS Link” functionality is included in list view web parts of SharePoint.  You can use this property for adding specific JavaScript files. You can add 1 or multiple when you want to add multiple you will have to separate them using the “|”  symbol. On the internet you can find a lot of articles on how to use the “JS Link” functionality. Normally the “JS Links”  functionality is used to adjust the rendering of a specific field type in my scenario I had to change the complete list view.  In this article I will show you how to define a custom list view using the “JS Link” functionality according to a specific scenario: On project sites we want to follow the status of site migrations (for example SharePoint sites). The status of these migrations can be saved within SharePoint lists:    In this list you will need to have two fields:   Title: The title of the site that needs to be migrated. Status: The status of the migration. In this example we will have three statuses: “Not started”, “In progress” en “Done”.  Using this list we can manage the status of the migrations. At this moment we only have to add functionality to see the migration status of all sites in one view without the use of a farm solution.    In the above image you can see the same list but a altered view with the use of “JS LInk”. This view shows a doughnut view with three different colours. So how can we achieve this without creating a full trust solution.  First off all we searched for a JavaScript library to be able to create a doughnut chart. We choose the following library:   Chart.js – http://www.chartjs.org  Now that we have a library we can start writing a JavaScript file that we need to load within the “JS Link” property. Within the JavaScript file we first off all register a namespace to not get in the way of other JavaScript code besides that we also define a view variables that we can  use within the namespace.  var migrationStatus = migrationStatus || {};  migrationStatus.InProgressString = 'In progress'; migrationStatus.DoneString = 'Done'; migrationStatus.NotStartedString = 'Not started';  migrationStatus.ListName = 'Migration'; migrationStatus.ItemsDone = 0; migrationStatus.ItemsInProgress = 0; migrationStatus.ItemsNotStarted = 0;  To be able to define template you will need to define specific overrides. These overrides can be defined by using the “RegisterTemplateOverrides” method on the “SPClientTemplates.TemplateManager” object. This method needs one object that defines what needs to be overridden. We will define this within the method: “CustomizeViewRendering”.  // Create a function for customizing the view rendering of the list migrationStatus.CustomizeViewRendering = function () {     var migrationStatusContext = {};     migrationStatusContext.Templates = {};     migrationStatusContext.Templates.View = migrationStatus.RenderMigrationViewBodyTemplate;     migrationStatusContext.OnPostRender = migrationStatus.OnMigrationViewPostRender;     SPClientTemplates.TemplateManager.RegisterTemplateOverrides(migrationStatusContext); };   In the “migrationStatusContext” object needs to be defined which rendering methods needs to be overridden. As you can see in the above snippet the “OnPostRender” and the “Templates.View” will be overridden. These methods we will discuss in the upcoming paragraphs.  Note:When you override the rendering methods, this will be for every list view on a page. If you want to override the view rendering of one specific list you will have to build in a check if you are performing the action on the correct list.   migrationStatus.RenderMigrationViewBodyTemplate = function (ctx) {     if (ctx.Templates.Body == '') {         return RenderViewTemplate(ctx);     }      if (ctx.ListTitle == migrationStatus.ListName) {         var listData = ctx.ListData;          for (var idx in listData.Row) {             var listItem = listData.Row[idx];              if (listItem.Status == migrationStatus.InProgressString) {                 migrationStatus.ItemsInProgress++;             } else if (listItem.Status == migrationStatus.NotStartedString) {                 migrationStatus.ItemsNotStarted++;             } else if (listItem.Status == migrationStatus.DoneString) {                 migrationStatus.ItemsDone++;             }         }          var htmlOutput = [];         htmlOutput.push('&lt;div id=\"listData\"&gt;&lt;a href=\"' + ctx.listUrlDir + '\"&gt; View this list&lt;/a&gt;&lt;/div&gt;');         htmlOutput.push('&lt;div id=\"migrationstatusView\" style=\"height:400px;width:400px;margin-top:10px;float:left;margin-bottom:10px;display:block;\"&gt;');         htmlOutput.push('&lt;canvas id=\"myChart\" width=\"250\" height=\"250\"&gt;&lt;/canvas&gt;');         htmlOutput.push('&lt;div id=\"chartLegend\" style=\"width:150px;float:right;\" /&gt;');         htmlOutput.push('&lt;/div&gt;');                   var retVal = htmlOutput.join('');         return retVal;     }     else {         return RenderViewTemplate(ctx);     } }  The “migrationStatus.RenderMigrationViewBodyTemplate” function needs to be used to adjust the view. This method also receives the context (ctx) object in which all information regarding the context (in this example list) is saved. From this context we can retrieve all of the list data.  But first we will have to check if the body of the template isn’t empty. When it is empty we will have to call the default renderer. After that we can check if the list we are performing the actions for is our migration list: “ctx.ListTitle == migrationStatus.ListName” if this isn’t the case we also call the default rendering function:  return RenderViewTemplate(ctx);  When this list is our list we count all of the statuses and place them in the specific variables. When all this is done we write out the HTML we need for the rendering of the graph and legend and we return this.  De “PostRender” function will be called as last. In this function the graph will be loaded with the correct data and besides that we will also load the legend of the graph.  migrationStatus.OnMigrationViewPostRender = function (ctx) {     if (ctx.ListTitle == migrationStatus.ListName) {          var pieData = [         {             value: migrationStatus.ItemsNotStarted,             color: \"#F7464A\",             highlight: \"#FF5A5E\",             label: migrationStatus.NotStartedString         },         {             value: migrationStatus.ItemsDone,             color: \"#46BFBD\",             highlight: \"#5AD3D1\",             label: migrationStatus.DoneString         },         {             value: migrationStatus.ItemsInProgress,             color: \"#FDB45C\",             highlight: \"#FFC870\",             label: migrationStatus.InProgressString         }];                  var options = {             //Boolean - Whether we animate the rotation of the Doughnut             animateRotate: true,             //String - A legend template             legendTemplate: ''         };          //get the chart element and setup the doughnut chart         var ctxChart = document.getElementById(\"myChart\").getContext(\"2d\");         var myPie = new Chart(ctxChart).Doughnut(pieData, options);          //generate the legend html         var legend = document.getElementById(\"chartLegend\");         legend.innerHTML = myPie.generateLegend();     } };  In the “PostRender” function we also need to check if we are performing the action for the correct list.  The final thing we need to adjust to the JavaScript file is adding the following line:  migrationStatus.CustomizeViewRendering();  This line of code will be called on the moment the file is loaded and by calling this function is will make sure that the overrides for the list views will be registered.  Know that the files are ready we only need to specify the files in the “JS Link” property of the list view.    In the “JS Link” we have specified the following value:  /siteassets/Migration/Chart.js|/siteassets/Migration/MigrationStatus.js   ","categories": ["Javascript"],
        "tags": ["Javascript","List View","SharePoint 2013"],
        "url": "/2014/08/custom-list-view-by-using-the-js-link-property/",
        "teaser": null
      },{
        "title": "Configuring Web Site Binding with Release Management",
        "excerpt":"Release Management provides a continuous deployment solution that makes release cycles repeatable, visible, and more efficient by automating deployments through every environment from test to production.  Within Release Management you have al large set of default Actions and Components to configure a release cycle. On MSDN you can find a list with some common actions that are available within Release Management:   Release actions to deploy an app for Release Management  In this list you will find a action for creating a web site “Create Web Site”  and configure a web site “Configure Web Site”. By using the create web site action you will create a web site with default bindings and you would expect to be able to change the bindings by using the “Configure Web Site” action but this is not the case. If you would like to configure the binding on the web site you will have to create a custom action.  You can create a custom action by navigating to the “Inventory” tab and in the action window selecting new.    Selecting new will open up a new window in which you are able to configure a new action. In this new action we will start PowerShell and make use of the “WebAdministration” module to add a new binding.    As you can see in the image above everything in the “General Information” section speaks for it self. In the example we choose to add the action to the already existing category “IIS” because it is a “IIS” action.  In the “Execution” section specify what the action should execute. In the “Command” field specify the command for execution. For PowerShell specify “powershell” and in the “Arguments” field specify the argument for the command:  -Command \"&amp; { Import-Module WebAdministration; New-WebBinding -Name '__SiteName__' -IPAddress '__IPAddress__' -Port '__Port__' -HostHeader '__HostHeader__' -Protocol '__Protocol__'}\"   As you can see in the code we Import the “WebAdministration” module and use the “New-WebBinding” function for adding a new binding with the following parameters:     Parameter Value   Name __SiteName__   Protocol __Protocol__   IPAdress __IPAdress__   Port __Port__   Hostheader __HostHeader__    By using the format __[Name]__ (before and after 2 underscores) Release Management knows that these values can be used as parameters in order to make the action reusable.  When you save this action it will become available to add in your release template.    You can drag and drop the action within your release flow and configure it by double clicking the action.    When you add a new Binding you will probably also want to be able to remove a binding. You can do this by creating a new action and adding the following argument.  -Command \"&amp; { Import-Module WebAdministration; Remove-WebBinding -Name '__SiteName__' -IPAddress '__IPAddress__' -Port '__Port__' -HostHeader '__HostHeader__'}\"  Note:When using the create web site action it will create a web site with the default binding. If you add a new binding to the web site you will also have to remove the default binding or else it could be in conflict with other web sites.  ","categories": ["DevOps"],
        "tags": ["ALM","Binding","Powershell","Release Management"],
        "url": "/2014/09/configuring-web-site-binding-with-release-management/",
        "teaser": null
      },{
        "title": "Offline Installation SharePoint 2013",
        "excerpt":"In many situation you would like to do a offline installation of SharePoint 2013 or you are required to do a offline installation because you do not have a network connection on the server.  The prerequisites installer of SharePoint 2013 downloads the requirements directly of the internet. So we will have to make sure this will not be done.  But first off all we will have to install several Windows Server features. You can install these features by using the following PowerShell command:  Import-Module ServerManager Add-WindowsFeature Net-Framework-Features,Web-Server,Web-WebServer,Web-Common-Http,Web-Static-Content,Web-Default-Doc,Web-Dir-Browsing,Web-Http-Errors,Web-App-Dev,Web-Asp-Net,Web-Net-Ext,Web-ISAPI-Ext,Web-ISAPI-Filter,Web-Health,Web-Http-Logging,Web-Log-Libraries,Web-Request-Monitor,Web-Http-Tracing,Web-Security,Web-Basic-Auth,Web-Windows-Auth,Web-Filtering,Web-Digest-Auth,Web-Performance,Web-Stat-Compression,Web-Dyn-Compression,Web-Mgmt-Tools,Web-Mgmt-Console,Web-Mgmt-Compat,Web-Metabase,Application-Server,AS-Web-Support,AS-TCP-Port-Sharing,AS-WAS-Support, AS-HTTP-Activation,AS-TCP-Activation,AS-Named-Pipes,AS-Net-Framework,WAS,WAS-Process-Model,WAS-NET-Environment,WAS-Config-APIs,Web-Lgcy-Scripting,Web-AppInit,Windows-Identity-Foundation,Server-Media-Foundation,Xps-Viewer   Note:In this PowerShell command I also included: “Web-AppInit” . This is the Application Initialization feature (This is not a prerequisites) but can come very handy for warming up your web applications (IIS 8.0 Application Initialization reduces response time for SharePoint 2013).  When this is done we can make use of the prerequisites installer that is included on the SharePoint 2013 installation drive. By supplying the executable with certain arguments it will look for the installation files on the locale drive.     Argument Installation   SQLNCli Microsoft SQL Server 2008 R2 SP1 Native Client   IDFX11 Microsoft Identity Extensions   IDFX Windows Identity Foundation (KB974405)   Sync Microsoft Sync Framework Runtime v1.0 SP1 (x64)   AppFabric Windows Server AppFabric   KB2671763 Cumulative Update Package 1 for Microsoft AppFabric 1.1 for Windows Server (KB2671763)   MSIPCClient Microsoft Information Protection and Control Client   WCFDataServices Microsoft WCF Data Services 5.0   WCFDataServices56 (Added in the SP1 installation) Microsoft WCF Data Services 5.6    When you have downloaded the prerequisites by using the below links:   Microsoft SQL Server 2008 R2 SP1 Native Client Microsoft Sync Framework Runtime v1.0 SP1 (x64) Windows Server AppFabric Cumulative Update Package 1 for Microsoft AppFabric 1.1 for Windows Server (KB2671763) Windows Identity Foundation (KB974405) Microsoft Identity Extensions Microsoft Information Protection and Control Client Microsoft WCF Data Services 5.0 Microsoft WCF Data Services 5.6 (Needed when doing a SP1 installation)After downloading the file we can make use of the following command to start the offline installation of the prerequisites:  Start-Process \"&lt;path&gt;\\PrerequisiteInstaller.exe\" -ArgumentList \"`/SQLNCli:`\"&lt;path&gt;\\sqlncli.msi`\" `                                                                   /IDFX:`\"&lt;path&gt;\\Windows6.1-KB974405-x64.msu`\" `                                                                   /IDFX11:`\"&lt;path&gt;\\MicrosoftIdentityExtensions-64.msi`\" `                                                                   /Sync:`\"&lt;path&gt;\\Synchronization.msi`\" `                                                                   /AppFabric:`\"&lt;path&gt;\\WindowsServerAppFabricSetup_x64.exe`\" `                                                                   /KB2671763:`\"&lt;path&gt;\\AppFabric1.1-RTM-KB2671763-x64-ENU.exe`\" `                                                                                                                                                                /MSIPCClient:`\"&lt;path&gt;\\setup_msipc_x64.msi`\" `                                                                   /WCFDataServices:`\"&lt;path&gt;\\WcfDataServices.exe`\" `                                                                   /WCFDataServices56:`\"&lt;path&gt;\\WcfDataServices56.exe`\"\"  On the TechNet Download site you can also find the scripts created by “Craig Lussier” that you can use to start a offline installation. Keep in mind that these script are not for SharePoint 2013 SP1 (If you want them compatible for SP1 you will have the additional arguments). Download and Install SharePoint 2013 Prerequisites on Windows Server 2012  ","categories": ["Administration"],
        "tags": ["Offline","Powershell","Prerequisites","SharePoint 2013","SP1"],
        "url": "/2014/09/offline-installation-sharepoint-2013/",
        "teaser": null
      },{
        "title": "Deploy SharePoint Solutions with Release Management",
        "excerpt":"In one of my previous post (Configuring Web Site Binding with Release Management) I showed how you can make a custom action for Release Management to configure web bindings.  As described in that post Release Management is a tool that provides a continuous deployment solution that makes release cycles repeatable, visible, and more efficient by automating deployments through every environment from test to production.  When you want to make use of Release Management in a scenario were you will deploy your SharePoint solutions within your DTAP environment you have to make some custom actions / components in order to facilitate this functionality.  In a upcoming series of posts I will work you trough the process of making and adjusting the TFS Build template in order to build your SharePoint solutions and how to make a custom PowerShell component for Release Management to deploy your SharePoint solutions.   Part 1 – Customize the Release Management Build template. Part 2 – Create a custom component to deploy the SharePoint Solutions. Part 3 – Creating the Release Template that runs after the Build.  In this series of posts I assume that you have setup your build server in order to build SharePoint solutions. If not read this article on MSDN:   How to Build Office Developer Tools Projects with TFS Team Build 2012  The individual parts will be written down in the upcoming weeks. I will try to write a Part a week but I also have to attend to the TechEd in Barcelona so I don’t know If I will be able to write down a post in that week.  ","categories": ["DevOps"],
        "tags": ["ALM","Powershell","Release Management","TFS"],
        "url": "/2014/10/deploy-sharepoint-solutions-with-release-management/",
        "teaser": null
      },{
        "title": "Customize the Release Management Build template &ndash; Part 1",
        "excerpt":"To setup a build in combination with Release Management you need the Release Management build template. This template looks like the default build template but has an additional sequence action to start the release after the build is succeeded. The Release Management Build template can be found within the install location of release management.  C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\Release Management\\Client\\bin\\ReleaseTfvcTemplate.12.xaml  We will use this template and rename it to “BuildTemplate-Release-General.xaml”. As location for the build template we will use TFS project called: “Build Templates”. By using a central location we make sure we will not have duplicate build templates within our Team Foundation Server environment.  For different types of builds you would like to add files to your drop location that you save within a solution folder for this we will have to make a small adjustment to our build template. To make this adjustment open the Build template and navigate to the sequence activity that contains the “msbuild” action.  You can find the “msbuild” activity by following the following path:  “Overall Build Process” – “Run on Agent” – “Try” – “Compile, Test and Publish”.  In the sequence activity that contains the “msbuild”  action we will add two argument by selecting “Compile, Test and Publish” and then Arguments in to lower left corner. If you scroll down the list click on “Create Argument” to add a new argument.    Add the following arguments:   CopySolutionFolder: Will be used to save the location of the Solution folder relative to the Team Foundation server. DropLocation: Will be used to save the drop location of the build. CustomBuildDirectory: Will be used to save the location of the Solution folder on the build server.    Know that we have created the argument we can add actions to set the values of the arguments. Drag a “GetEnvironmentVariable&lt;T&gt;” action to the top of the “Compile, Test and Publish” sequence and call it “Get Drop Location Folder” and set the properties as you can see in the image.    The “WellKnownEnvironmentVariables.DropLocation” is the environment variable we need in order to get the drop location of the build. In the result property define the argument (“DropLocation”) in which you want to save the value.  Place an “If” control flow underneath the “GetEnvironmentVariable” action to check whether the value of the argument “CopySolutionFolder” is not null or empty. This because we will define this argument within the build definition interface. You can add an argument to the build definition interface by updating the Metadata Argument. Select this argument and edit it by selecting the three dots within the default value column.    Scroll down the list of parameters and check whether the argument you created is in the list. If this argument is not already in the list add it.  &nbsp;    In the If control flow we check if the value of the “CopySolutionFolder” argument is not null or empty by defining the following condition:  String.IsNullOrEmpty(CopySolutionFolder)  If the argument is null or empty we will not take any action and if it contains a value we will try to get the local path of the solution folder in order to copy the contents to the drop location. To achieve this we need to add a sequence action to the else branch.    To retrieve the local path of the solution folder we place the “GetLocalPath” activity in the sequence. This action will allow us to add a TFS project relative path in the build definition. For example the value could be: “$/SharePoint Portal/MSFTPlayground.SharePoint.Farm/Deployment Configuration”. The result of the “GetLocalPath” action will be saved in the CustomBuildDirectory argument.    To finalize the build definition we add the “CopyDirectory” activity to the sequence. The source of the activity will be the solution folder (“CustomBuildDirectory”) and the destination will be the drop location (“DropLocation”).    Save the build template and check the file in into TFS. With the file in TFS you are able to use this template within a build definition. This template can be used by selecting show details in the process tab of a build definition and then selecting “New” to browse to the location were you have saved the build template within TFS.    Underneath the template selection you can see the parameters of the build template.    The categories 1,2,3,4,5 are the default categories. The Release category is the category that can be used to define the parameters regarding “Release Management”. The “Extra Files” category this is the category for our added argument.  In the next couple of post we will see how this build template will be used to start off a “Release” within “Release Management” order to start a SharePoint deployment.  ","categories": ["DevOps"],
        "tags": ["ALM","Build Template","SharePoint"],
        "url": "/2014/10/customize-the-release-management-build-template-part-1/",
        "teaser": null
      },{
        "title": "Adjust the ReleaseTfvcTemplate.12.xaml build template to work with the BizTalk Deployment Framework",
        "excerpt":"If you want to use the BizTalk Deployment framework in combination with Release Management you need to adjust the default build template that comes with Release Management.  If you did not find it already you can find the default build template in the installation folder of release management.  C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\Release Management\\Client\\bin\\ReleaseTfvcTemplate.12.xaml  Copy this template to TFS and open it to make a small adjustment. In the workflow explorer Navigate to the following sequence component:  “Overall Build Process” – “Run on Agent” – “Try” – “Compile, Test and Publish”.  Whit this component selected, select “Variable” in the lower left corner.    In the popup window create a new “Variable” by clicking “Create Variable” and call it “BinariesDirectory” and make it of the type “String”.    This variable will be used to save the directory in which the Deployment Framework components can be found.  Next step is to add a “GetEnviromentVariable&lt;T&gt;” action just above the “Run MSBuild” action. Select the type “String” and call the action “Get Binaries Directory”.  To get the Binaries Directory we want to make use of a WellKnownEnvironmentVariable (MSDN). To use such a variable you need to import a assembly reference. You can add a assembly reference by clicking on “Import” next to “Arguments” in the footer of the workflow screen.    Import the “Microsoft.TeamFoundation.Build.Activities.Extensions” namespace to make use of the WellKnownEnvironmentVariables. Know you can set the properties of the “Get Binaries Directory” action to get the “WellKnownEnvironmentVariables.BinariesDirectory” environment variable.    With this value we can change the MSBuild argument to build the MSI from the BizTalk Deployment Framework. Select the “Run MSBuild” action and adjust the “CommandLineArguments” property to the following value.  String.Format(\"/p:SkipInvalidConfigurations=true;TeamBuildOutDir=\"\"{0}\"\" {1}\", BinariesDirectory, AdvancedBuildSettings.GetValue(Of String)(\"MSBuildArguments\", String.Empty))  Save and Check In the build template. If you use this build template to define a build definition you are able to use a Release within Release Management that uses the output of this build.  One thing to keep in mind that you will have to set the following properties on your build definition:   MSBuild platform: X86 Output location: AsConfigured    ","categories": ["DevOps"],
        "tags": ["BizTalk Deployment Framework","Build Template","Release Management","TFS"],
        "url": "/2014/10/adjust-the-releasetfvctemplate-12-xaml-build-template-to-work-with-the-biztalk-deployment-framework/",
        "teaser": null
      },{
        "title": "Article Published DIWUG Magazine 14 &ndash; Management of Host Named Site Collections",
        "excerpt":"A while ago I wrote an article about the management of host named site collections in SharePoint 2013. The magazine it was written for was released today.  If you want to read the article you can download it here, you can find my article on page 35:  Management of Host Named Site Collections  The PDF is the complete DIWUG magazine that you can also download from the DIWUG website. The complete magazine contains the following articles:    Exploring the changes to how Office 365 uses SharePoint to manage documents in Dynamics CRM - Peter Baddeley SharePoint 2013 Search Find Content - Ronald Laan In a nutshell: Identity delegation leveraging the Claims to Windows Token Service - Spencer Harbar 10 tips to drive user adoption - Jasper Oosterveld  Management of host-named site collections - Maik van der Gaag Are “Out of the Box” SharePoint solutions right for you? - Neil Richards SharePoint Lists in Access Apps - Christiaan Blaauw   Let me know what you think about the article!!    ","categories": ["Article"],
        "tags": ["Article","Diwug","Magazine"],
        "url": "/2014/11/article-published-diwug-magazine-14-management-of-host-named-site-collections/",
        "teaser": null
      },{
        "title": "Create a custom component to deploy SharePoint Solutions &ndash; Part 2",
        "excerpt":"In the first part of the series we have adjusted a build template in order to copy content from source control to the drop location after the build. This content can then be used to supply release management with information about the deployment. This functionality will be used to copy a configuration file to the drop location. With PowerShell were our custom component will exist off we can read out that configuration file.  As you can read we will create our custom component in specific steps and we will start with the configuration file.   Configuration File  The configuration file will contain all of the information that is needed to deploy a SharePoint solution package. As example you would like to know to which web application the solution needs to be deployed to.  As example take a look at the following sample configuration file:  &lt;?xml version=\"1.0\" encoding=\"utf-8\" ?&gt; &lt;!--    Section to specify the configuration for the deployment --&gt; &lt;Configuration&gt;   &lt;!--     Section to specify environment information   --&gt;   &lt;Environment&gt;     &lt;DEV&gt;       &lt;WebApplications&gt;         &lt;WebApplication ID=\"1\" Url=\"http://sample.msftplayground.local\"&gt;&lt;/WebApplication&gt;       &lt;/WebApplications&gt;     &lt;/DEV&gt;     &lt;TST&gt;       &lt;WebApplications&gt;         &lt;WebApplication ID=\"1\" Url=\"http://sample-tst.msftplayground.local\"&gt;&lt;/WebApplication&gt;       &lt;/WebApplications&gt;     &lt;/TST&gt;     &lt;ACC&gt;       &lt;WebApplications&gt;         &lt;WebApplication ID=\"1\" Url=\"http://sample-acc.msftplayground.local\"&gt;&lt;/WebApplication&gt;       &lt;/WebApplications&gt;     &lt;/ACC&gt;     &lt;PROD&gt;       &lt;WebApplications&gt;         &lt;WebApplication ID=\"1\" Url=\"http://sample.msftplayground.local\"&gt;&lt;/WebApplication&gt;       &lt;/WebApplications&gt;     &lt;/PROD&gt;   &lt;/Environment&gt;   &lt;!--     Section to specify information on solutions were to deploy to or retract.     --&gt;   &lt;Solutions&gt;     &lt;!--     Section to specify the solutions that need to deployed.     --&gt;     &lt;Deploy&gt;       &lt;!--       Section to specify specific solution information              Name = Name of the solution        Upgrade = Upgrade the solution if it exists       DeployToAll = Deploy to all content applications       DeployNotExits = Deploy the solution if it does not exist when upgrade is set to true       WebApplication = The web application to deploy to specified with the ID when deploy to all is false       --&gt;       &lt;Solution Name=\"MSFTPlayground.SharePoint.wsp\" Upgrade=\"true\" DeployToAll=\"false\" DeployNotExists=\"true\"&gt;         &lt;WebApplication ID=\"1\" /&gt;         &lt;WebApplication ID=\"2\" /&gt;         &lt;WebApplication ID=\"3\" /&gt;       &lt;/Solution&gt;     &lt;/Deploy&gt;     &lt;Retract&gt;       &lt;!--       Section to specify specific solution information              Name = Name of the solution        Remove = Remove the solution after retraction       RetractAll = Retract from all content applications       --&gt;       &lt;Solution Name=\"MSFTPlayground.SharePoint.wsp\" Remove=\"true\" RetractAll=\"true\"&gt;         &lt;WebApplication ID=\"1\" /&gt;         &lt;WebApplication ID=\"2\" /&gt;         &lt;WebApplication ID=\"3\" /&gt;       &lt;/Solution&gt;     &lt;/Retract&gt;   &lt;/Solutions&gt; &lt;/Configuration&gt;  In the configuration file you can specify which solution package you would like to deploy and to what web application. The web applications are specified in the “Environment” section. The configuration file contains a “Environment” section to be able to use the same mechanism for multiple environments that use different URLs. Within the solution section we then refer to a specific web application by its ID.  To make it more complex we also added some other properties (Upgrade, DeployToAll) to make the components more advanced.  PowerShell  With the configuration file we can start writing the PowerShell that will read out the configuration file and deploys the solution.  param(     [string]$filename = $(throw \"Specify the Config filename that is located in the drop folder\"),     [string]$environment = $(throw \"Specify the Environment you would like to deploy to.\") )  if ((Get-PSSnapin \"Microsoft.SharePoint.PowerShell\" -ErrorAction SilentlyContinue) -eq $null) {     Write-Host \"Loading Microsoft SharePoint PowerShell\" -ForegroundColor Green     Add-PSSnapin \"Microsoft.SharePoint.PowerShell\" } Write-Host \"#### Start deploying solutions ####\"  if ((Test-Path $fileName) -eq $true) {     . \".\\functions.ps1\"     Write-Host \"Loading configuration file $filename\" -ForegroundColor Green      try{         $config = [xml] (Get-Content $fileName)         Deploy-Solutions     }catch{         Write-Host $_.Exception.Message -ForegroundColor Red         throw \"Exception occured while deploying solutions\"     } }else {     throw \"Error loading configuration file, exiting the deployment\" }  Write-Host \"#### End deploying solutions ####\"  This PowerShell file needs to be run with two parameters:   Filename: The path to the configuration file in the drop location. Environment: The environment you would like to deploy to. This should be the same as the environment section you have specified in the configuration file.  Within the PowerShell file the SharePoint PowerShell module is loaded and besides that I load a custom functions file. In the functions file I have a couple of PowerShell methods that I use to do some advanced stuff on the configuration file, and also some more SharePoint functions. When everything is loaded the configuration file is loaded in the  config variable and we call the function “Deploy-Solutions” that is within the same file.  function Deploy-Solutions(){     $path = \".\\\";     $type = \"Deploy\";      foreach($solution in $config.Configuration.Solutions.Deploy.Solution){                  $solutionName =  $solution.Name;         $currentDirectory = Get-Location         $solutionPath = [IO.Path]::GetFullPath([String]::Concat($currentDirectory, \"\\\" ,$path, \"\\\", $solutionName))             Write-Host \" &gt; Solution path is '$solutionPath'\" -ForegroundColor Yellow          $webApplicationScoped = Get-SolutionProperty $solutionName \"ContainsWebApplicationResource\"         $solutionDeployed = Get-SolutionProperty $solutionName \"Deployed\"                                         [bool]$deployToAll = Check-SolutionConfigProperty $solutionName \"DeployToAll\" $type         [bool]$upgrade = Check-SolutionConfigProperty $solutionName \"Upgrade\" $type         [bool]$deployNotExists = Check-SolutionConfigProperty $solutionName \"DeployNotExists\" $type         $webApps = Get-WebAppsForSolution $solutionName $environment $type          # Check if file exists         if ((Test-Path $solutionPath) -eq $false) {             Write-Host \" &gt; Solution file doesn't exist!\" -ForeGroundColor Red             throw \"Solution file doesn't exist!\";         }         $deploySolution = $false;          if (Check-SolutionExists $solutionName) {             Write-Host \" &gt; Solution exists in solution store\" -ForegroundColor Yellow             if(!$upgrade){                 Write-Host \" &gt; Solution exists in solution store\" -ForegroundColor Yellow                                                                 if($solutionDeployed) {                     Write-Host \" &gt; Solution is deployed, uninstall solution $solutionName\" -ForegroundColor Yellow                     if ($webApplicationScoped) {                         $solution = Get-SPSolution $solutionName                         $deployedTo = $solution.DeployedWebApplications                          foreach ($webApp in $deployedTo) {                             Write-Host \" &gt; Uninstall from web application '$webApp'\" -ForeGroundColor Yellow                             Uninstall-SPSolution -Identity $solutionName -WebApplication $webApp -Confirm:$false  -ErrorAction Stop                             Wait-ForJobToFinish($solutionName)                         }                                                                         } else {                         Write-Host \" &gt; Solution doesn't contain web application resource, uninstall globally.\" -ForeGroundColor Yellow                         Uninstall-SPSolution -Identity $solutionName -Confirm:$false -ErrorAction Stop                         Wait-ForJobToFinish($solutionName)                     }                     Write-Host \" &gt; Solution uninstalled\" -ForeGroundColor Green                 }                 Write-Host \" &gt; Remove solution $solutionName from solution store\" -ForeGroundColor Yellow                 Remove-SPSolution -Identity $solutionName -Confirm:$false -ErrorAction Stop                 Write-Host \" &gt; Solution removed\" -ForeGroundColor Green                         $deploySolution = $true                                       }else{                 Write-Host \" &gt; Upgrade SPSolution $solutionName\" -ForegroundColor Yellow                 Update-SPSolution -LiteralPath $solutionPath -Identity $solutionName -Force -GACDeployment -ErrorAction Stop                        Wait-ForJobToFinish($solutionName)                                 $deploySolution = $false             }                       }else{             Write-Host \" &gt; Solution does not exists in solution store\" -ForegroundColor Yellow              if(($upgrade -And $deployNotExists)){                 Write-Host \" &gt; Solution does not exists and will be added because it needed to be deployed when it did not exists\"  -ForegroundColor Yellow                 $deploySolution = $true             }              if(!($upgrade)){                 Write-Host \" &gt; Solution does not exists and will be added in order to deploy\"  -ForegroundColor Yellow                 $deploySolution = $true             }         }                  if($deploySolution){             Write-Host \" &gt; Add solution $solutionName to solution store\" -ForeGroundColor Yellow             Add-SPSolution -LiteralPath $solutionPath -ErrorAction Stop             Write-Host \" &gt; Solution added\" -ForeGroundColor Green              Write-Host \" &gt; Deploy solution $solutionName\" -ForeGroundColor Yellow             $webApplicationScoped = Get-SolutionProperty $solutionName \"ContainsWebApplicationResource\"              if ($webApplicationScoped) {                 if ($deployToAll) {                     Write-Host \" &gt; Deploying to all Web Applications\" -ForeGroundColor Yellow                     Install-SPSolution –Identity $solutionName -GACDeployment -AllWebApplications -Force -ErrorAction Stop                     Wait-ForJobToFinish($solutionName)                 } else {                     foreach ($webApp in $webApps) {                         Write-Host \" &gt; Deploying to $webApp\" -ForeGroundColor Yellow                                                                                   Install-SPSolution –Identity $solutionName -WebApplication $webApp -Force -GACDeployment -ErrorAction Stop                         Wait-ForJobToFinish($solutionName)                     }                 }             } else {                 Install-SPSolution –Identity $solutionName -GACDeployment -Force -ErrorAction Stop                 Wait-ForJobToFinish($solutionName)             }         }          $success = Get-SPSolutionSuccessState $solutionName              if($success){                 Write-Host \" &gt; Solution $solutionName installed\" -ForeGroundColor Green             }             else{                Write-Host \" &gt; Solution $solutionName could not be installed correctly.\" -ForeGroundColor Green                throw \"Error occured while installing solution $solutionName\"             }     }                           }  In the Deploy-Solution function we navigate trough the xml and deploy the solution packages specified in the configuration file. Within the Deploy-Solution function we also use some functions that are specified in the functions.ps1 file.  With the PowerShell files in place we can start adding things to Release Management. The first action in Release Management is creating a new Tool. By going to “Inventory” – “Tools”  and click “New” to create a new tool.    ","categories": ["Development"],
        "tags": ["Component","Deployment","Powershell","Release Management"],
        "url": "/2015/01/create-custom-component-deploy-sharepoint-solutions-part-2/",
        "teaser": null
      },{
        "title": "Creating the Release Template that runs after the Build &ndash; Part 3",
        "excerpt":"With the component created in part 2 we can start using it to deploy applications to our SharePoint environment. We start by creating a new template.  For this post I assume you already created the release paths and have added the SharePoint servers to Release Management. To create a new template open Release Management and click on “Configure Apps” if you have the sub section “Agent-Based Release Templates” open you will see all Release Templates that are used Agent Based. In my demo environment I have three templates for example.    In image you can see the agent based release templates available in my environment:   SharePoint Release Template, release template for a complete SharePoint release path. BizTalk Release Template, release template for a BizTalk release path. Web Portal Release, release template for a web portal.  For this section we will start off with a new one by clicking on “New” .    In the window that appears you will have to specify the information.   Name: The name of the release template. Description: The description of the release template. Release Path: The release path that is already defined. Build Definition: The build that you would like to use. You can specify if the build is able to kick off a release. Security: In this section you can specify who or which group is able to do what within this release template. Be careful, by default everyone that has access to release management has edit rights on the release template.  With the properties filled in you will see a kind off workflow interface in which we will be able to specify which steps we need to take during a release in a specific stage. First off all we will drag the right SharePoint Server from the left side within the deployment sequence.    With the correct server in the release template we can specify what needs to be done on that server. We could add our own components but you are also able to use some predefined actions.  For know we will add our own component. But the component is not available by default. To make it available right click on components and selecting “Add” this will show a list off all available components.    Select the component that we created and click link on the top of the screen. With the component available in the Release Template drag and drop it within the server that we want to perform that action. Next you will need to expand the component in order to fill in the parameters.    With this done you can save the template and kick off a release by using TFS Build. If you change the build definition you can kick off a release by changing the properties in the section called “Release”, by setting “Release Build” to true TFS Build will try to start a release that is attached to that build definition after it successfully build the application.    Using this mechanism explained in the series of posts you are able to structure your SharePoint releases or any other type of release like BizTalk of HTML.By scripting new components (because a SharePoint release does not only exists of deploying solutions) you could also have the complete process of SharePoint deployment.    In this deployment sequence you have several components that all take different actions, some examples are:   Deploy SharePoint Solutions, will deploy solutions based on the drop folder. Retract SharePoint Solutions, will retract solutions based on the drop folder. Activate Site Features, will activate SharePoint site features. WarmUp SharePoint, will warmup SharePoint after a deployment.  If you take it some steps further you could build in a specific role back mechanism that restores your SharePoint environment or deploy to all your development environments in parallel.    &nbsp;  I hope you have a good impression of what can be done by using Release Management. If you have any further questions please let me know.  ","categories": ["DevOps"],
        "tags": ["Deployment","Powershell","Release Management","SharePoint"],
        "url": "/2015/01/creating-the-release-template-that-runs-after-the-build-part-3/",
        "teaser": null
      },{
        "title": "Starting a Workflow with CSOM",
        "excerpt":"There are situations were you would like to start a workflow by using  code. In one of my last projects there was that kind of situation, in that project we needed to start a workflow on every item in the SharePoint list. In the past I had done this kind of work many times with the server object model but since this was a SharePoint Online instance I was not able to use the server object model.  As you are unable to use the server object model you need to use the Client Side Object Model (CSOM).  In the upcoming article I will walk you trough the code step by step. Make sure that before you start that you add the OfficeDevPpPCore16 nuget package to your application.  &nbsp;  First thing you need to do when using the client side object model is getting the client context for the site. The URL that you use for the context will need to be the absolute URL of the site on which you want to start the Workflow.  In order to build the client context you need the login name of the user, his password and the site URL.  &nbsp;  Console.WriteLine(\"Enter the Office 365 Login Name\"); string loginId = Console.ReadLine(); string pwd = GetInput(\"Password\", true);  Console.WriteLine(\"Web Url:\"); string webUrl = Console.ReadLine();  Console.WriteLine(\"List Name:\"); string listName = Console.ReadLine();  Console.WriteLine(\"Workflow Name\"); string workflowName = Console.ReadLine();  var passWord = new SecureString(); foreach (char c in pwd.ToCharArray()) passWord.AppendChar(c);  using (var ctx = new ClientContext(webUrl)) {     ctx.Credentials = new SharePointOnlineCredentials(loginId, passWord); }    In the above code the is a reference to a method called “GetInput” We use this method for getting the password of the user without seeing what the user is typing in the console window.  &nbsp;  private static string GetInput(string label, bool isPassword) {     Console.ForegroundColor = ConsoleColor.White;     Console.Write(\"{0} : \", label);     Console.ForegroundColor = ConsoleColor.Gray;      string strPwd = \"\";      for (ConsoleKeyInfo keyInfo = Console.ReadKey(true); keyInfo.Key != ConsoleKey.Enter; keyInfo = Console.ReadKey(true)) {         if (keyInfo.Key == ConsoleKey.Backspace) {             if (strPwd.Length &gt; 0) {                 strPwd = strPwd.Remove(strPwd.Length - 1);                 Console.SetCursorPosition(Console.CursorLeft - 1, Console.CursorTop);                 Console.Write(\" \");                 Console.SetCursorPosition(Console.CursorLeft - 1, Console.CursorTop);             }         } else if (keyInfo.Key != ConsoleKey.Enter) {             if (isPassword) {                 Console.Write(\"*\");             } else {                 Console.Write(keyInfo.KeyChar);             }             strPwd += keyInfo.KeyChar;          }      }     Console.WriteLine(\"\");      return strPwd; }  When the client context is setup we can start with the actions we need to perform. The first action we will perform is getting a reference to all required workflow components.components  var workflowServicesManager = new WorkflowServicesManager(ctx, ctx.Web); var workflowInteropService = workflowServicesManager.GetWorkflowInteropService(); var workflowSubscriptionService = workflowServicesManager.GetWorkflowSubscriptionService(); var workflowDeploymentService = workflowServicesManager.GetWorkflowDeploymentService(); var workflowInstanceService = workflowServicesManager.GetWorkflowInstanceService();  &nbsp;  This above components are all needed in order to start a workflow. The component we will use first is the deployment service, this service will be used to get all published workflow definitions on the site.  var publishedWorkflowDefinitions = workflowDeploymentService.EnumerateDefinitions(true); ctx.Load(publishedWorkflowDefinitions); ctx.ExecuteQuery();  var def = from defs in publishedWorkflowDefinitions           where defs.DisplayName == workflowName           select defs;  WorkflowDefinition workflow = def.FirstOrDefault();  if(workflow != null) {  }  As you can see in the above code the workflow definitions are retrieved from the site. In this collection we try to find our workflow definition we need by using the workflow name.  &nbsp;  // get all workflow associations var workflowAssociations = workflowSubscriptionService.EnumerateSubscriptionsByDefinition(workflow.Id); ctx.Load(workflowAssociations); ctx.ExecuteQuery();  // find the first association var firstWorkflowAssociation = workflowAssociations.First();  // start the workflow var startParameters = new Dictionary&lt;string, object&gt;();  if (ctx.Web.ListExists(listName)) {     List list = ctx.Web.GetListByTitle(listName);      CamlQuery query = CamlQuery.CreateAllItemsQuery();     ListItemCollection items = list.GetItems(query);      // Retrieve all items in the ListItemCollection from List.GetItems(Query).     ctx.Load(items);     ctx.ExecuteQuery();     foreach (ListItem listItem in items) {         Console.WriteLine(\"Starting workflow for item: \" + listItem.Id);         workflowInstanceService.StartWorkflowOnListItem(firstWorkflowAssociation, listItem.Id, startParameters);         ctx.ExecuteQuery();     } }   When the correct definition is loaded we need to association  to be able to start the workflow. Together with the association you also need start parameters to start the workflow, because in our situation we do not have any start parameters we pass in a empty dictionary.  If the list exists we create a CAML query to retrieve all the items in the list.  As last step we can iterate trough the list items and start the workflow for each item by calling the “StartWorkflowOnListItem” method.  &nbsp;  Below you can find the complete reference for the source code.  &nbsp;  using Microsoft.SharePoint.Client; using Microsoft.SharePoint.Client.WorkflowServices; using System; using System.Collections.Generic; using System.Linq; using System.Security; using System.Text; using System.Threading.Tasks;  namespace CSOMStartWorkflow {     class Program {         static void Main(string[] args) {              Console.WriteLine(\"Enter the Office 365 Login Name\");             string loginId = Console.ReadLine();             string pwd = GetInput(\"Password\", true);              Console.WriteLine(\"Web Url:\");             string webUrl = Console.ReadLine();              Console.WriteLine(\"List Name:\");             string listName = Console.ReadLine();              Console.WriteLine(\"Workflow Name\");             string workflowName = Console.ReadLine();              var passWord = new SecureString();             foreach (char c in pwd.ToCharArray()) passWord.AppendChar(c);              using (var ctx = new ClientContext(webUrl)) {                 ctx.Credentials = new SharePointOnlineCredentials(loginId, passWord);                  var workflowServicesManager = new WorkflowServicesManager(ctx, ctx.Web);                 var workflowInteropService = workflowServicesManager.GetWorkflowInteropService();                 var workflowSubscriptionService = workflowServicesManager.GetWorkflowSubscriptionService();                 var workflowDeploymentService = workflowServicesManager.GetWorkflowDeploymentService();                 var workflowInstanceService = workflowServicesManager.GetWorkflowInstanceService();                  var publishedWorkflowDefinitions = workflowDeploymentService.EnumerateDefinitions(true);                 ctx.Load(publishedWorkflowDefinitions);                 ctx.ExecuteQuery();                  var def = from defs in publishedWorkflowDefinitions                           where defs.DisplayName == workflowName                           select defs;                  WorkflowDefinition workflow = def.FirstOrDefault();                  if(workflow != null) {                       // get all workflow associations                     var workflowAssociations = workflowSubscriptionService.EnumerateSubscriptionsByDefinition(workflow.Id);                     ctx.Load(workflowAssociations);                     ctx.ExecuteQuery();                      // find the first association                     var firstWorkflowAssociation = workflowAssociations.First();                                          // start the workflow                     var startParameters = new Dictionary&lt;string, object&gt;();                      if (ctx.Web.ListExists(listName)) {                         List list = ctx.Web.GetListByTitle(listName);                          CamlQuery query = CamlQuery.CreateAllItemsQuery();                         ListItemCollection items = list.GetItems(query);                          // Retrieve all items in the ListItemCollection from List.GetItems(Query).                         ctx.Load(items);                         ctx.ExecuteQuery();                         foreach (ListItem listItem in items) {                             Console.WriteLine(\"Starting workflow for item: \" + listItem.Id);                             workflowInstanceService.StartWorkflowOnListItem(firstWorkflowAssociation, listItem.Id, startParameters); ","categories": ["Office 365","SharePoint"],
        "tags": ["C#","CSOM","SharePoint Online"],
        "url": "/2015/08/starting-a-workflow-with-csom/",
        "teaser": null
      },{
        "title": "Video on &lsquo;Custom List View by Using the JS Link Property&rsquo;",
        "excerpt":"About a year ago I wrote a article on how to create a custom list view by using the JS link property. This turned out to be a great article and was very much appreciated by the readers of my blog.&nbsp; Webucator a company that gives instructor led training contacted me a month ago if they could make a small video on the subject.  &nbsp;  I was honored by the request and they made the video which is really awesome. Take a look at the video in this blog post and you could also take a look at the training services webucator has to offer.  &nbsp;   Webucator   Webucator SharePoint Training    ","categories": ["General"],
        "tags": ["Webucator"],
        "url": "/2015/10/video-on-custom-list-view-by-using-the-js-link-property/",
        "teaser": null
      },{
        "title": "Forcing a Device Channel",
        "excerpt":"Device channels are a way of using different master pages for different devices. MSDN describes the functionality as followed:  &nbsp;  “Browsing the web on a mobile device is now so common that a SharePoint site must be optimized for readability and ease of use on smartphones and other mobile devices such as tablets. With device channels in SharePoint 2013, you can render a single publishing site in multiple ways by using different designs that target different devices. This article can help you plan for using the device channels feature in SharePoint 2013. It provides a detailed overview of the feature itself, and provides the necessary information for creating a device channel. Also, after reading this article, you'll know what device channels you need to implement, and how to implement those channels.”  &nbsp;  Using the out of the box settings you can set the master page by information that is in the user agent string. The information in this string will give SharePoint the information to select the right master page. How the out of the box functionality works can be found here:  &nbsp;  https://msdn.microsoft.com/en-us/library/office/jj862343.aspx  &nbsp;  This functionality can also be used for other scenarios for example in a CMS setup. When you want to apply this to other scenarios you will have to force a device channel. Forcing device channels can be done using two options:  &nbsp;   Query String: Append the “DeviceChannel” query string to your URL and set the value to the “Alias” of your device channel. Cookie: You can set a cookie named “DeviceChannel”  and set the value of the cookie to the name of your “Alias”.  &nbsp;  You can set this cookie in code in order to persist the use of the master page.  &nbsp;  if (HttpContext.Current.Request.Cookies[\"DeviceChannel\"] == null) {      string url = HttpContext.Current.Request.Url.ToString();     using (SPSite site = new SPSite(url)) {         string authUrl = site.GetProperty(PropertyBagKeys.AuthoringSiteKey, string.Empty);         string devChannel = site.GetProperty(PropertyBagKeys.AuthDeviceChannel, string.Empty);          if (!string.IsNullOrEmpty(authUrl)) {             if (url.StartsWith(authUrl)) {                 HttpCookie cookie = new HttpCookie(\"DeviceChannel\");                 cookie.Value = string.IsNullOrEmpty(devChannel) ? \"Default\" : devChannel;                 HttpContext.Current.Request.Cookies.Add(cookie);             } else {                 HttpContext.Current.Request.Cookies.Remove(\"DeviceChannel\");             }         }     }                } ","categories": ["Development","SharePoint"],
        "tags": ["C#","Device Channel","SharePoint 2013"],
        "url": "/2015/11/forcing-a-device-channel/",
        "teaser": null
      },{
        "title": "Release Management in Visual Studio Team Services",
        "excerpt":"Visual Studio Team Services is the formally know Visual Studio Online. The old name brought a lot of confusion I think the new name will do a lot better.  Yesterday they announced the Public Preview of Release Management in VSTS named Release Management Service. This service will help you automate deployment to multiple environments.  If you have ever worked with the On-Premise version you will know that it is easy to setup and it gives you the option to visualize and track the progress of your deployment.  &nbsp;    &nbsp;  In a set off step you can setup a release:  &nbsp;   Check-in code in Visual Studio Team Services. Setup a build for your source  code. Add a Azure Endpoint to VSTS. Create your Release Definition.  &nbsp;  For more information you can check the following Microsoft site:   http://aka.ms/rmpreview  &nbsp;  If you are not following the Microsoft Connect Event make sure you will follow the Connect event today:   http://www.visualstudio.com/connect2015  ","categories": ["Development"],
        "tags": ["Release Management","VSTS"],
        "url": "/2015/11/release-management-in-visual-studio-team-services/",
        "teaser": null
      },{
        "title": "HTTPS only for Azure Websites",
        "excerpt":"In some situations you would like your Azure website to only work under HTTPS. By default a Azure website will work under HTTP and HTTPS. Today I was searching for a option how to disable HTTP traffic. Looking in de old and new portal did not help at all.  &nbsp;  The next option was to rewrite the URL. You can if you need to rewrite the URL from your web.config file. If you place a “rewrite” element within the “system.webServer” element of your web.config file you can specify rules in order to rewrite your URL.  Take a look at my example for rewriting to HTTPS.  &nbsp;  &lt;system.webServer&gt;   &lt;rewrite&gt;     &lt;rules&gt;       &lt;rule name=\"Redirect HTTP to HTTPS\"&gt;         &lt;match url=\"(.*)\" /&gt;         &lt;conditions&gt;           &lt;add input=\"{HTTPS}\" pattern=\"off\" ignoreCase=\"true\" /&gt;         &lt;/conditions&gt;         &lt;action type=\"Redirect\" url=\"https://{HTTP_HOST}/{R:1}\" redirectType=\"Permanent\"/&gt;       &lt;/rule&gt;     &lt;/rules&gt;     &lt;/rewrite&gt; &lt;/system.webServer&gt;  &nbsp;  The rule is named “Redirect HTTP to HTTPS”  and matches all URLs. Based on the input conditions the rule will not be applied on HTTPS traffic.  The action will then rewrite the URL to HTTPS.  &nbsp;  With help from:  Benjamin Perkins  ","categories": ["Azure"],
        "tags": ["Azure","Url Rewrite","Website"],
        "url": "/2015/12/https-only-for-azure-websites/",
        "teaser": null
      },{
        "title": "Application Insights integration in Visual Studio",
        "excerpt":"Application Insights has been around for a while now and after a while I was wondering if there was a integration within Visual Studio.  For the readers that do not know what Application Insights is a small description:  “Application Insights is an extensible analytics service that monitors your live application. It helps you detect and diagnose performance issues, and understand what users actually do with your app. It's designed for developers, to help you continuously improve the performance and usability of your app.”  After doing a couple of Azure related projects that all use Application Insights, some kind of integration with Visual Studio would be great. The integration I knew was the integration within the context menu:     Open Application Insights: Opens the application insights screen within the Azure preview portal.   Configure Application Insights: Opens the application insights configuration screen within the Azure preview portal.   Search: Opens a integrated window within Visual Studio to search trough events within Application Insights.  Untill today I was only aware of the first two options maybe upgrade 1 of Visual Studio 2015 added the search functionality.  The update I received today made me wondering what the integration within Visual Studion 2015 is.    The “Search” option can also be found within the “View” menu under “Other Windows” and then “Application Insights Search”.    When the window opens it will try to make a connection to an Azure account and you will get the option to select an Application Insights application. After you made you choice your are able to search and select a specific time frame to search in.  The search results can be filterd by specific events:   Custom Event Dependency Exception Page View Request Trace  When you perform the search it will give you a list of results.    On the left side of the screen you can further drill down into the results by the different properties the events contain. By selecting a specific event a detail view will be added in the window. This view will include all neccesary information regarding the event.    The details window will also include a lot of information about exceptions.    The information shown in the integration is really fantastic specially if you are looking for specific events. Another great point about this integration is that you do not require to switch windows when developing a application that is using application insights.   ","categories": ["DevOps"],
        "tags": ["Application Insights","Azure","Visual Studio"],
        "url": "/2015/12/application-insights-integration-visual-studio/",
        "teaser": null
      },{
        "title": "Application Insights configurable instrumentation key",
        "excerpt":"Starting with Application Insights can be done by adding NuGet Packages to your project.    In the above screenshot you see two NuGet Packages:   Microsoft.ApplicationInsights.JavaScript: NuGet Package for JavaScript applications.   Microsoft.ApplicationInsights.Web: NuGet Package for .Net web applications.  By adding both packages you can use application insights in for example a MVC application. Adding those NuGet packages will add a default JavaScript section within the “_Layout.cshtml” file. This default section will contain a instrumentation key that points to the correct Application Insights instance in Azure. This key is added as a string value.  When running multiple instances of your application (development, test, acceptance and production), you don’t want all events to be registered in the same Application Insights instance. You can change the string value on every deploy but you can also make it configurable.  When you start off the “_Layout.cshtml” file looks like this.    To make that section configurable we open the “Global.asax” file in the project and add the following using statement.  using Microsoft.ApplicationInsights.Extensibility;  With this using statement you can set specific configurable values of application insights and in special the instrumentation key. After the using is added we add we add the application start event to the file and set the “TelemetryConfiguration” “InstrumentationKey” to a key we specify in the “Web.Config” file.  protected void Application_Start() {     TelemetryConfiguration.Active.InstrumentationKey = WebConfigurationManager.AppSettings[AppSettingKeys.InstrumentationKey]; }  When this value is set the below line will get the specified instrumentation key in you Razor files.  @Microsoft.ApplicationInsights.Extensibility.TelemetryConfiguration.Active.InstrumentationKey  The string value of the instrumentation key can be replaced by the above line and the complete JavaScript reference for application insights will then look like this.  &lt;script type=\"text/javascript\"&gt;     var appInsights = window.appInsights || function (config) {         function s(config) { t[config] = function () { var i = arguments; t.queue.push(function () { t[config].apply(t, i) }) } } var t = { config: config }, r = document, f = window, e = \"script\", o = r.createElement(e), i, u; for (o.src = config.url || \"//az416426.vo.msecnd.net/scripts/a/ai.0.js\", r.getElementsByTagName(e)[0].parentNode.appendChild(o), t.cookie = r.cookie, t.queue = [], i = [\"Event\", \"Exception\", \"Metric\", \"PageView\", \"Trace\"]; i.length;) s(\"track\" + i.pop()); return config.disableExceptionTracking || (i = \"onerror\", s(\"_\" + i), u = f[i], f[i] = function (config, r, f, e, o) { var s = u &amp;&amp; u(config, r, f, e, o); return s !== !0 &amp;&amp; t[\"_\" + i](config, r, f, e, o), s }), t     }({     instrumentationKey: '@Microsoft.ApplicationInsights.Extensibility.TelemetryConfiguration.Active.InstrumentationKey'     });      window.appInsights = appInsights;     appInsights.trackPageView(); &lt;/script&gt;  When this option is configured and deployed the instrumentation key can be altered by using the Azure Portal.    ","categories": ["DevOps"],
        "tags": ["Application Insights","Azure"],
        "url": "/2015/12/application-insights-configurable-instrumentation-key/",
        "teaser": null
      },{
        "title": "Development Cycles (DevOps) and Office365",
        "excerpt":"Just before the Christmas Holiday’s I wanted to share some insights with the readers of my blog around Development Cycles and Office365.  Development Cycles can also be interpreted as DevOps. Where DevOps is more the culture and practice around the development cycles. According to Wikipedia DevOps can be described as this:  “DevOps (a clipped compound of \"development\" and \"operations\") is a culture, movement or practice that emphasizes the collaboration and communication of both software developers and other information-technology (IT) professionals while automating the process of software delivery and infrastructure changes. It aims at establishing a culture and environment where building, testing, and releasing software, can happen rapidly, frequently, and more reliably.”  As this description on DevOps is quite good how can this be setup for Office365.    As you can see in the above image you will have four environments in a ideal situation.    [tenant name]dev: Development environment for the developers. When your developers have a MSDN license you could let them work on their MSDN tenant. [tenant name]tst:Test environment for the integration test. Every developer will apply his/her changes on the environment using scripts, or by hand (delivered in a document) if the changes can not be scripted.  [tenant name]acc:Acceptance environment can be used as a test environment for your test users. [tenant name}: This environment can be used by the end users.  In smaller environments you could remove the acceptance environment and do the end users tests after the integration tests are finished on the test environments.    In the last year I have also seen projects were they create a test, acceptance and production site collection on the same tenant. When using this approach you won’t be able to test and alter global Office365 configurations.  A downside of the full blown approach is that you will have to manage multiple user stores. Most useful as a user store is Azure Active Directory. A On-premise AD can only be synced to one Azure Active Directory, all other instances will have user accounts as onmicrosoft.com accounts.  Whit scripting deployments you will have to make sure you create and develop the scripts as generic as possible, this makes sure you can reuse the scripts (for example for other clients).   To make PowerShell script reusable you will have to add configuration file this can be done in XML of JSON for example.   One search via Google will deliver a lot of scripts that can be configured via XML. If you ever need help with your scripts just let me know.  As stated above the changes that can’t be scripted, need to be documented in order for a administrator to configure those things during the deployment.  Another important thing about this DevOps setup is to configure the development, test and acceptance tenants as First release tenants. This will make sure you receive the updates on Office365 before you receive them on the production environment, and take appropriate actions if needed.   &nbsp;  With help from:  Chris Hines  ","categories": ["DevOps"],
        "tags": ["DevOps","Office 365","Powershell"],
        "url": "/2015/12/development-cycles-devops-office365/",
        "teaser": null
      },{
        "title": "Deploy your Azure Website with VSTS Build",
        "excerpt":"From VSTS (Visual Studio Team Services) your can deploy to a Azure Website directly directly from a build. Setting this up will require a couple of steps. In this blog post I will help you trough these steps.  Create New Build  Navigate to VSTS within the browser and open the team page where a build definition needs to be added. From the team page select build to go to the build configuration section.    The build tab will show the status of current builds. On this page you are also able to create a new build. To create a new build select the + icon in the upper left corner.    Pressing the + icon will make a dialog appear. In this dialog you need to select what kind of build needs to be created. We want to deploy to Azure so we select the “Deployment”&nbsp; tab and then select “Azure WebSite”.    When selected press next. On the next screen you have to specify a couple of build settings settings. For example the “Repository type” on this screen you can also select a check box called “continuous integration: build each check-in”. Selecting this option will make sure a new build will be queued after a check-in.    Select the configuration depending on you situation and click on “Create”. In the background the build definition will be created and you will be navigated to a screen that contains all the steps in order to deploy to Azure.  Configure the Build Definition Build Blocks     The first two building blocks of the build definition can stay as they are by default. In the first building block there is a setting called “MSBuild Arguments”. This settings shows for example were the Web Deployment Package will be placed:  /p:PackageLocation=\"$(build.artifactstagingdirectory)\\\\  The second block will execute tests if there are any in your solution. By default it will stop the build if there are tests that fail during the build. By selecting “Continue on error” the build will continue even if tests fail.    The next block is the block that will execute the deployment. As you can see the block is red and that is because not all required properties are filled in.  Select the building block you will notice that the Azure subscription isn’t filled.  When starting with a new VSTS tenant you will not have any Azure subscriptions configured. The Azure Subscription can be configured by clicking on “Manage”.    Configure Azure Service Endpoint  Clicking on manage will open a new browser window will that brings you tot the services endpoint configuration page of VSTS. On this page a Azure Subscription needs to be added by selecting the + icon in the upper left corner, and then selecting Azure.    In the dialog that appears authentication information for windows Azure subscription needs to be filled in. This information has to be retrieved and this can be done with the Azure PowerShell window.   Open the Microsoft Azure PowerShell window in administrative mode  Execute the following PowerShell command:  Get-AzurePublishSettingsFile    Executing the commend will open a web browser and automatically download your subscription file  Open the subscription file (named [SubscriptionName]-[Date]-credentials.publishsettings)  Find the Subscription ID and the ManagementCertificate in the file.  In the dialog window select “Certificate Based” and copy and past all the right information.    Press “OK” to save the Service Endpoint and navigate back to the build tab in your browser.  With the service endpoint configured the&nbsp; Azure Subscription field needs to be refreshed. When the field is refreshed you can select the appropriate Azure Subscription.  After setting the Azure Subscription the Web Site information needs to be added. The Web Deploy Package settings will deploy all zip files within your build it is also possible to specific a specific package if your know the name by altering the line.  $(build.stagingDirectory)\\MSFT-DEMO-WEBAPP.zip    The last two building block have the right configuration by default. If you want you can also delete them from the build definition.  Now that everything is setup correctly the build definition can be saved. When saving for the first time you will be able to give it a appropriate name.    When saved the build can be queued by selecting “Queue Build”.    ","categories": ["DevOps"],
        "tags": ["Azure","Build","Deployment","VSTS"],
        "url": "/2016/01/deploy-azure-website-vsts-build/",
        "teaser": null
      },{
        "title": "Azure Hybrid Connection",
        "excerpt":"Hybrid connection is a component of Azure BizTalk Services. With a Hybrid connection you can easily connect Azure App Services with on-premise services behind a firewall.    Hybrid connection is one of the possible solutions to connect to a on-premise environment. The Hybrid connection has the following advantages:   Safely unlock on-premise data. Multiple Azure App Services can use the same hybrid connection. Minimal number of TCP ports needed for access to the locale network. Only access to specific on-premise resources, configured in the Hybrid Connection. Can connect to every on-premise source with a static TCP-port for example: SQL Server, MySQL, HTTP Web API’s and Web Services.  Note:”TCP sources with a dynamic port are not supported at this moment”   Hybrid connection can be used with every framework the Azure App Services Supports. Connection properties when Hybrid Connections are in place are exactly the same as when you setup the connection in a on-premise environment.  Reading those advantages make you wonder on how the Hybrid Connection is secured. The connections are secured with a Shared Access Signature (SAS). This will make sure the connection between the Azure application and the on-premise Hybrid Connection Manager are secured.  To setup a Hybrid Connection you do not require to have any inbound ports open. In a situation were you do not open any port the Hybrid Connection will use TCP-port 80 and 443 by default. This configuration is not preferred.  If a preferred configuration is needed you will have to configure the following out bound http-traffic. For Hybrid Connections it is not needed to configure inbound traffic ports in the firewall.     Port Why Outbound   9350-9354 These ports will be used for data traffic. The Service Bus Relays manager uses port 9350 to determine if TCP traffic is available. If it is it will also presume 9352 is available. Data traffic will go over 9352 yes   5671 If port 9352 is used for data traffic port 5671 will be used as control channel. yes   80-443 If port 9352 and 5671 are not available port 80 and 443 will be used for data traffic and control channel.  Note: This is not a preferred configuration. yes    &nbsp;  Setting up a Hybrid Connection  In order to setup a Hybrid Connection you can connect to the Azure Preview portal and navigate to the Azure App Service you would like to configure the Hybrid Connection for.    Select “Settings”, “Networking”, “Configure your hybrid connection endpoint”. A screen will appear on the left side with your current Hybrid Connections. To add a new one click Add on the top of the screen.    Every time the process goes on a new screen will appear at the left side. After selecting the Add button you will get a option to create a new hybrid connection or use a existing hybrid connection.  Because this is the first time we do not have any hybrid connection we will select “New hybrid connection”.  You will now have to fill in the following information:     Name Unique Azure name for the hybrid connection   Hostname The hostname of the on-premise you will connect to. This can also be the FQDN or the IP Address   Port The static TCP-port you will connect to.    After the information is filled in a BizTalk Service needs to be configured that will handle the connection (Service Bus Relay). You will configure a new one with a appropriate name.  By selecting Ok on both the BizTalk Service creation window and the New hybrid connection window the Hybrid Connection will be configured and created.  When this is done the hybrid connection will be shown in the hybrid connections overview of the Azure App Service.    As you can see in the picture the hybrid connection is not yet connected. You can connect the hybrid connection by taking the following steps.   Login to the server were you would like to install the Hybrid Connection Manager. On the server navigate to the Azure Preview Portal. Within the Azure Preview Portal navigate to the Azure App Service were you configured  the Hybrid Connection. Open the Hybrid Connections overview by clicking: “Settings”, “Networking”, “Configure your hybrid connection endpoint”. Click on the hybrid connection that we created.  In the window that appears you can see that it isn’t configured. To configure the connection click on “Listener Setup”.    Clicking on “Listener Setup” will open a window were you get the option to configure the Hybrid Connection. This can be done manually by using the connection strings or by installing en configuring it directly.  Let’s click “Install and Configure now”. This opens a new tab that will download an installation file.    Run this application and it will make sure that the Hybrid Connection Manager will be installed and configured.    After the installation is finished you will also notice that the configuration is successful within the azure preview portal.    With the Hybrid Connection in place you can connect to your on-premise systems just like you would do when you deploy your service to a on-premise environment.  ","categories": ["Azure"],
        "tags": ["Azure","Hybrid Connection"],
        "url": "/2016/01/azure-hybrid-connection/",
        "teaser": null
      },{
        "title": "Azure WebJobs made simple",
        "excerpt":"The last couple of weeks I have been playing around with Office Mix. For those who don’t know Office Mix, Office Mix is a add-in for PowerPoint that is in preview to easily create and share interactive online videos.  With this add-in it is very easy to create a Video of your presentations and also insert screen recordings. For a first pilot I created a PowerPoint presentation called: “Azure WebJobs made simple”.  With Azure WebJobs you have options to run different background tasks. The WebJobs will be hosted on the same virtual machine as you Azure Web App.  The following file types can be used as a Azure WebJob:   .exe - .NET assemblies .cmd, .bat, .exe (using windows cmd) .sh (using bash) .php (using php) .py (using python) .js (using node)  For this blog post and for the video I created a simple Console Application that executes the following code.  class Program {     static void Main(string[] args) {          Console.WriteLine(\"Starting Web Job Application\");          Stopwatch timer = new Stopwatch();         timer.Start();          Console.WriteLine(DateTime.Now);          for (int i = 0; i &lt; 15; i++) {             WriteInfo(i);             Thread.Sleep(500);         }          timer.Stop();         Console.WriteLine(\"Web Job Procession took: \" + timer.Elapsed.Seconds + \" seconds\");          Console.WriteLine(\"Finished Web Job Application\");     }      static void WriteInfo(int number) {         Console.WriteLine(\"Working on item: \" + number);     } }  With the use of “Console.WriteLine” you can post message on the WebJob console. Within Azure there are two types of WebJobs.   Continuous: Actions are taken when for example a item is added in a Queue. On demand: Actions run on demand.  Besides these two option you are also able to schedule WebJobs. This can be done by using the old portal or by publishing the WebJob with Visual Studio.  The following actions are displayed in the video:   What can run on Azure Create a Azure Web App ","categories": ["Development"],
        "tags": ["Azure","Azure Web Apps","Office Mix","WebJobs"],
        "url": "/2016/02/azure-webjobs-made-simple/",
        "teaser": null
      },{
        "title": "Adding Azure App Service Application Settings with PowerShell",
        "excerpt":"Within Azure there is a option to change several configuration settings. When working with  Deployment Slots this means you have to click a lot within the Azure Portal and that can be a very time consuming operation.  The configuration of the Azure App Service can be automated by using PowerShell, for this first example we will start configuring Application settings.  To maintain the application settings values we start off by making a configuration file in which the key value pairs can be saved.  &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt; &lt;Azure SubscriptionName=\"Pay-As-You-Go\"&gt;   &lt;AppService Name=\"api-app\" ResourceGroup=\"Demo-Group\"&gt;       &lt;AppSettings&gt;         &lt;AppSetting Key=\"Test\" Value=\"Test\"&gt;&lt;/AppSetting&gt;       &lt;/AppSettings&gt;       &lt;/AppService&gt;   &lt;AppService Name=\"api-app\" ResourceGroup=\"Demo-Group\" Slot=\"acc\"&gt;     &lt;AppSettings&gt;       &lt;AppSetting Key=\"Test\" Value=\"Test\"&gt;&lt;/AppSetting&gt;     &lt;/AppSettings&gt;   &lt;/AppService&gt; &lt;/Azure&gt;  If you look at the configuration file you will notice that there is one API application within Azure. This API Application also has a deployment slot called “acc”.  With the use of PowerShell we can read out this configuration file and add the application settings to the right deployment slot.  $fileName = \"configuration.xml\";  Write-Host \"Loading configuration file for Azure:\" $fileName;  $config = [xml] (Get-Content $fileName)  if ($config -eq $null) {     Write-Host \"Error loading configuration file, make sure the file exists.\" -ForegroundColor Red     Exit } $subscription = $config.Azure.SubscriptionName  Write-Host \"Login to the Azure Environment:\" -ForegroundColor Green  #login to Azure Login-AzureRmAccount  Write-Host \"Setting the correct Subscription:\" -ForegroundColor Green  #get the Azure Subscription Get-AzureRmSubscription –SubscriptionName $subscription | Select-AzureRmSubscription  foreach($webApp in $config.Azure.AppService){          $appService = '';     $group = $webApp.ResourceGroup;     $name = $webApp.Name;     $slot = $webApp.Slot;      if(!$slot){         Write-Host \"Applying settings for app service : \" $name \" in group: \" $group -ForegroundColor Green        $appService = Get-AzureRmWebApp -Name $name -ResourceGroupName $group;     }else{       Write-Host \"Applying settings for app service : \" $name \" in group: \" $group \"in slot: \" $slot -ForegroundColor Green       $appService = Get-AzureRmWebAppSlot -Name $name -Slot $slot -ResourceGroupName $group;     }      $appSettings = $appService.SiteConfig.AppSettings      #setup the current app settings     $settings = @{}     ForEach ($setting in $appSettings) {         $settings[$setting.Name] = $setting.Value     }      #adding new settings to the app settigns     foreach($appSetting in $webApp.AppSettings.AppSetting){         $settings[$appSetting.Key] = $appSetting.Value;     }      if(!$slot){         $app = Set-AzureRMWebApp -Name $name -ResourceGroupName $group -AppSettings $settings     }else{         $app = Set-AzureRMWebAppSlot -Name $name -ResourceGroupName $group -AppSettings $settings -Slot $slot     }      Write-Host \"Application settings applied to: \" $appService.Name -ForegroundColor Green }  In the code snip-it above there are a lot of comments so I hope you understand what happens within the script. The main difference for updating the production environment in the slot is described in the following paragraph.  By using “Set-AzureRMWebApp” you can set the the properties of the production version. By using “Set-AzureRMWebAppSlot” and specifying the “Slot” parameter you update the settings of the specified slot.  Know I hear you think: What about the specific slot settings as within the picture below.    Until know I haven’t found a solution other then doing this in the Azure Portal or by using Azure Classic scripts.  In order to run the upcoming PowerShell a Azure subscription needs to be configured (https://www.opsgility.com/blog/windows-azure-powershell-reference-guide/getting-started-with-windows-azure-powershell/).  With the use of “Select-AzureSubscription” the default Azure subscription will be set. The subscription that will be used is configured in the configuration file.  After that the “Set-AzureWebsite” will be used to set the property “SlotStickyAppSettingNames”. This property needs to be set to a string array that contains all the key’s that need to be sticky slot settings.  Set-AzureWebsite -Name \"api-app\" -SlotStickyAppSettingNames @(\"Test\")   To support this within the configuration file we alter the app settings. Besides that you must make sure that you set this property on the default (production) slot of your app service. Otherwise you will get the following exception.  Set-AzureWebsite : &lt;string xmlns=\"http://schemas.microsoft.com/2003/10/Serialization/\"&gt;{\"Code\":\"BadRequest\",\"Message\":\"The names  of the app settings and connection strings can be only set on a parent site level because they apply to all site's slots.\",\"Exten dedCode\":\"04084\",\"MessageTemplate\":\"The names of the app settings and connection strings can be only set on a parent site level b ecause they apply to all site's slots.\",\"Parameters\":[],\"InnerErrors\":null}&lt;/string&gt; At line:2 char:1 + Set-AzureWebsite -Name \"api-app\" -SlotStickyAppSettingNames @(\"Test\") -Slot \"acc\" + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     + CategoryInfo          : CloseError: (:) [Set-AzureWebsite], CloudException     + FullyQualifiedErrorId : Microsoft.WindowsAzure.Commands.Websites.SetAzureWebsiteCommand  With the extension the configuration file know looks liked the below code snip-it.  &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt; &lt;Azure SubscriptionName=\"Pay-As-You-Go\"&gt;   &lt;AppService Name=\"api-app\" ResourceGroup=\"Demo-Group\"&gt;       &lt;AppSettings&gt;         &lt;AppSetting Key=\"Test\" Value=\"Test\" Slot=\"true\"&gt;&lt;/AppSetting&gt;       &lt;/AppSettings&gt;       &lt;/AppService&gt;   &lt;AppService Name=\"api-app\" ResourceGroup=\"Demo-Group\" Slot=\"acc\"&gt;     &lt;AppSettings&gt;       &lt;AppSetting Key=\"Test\" Value=\"Test\"&gt;&lt;/AppSetting&gt;     &lt;/AppSettings&gt;   &lt;/AppService&gt; &lt;/Azure&gt;   With the configuration file altered the classic PowerShell code and the “Set-AzureWebsite” can be added to the PowerShell script that results in the following file.  $fileName = \"configuration.xml\";  Write-Host \"Loading configuration file for Azure:\" $fileName;  $config = [xml] (Get-Content $fileName)  if ($config -eq $null) {     Write-Host \"Error loading configuration file, make sure the file exists.\" -ForegroundColor Red     Exit } $subscription = $config.Azure.SubscriptionName  Write-Host \"Login to the Azure Environment:\" -ForegroundColor Green  #login to Azure Login-AzureRmAccount  Write-Host \"Setting the correct Subscription:\" -ForegroundColor Green  #get the Azure Subscription Get-AzureRmSubscription –SubscriptionName $subscription | Select-AzureRmSubscription Select-AzureSubscription -SubscriptionName $subscription  foreach($webApp in $config.Azure.AppService){          $appService = '';     $group = $webApp.ResourceGroup;     $name = $webApp.Name;     $slot = $webApp.Slot;      if(!$slot){         Write-Host \"Applying settings for app service : \" $name \" in group: \" $group -ForegroundColor Green        $appService = Get-AzureRmWebApp -Name $name -ResourceGroupName $group;                if(!$slot){             if($appSetting.Slot -eq 'true' -or $appSetting.Slot -eq '1'){                 $stickyslot += $appSetting.Key;             }         }             }else{       Write-Host \"Applying settings for app service : \" $name \" in group: \" $group \"in slot: \" $slot -ForegroundColor Green       $appService = Get-AzureRmWebAppSlot -Name $name -Slot $slot -ResourceGroupName $group;     }      $appSettings = $appService.SiteConfig.AppSettings      #setup the current app settings     $settings = @{}     ForEach ($setting in $appSettings) {         $settings[$setting.Name] = $setting.Value     }      #adding new settings to the app settigns     foreach($appSetting in $webApp.AppSettings.AppSetting){         $settings[$appSetting.Key] = $appSetting.Value;     }      if(!$slot){         $app = Set-AzureRMWebApp -Name $name -ResourceGroupName $group -AppSettings $settings                  if($stickyslot.Count -lt 0){             Write-Host \"Set Sticky Slot Settings\" -ForegroundColor Yellow             Set-AzureWebsite -Name $name -SlotStickyAppSettingNames $stickyslot         }     }else{         $app = Set-AzureRMWebAppSlot -Name $name -ResourceGroupName $group -AppSettings $settings -Slot $slot     }      Write-Host \"Application settings applied to: \" $appService.Name -ForegroundColor Green } ","categories": ["Development"],
        "tags": ["Azure","Azure App Services","Powershell"],
        "url": "/2016/02/adding-azure-app-service-application-settings-powershell/",
        "teaser": null
      },{
        "title": "Motion10 hosting one of the Global Azure Bootcamp",
        "excerpt":"My employer Motion10 will be hosting one of the many Global Azure Bootcamp around the world.  In April 2013 the first Global Windows Azure Bootcamp was held at more than 90 locations around the globe. In March 2014 this was topped with 136 locations, and last year 2015 there where 182 locations!  This year the Global event will be held again. It is a one-day deep dive class to help thousands of people get up to speed on developing solutions for Azure.  Motion10 (Rotterdam, Netherlands) will be one of the location in 2016. Me, Tomasso Groenendijk (Azure MVP) and Paul Baars will prepare the presentations and hand-on labs.  All information that I prepare will be shared on this Blog after the bootcamp, and register for the event when you are in the area on 16 April.   Register More Information (Dutch) Global Azure Bootcamp Event  Agenda  Part 1     10:00 – 10:15 Welcome   10:15 – 11:00 API Apps – Tomasso Groenendijk (Azure MVP)   11:15 – 12:15 Hands-on   12.15 – 12:45 Azure Game   12:45 Lunch    Part 2     13:30 – 14:15 Web Apps – Maik van der Gaag   14:30 – 15:30 Hands-on   15:30 – 16:00 Logic Apps – Paul Baars   16:00 Drinks    &nbsp;  What is Global Azure Bootcamp 2016?    Building on the exceptional success of previous years, Global Azure Bootcamp 2016 (#GlobalAzure) is a free one-day training event, taking place on the 16th of April 2016 in several venues worldwide, driven by local Microsoft Azure community enthusiasts and experts. It consists of a day of sessions and labs based on the Microsoft Azure Readiness Kit or custom content. The event has been originally designed by 5 Microsoft Azure MVPs in order to benefit the local community members and teach essential Microsoft Azure skills and know-how. While supported by several sponsors, including Microsoft, the event is completely independent and community-driven.  ","categories": ["Azure","Event"],
        "tags": ["Azure","Global Azure Bootcamp","motion10","Presenting"],
        "url": "/2016/02/motion10-hosting-one-of-the-global-azure-bootcamp/",
        "teaser": null
      },{
        "title": "Managing your Azure Hybrid Connection with PowerShell",
        "excerpt":"When you have a Azure Hybrid Connection setup the wrong way, you can delete the connection trough the UI but then the Hybrid Connection Manager still knows the connection.  If you for example are moving things around and you would like to keep the connection within the UI you will have to use PowerShell to alter the connections.  All connection strings needed can be found within the Azure Portal  Adding a Connection to the Hybrid Connection Manager  Add-HybridConnection -ConnectionString [ConnectionString] Get all configured connections  Get-HybridConnection   or with specific properties  Get-HybridConnection | select -Property Uri   Get a specific connection  Get-HybridConnection -Uri [URI] or  Get-HybridConnection -ConnectionString [ConnectionString]   &nbsp;  Remove a specific connection  Remove-HybridConnection -Uri [URI] or  Remove-HybridConnection -ConnectionString [ConnectionString] Update connection string  Update-HybridConnection -ConnectionString [ConnectionString] &nbsp;  ref: Microsoft Information  ","categories": ["Azure"],
        "tags": ["Azure","Hybrid Connection"],
        "url": "/2016/02/managing-azure-hybrid-connection-powershell/",
        "teaser": null
      },{
        "title": "Unable to Establish connection with Azure Remote Debugging",
        "excerpt":"Within one of my projects we had problems with attaching the debugger to our Azure Instances. During the the process we would get a model stating that the operation is taking longer as expected and finally got the model dialog shown below.    “The following error occurred while launching remote debugging: Unable to connect to the Microsoft Visual Studio Remote Debugger named [debugger name]. The Visual Studio 2015 Remote Debugger (MSVSMON.EXE) does not appear to be running on the remote computer. This may be because a firewall is preventing communication to the remote computer. Please see Help for assistance en configuring remote debugging.”  In the event log of the App Service we found a error stating that the connection could not be made, and that a instance of the remote debugger is already running.  &lt;Event&gt;     &lt;System&gt;         &lt;Provider Name=\"Visual Studio Remote Debugger\"/&gt;         &lt;EventID&gt;1000&lt;/EventID&gt;         &lt;Level&gt;1&lt;/Level&gt;         &lt;Task&gt;0&lt;/Task&gt;         &lt;Keywords&gt;Keywords&lt;/Keywords&gt;         &lt;TimeCreated SystemTime=\"2016-02-17T11:21:39Z\"/&gt;         &lt;EventRecordID&gt;1544228421&lt;/EventRecordID&gt;         &lt;Channel&gt;Application&lt;/Channel&gt;         &lt;Computer&gt;RD000D3A2303CA&lt;/Computer&gt;         &lt;Security/&gt;     &lt;/System&gt;     &lt;EventData&gt;         &lt;Data&gt;remdbguser connected.&lt;/Data&gt;     &lt;/EventData&gt; &lt;/Event&gt; &lt;Event&gt;     &lt;System&gt;         &lt;Provider Name=\"Visual Studio Remote Debugger\"/&gt;         &lt;EventID&gt;1001&lt;/EventID&gt;         &lt;Level&gt;3&lt;/Level&gt;         &lt;Task&gt;0&lt;/Task&gt;         &lt;Keywords&gt;Keywords&lt;/Keywords&gt;         &lt;TimeCreated SystemTime=\"2016-02-17T11:21:41Z\"/&gt;         &lt;EventRecordID&gt;1544230937&lt;/EventRecordID&gt;         &lt;Channel&gt;Application&lt;/Channel&gt;         &lt;Computer&gt;RD000D3A2303CA&lt;/Computer&gt;         &lt;Security/&gt;     &lt;/System&gt;     &lt;EventData&gt;         &lt;Data&gt;Msvsmon was unable to start a server named '127.0.0.1:51040'. The following error occurred: An instance of the remote debugger is already running on this computer, or another process is already bound to the specified TCP/IP port. View Msvsmon's help for more information.&lt;/Data&gt;         &lt;Binary&gt;08005E80&lt;/Binary&gt;     &lt;/EventData&gt; &lt;/Event&gt;  Since the remote debugger was activated on the Azure App Service it had to be a firewall configuration.  After a lot of searching we found out that the following ports needs to be opened to be able to remote debug a Azure App Service:     Ports Incoming/Outgoing Protocol Description   3702 Outgoing UDP Remote debugger discovery   4020 Outgoing TCP Visual Studio 2015   4021 Outgoing TCP Visual Studio 2015   4016 Outgoing TCP Visual Studio 2012   4018 Outgoing TCP Visual Studio 2013    The remote debugging port number is incremented by 2 for each Visual Studio version. You can open these ports for all or limit it to all Azure Datacenters IP’s that you can find here.   Microsoft Azure Datacenter IP Ranges  Also make sure you enable NAT (Network Address Translation) for the firewall exclusion because without it it will not work.  Reference:https://msdn.microsoft.com/en-us/library/mt592019.aspx and https://msdn.microsoft.com/en-us/library/mt592018.aspx  ","categories": ["Azure","Troubleshooting"],
        "tags": ["Azure","Azure App Services","Debugging","Remote"],
        "url": "/2016/03/unable-establish-connection-azure-remote-debugging/",
        "teaser": null
      },{
        "title": "Securing an API app with an Application Identity&ndash;Part 1",
        "excerpt":"Deploying an API within Azure is a very simple tasks. But how about security? The API itself is exposed to the outside world. When for example have a API app that is connected to on-premise systems with an Hybrid Connection the API will have to be secured in a certain way.  An option is to use Azure Active Directory. Within Azure Active Directory you can register an Application Identity for the API App. The connecting application will also have to be registered within Active Directory in order to connect to the API Application (This part will be discussed in part 2).  Part 1 will contain the setup of the API application in Part 2 the configuration and code changes for the client application will be discussed.  First off all the API app needs to be registered within Azure Active Directory. In order to manage Azure Active Directory the old Azure portal : https://manage.windowsazure.com needs to be used.  Azure Active Directory  Open up the Active Directory and click on the “Applications” tab.    In the bar at the bottom of the screen you can add a new application by selecting new. Selecting “New” will open a wizard for adding an application.    Select the option “Add an application my organization is developing”.    Enter the name of the application and since our application is an API app select “Web Application and/or Web API” and click next.    Fill-in a Sign-On URL if you do not have a specific sign-in URL fill in your default URL for example “https://example.com” (Note: The URL needs to be secured with SSL).  The APP ID URI is a URL specific for your tenant, this URL will need to contain the tenant name. It needs to be constructed in the following way: https://[tenant-name]/[app-name]  Save the options, the configuration screen of the application will be opened.  Code Changes  When the steps within Azure Active Directory are completed the API also needs some small code changes. To secure the application it self a couple of configuration need to be saved and we will save these in the web.config file.  &lt;add key=\"auth:Tenant\" value=\"[Tenant name, example.onmicrosoft.com]\" /&gt; &lt;add key=\"auth:APPIDURI\" value=\"[App ID URI of your API Application]\" /&gt; &lt;add key=\"auth:TrustedCallerClientIds\" value=\"[Trusted Callers ClientIds seperated by ';']\" /&gt;  &nbsp;   APPIDURI: APP ID URI of your application. Tenant:The name of the Azure AD tenant in which you registered the application ([tenant].onmicrosoft.com) TrustedCallerIds:The Client Id’s of the clients that are authorized to access the application seperated by ‘;’.  In the “Startup” class register the Windows Azure Authentication mechanism.  public void ConfigureAuth(IAppBuilder app) {     app.UseWindowsAzureActiveDirectoryBearerAuthentication(         new WindowsAzureActiveDirectoryBearerAuthenticationOptions {             TokenValidationParameters = new System.IdentityModel.Tokens.TokenValidationParameters() {                 ValidAudience = ConfigurationManager.AppSettings[\"auth:APPIDURI\"]             },             Tenant = ConfigurationManager.AppSettings[\"auth:Tenant\"]         }); }  Here you register your application and tenant for authentication. By these settings it will know on which tenant it is registered.  Next up it the API method itself. The methods within the API can be secured with a attribute called “Authorize”.  [Authorize] public string Get(string id) {     if (string.IsNullOrEmpty(id))         throw new ArgumentNullException(\"id\", \"Argument 'id' cannot be 'null' or 'string.empty'\");      //Get API }  All of this together will make sure the application is secured. With a few additional steps you can also secure the application for specific caller applications. That is were the “TrustedCallersIds” configuration comes is.  From the ClaimsPrincipal of the caller application the client id can be retrieved. With the client id you check the values against the values saved within the web.config.  [Authorize] public string Get(string id) {      Collection&lt;string&gt; trustedCallerClientId = AuthConfiguration.TrustedCallerClientIds;     string currentCallerClientId = ClaimsPrincipal.Current.FindFirst(\"appid\").Value;      if (trustedCallerClientId.Contains(currentCallerClientId)) {         //Get API     }else{         throw new AuthenticationException(\"Client is not Authorized\");     } }  The “AuthConfiguration” class is a object that retrieves the Authentication settings from the web.config. The “TrustedClientIds” property retrieves the TrustedClientIds from the web.config.  public static Collection&lt;string&gt; TrustedCallerClientIds {     get {         List&lt;string&gt; retVal =new List&lt;string&gt;();         string ids = ConfigurationManager.AppSettings[\"auth:TrustedCallerClientIds\"];         string[] itemIds = ids.Split(new string[]{\";\"}, StringSplitOptions.RemoveEmptyEntries);         retVal.AddRange(itemIds);         return retVal.ToCollection();     } } ","categories": ["Development"],
        "tags": ["API","Application Identity","Azure","Azure Active Directory"],
        "url": "/2016/03/securing-api-app-application-identitypart-1/",
        "teaser": null
      },{
        "title": "Installing Azure Stack on a laptop with Windows 10",
        "excerpt":"Since the Azure Stack public preview has been released I had the idea of trying to install it on my corporate laptop. My corporate laptop has the following hardware specs:   Core I7 32 GB Memory C Drive: 150 GB SSD D Drive: 500 GB SSD  Since Microsoft released the minimal system requirements that are a lot higher (https://azure.microsoft.com/en-us/documentation/articles/azure-stack-deploy/) I saw this as a real challenge. The first time I tried it failed miserably but the second time was a great success.  Download Azure Stack POC items  First step is to download the Microsoft Azure Stack Technical Preview files from the Microsoft site:   https://azure.microsoft.com/en-us/overview/azure-stack/try/?v=try    Create Virtual Machine  After I downloaded all the files the files for the Technical Preview I created a new VM within Hyper-V based on the virtual hard drive “WindowsServer2016Datacenter.vhdx” that was in the set of you downloaded.    The Virtual Machine was configured with the following specs:   27 GB Ram 6 Cores One network adapter with Internet Access 3 Additional Data drives dynamically expanding.  What already might come to mind is the following: Can I use Hyper-V within a Hyper VM machine (Nested Virtualization). The answer is “Yes” if you are running Insider builds of for example Windows 10. In the insider builds there is a early preview feature called Nested Virtualization. It is available if you are running Build 10565 or later, and comes with no performance or stability guarantees.  This article on MSDN explains how you can setup nested virtualization:   Nested Virtualization  I performed the following PowerShell script from my client machine with the VM running.  Invoke-WebRequest https://raw.githubusercontent.com/Microsoft/Virtualization-Documentation/master/hyperv-tools/Nested/Enable-NestedVm.ps1 -OutFile ~/Enable-NestedVm.ps1  ~/Enable-NestedVm.ps1 -VmName \"Windows 2016 TP 4 - Azure Stack Host\"   Invoke-Command -VMName \"Windows 2016 TP 4 - Azure Stack Host\" -ScriptBlock { Enable-WindowsOptionalFeature -FeatureName Microsoft-Hyper-V -Online; Restart-Computer }  &nbsp;  The following screenshot shows the output from these commands:    &nbsp;  Note: When using Nested Virtualization you can not use dynamic memory.  Adjust Setup Files of Azure Stack  In order to install Azure Stack on minimal hardware some setup files need to be adjusted. In order to make these adjustments Mount the virtual drive called: “MicrosoftAzureStackPOC.vhdx”.  On this drive the prerequisites check needs to be changed. The prerequisites checks the amount of memory in the PC. The amount can not be less then 64 GB. If you do not change the PowerShell file and you do not have sufficient amount of memory you will encounter the following error.  \"Check system memory requirement failed. At least 64GB physical memory is required.\"  In order to skip or pass this step the PowerShell that executes the prerequisites needs to be adjusted.  “Invoke-AzureStackDeploymentPrecheck.ps1”    In the file look for the line:  if ($totalMemoryInGB -lt 64) {     throw \"Check system memory requirement failed. At least 64GB physical memory is required.\" }  Change it to the minimum amount of memory your machine will use. This is not the only change you need to make when you want to run Azure Stack on minimal memory. The Azure Service Fabric installer will configure a couple (10) of machines with a specific amount of memory. Running all these machines together will not work on the amount of memory so the configuration needs to be changed.  The configuration for these machines can be adjusted within a file called “PoCFabricSettings.xml” that you can find in the folder “Drive:\\AzureStackInstaller\\PoCFabricInstaller”.  Every VM that will be created during the installation has a configuration element in this file. In order to get it running on my Hardware I needed to adjust the some elements for all machines to the values below.  &lt;ProcessorCount&gt;1&lt;/ProcessorCount&gt; &lt;RAM&gt;1&lt;/RAM&gt;    &lt;MinRAM&gt;1&lt;/MinRAM&gt;    &lt;MaxRAM&gt;2&lt;/MaxRAM&gt;    Installation of Azure Stack  When the setup files are adjusted the drive can be unmounted and all files from the bits can be copied to the newly created VM.  When you login to the VM you need to run a PowerShell window as Administrator and start the “DeployAzureStack.ps1” file for the installation. I suggest running the script with the verbose parameter to get some more information during the installation.  .\\DeployAzureStack.ps1 -verbose  If something looks strange or something went wrong during the installation you can check the log files. The log files are located in the following ProgramData folder.   C:\\ProgramData\\Microsoft\\AzureStack\\Logs  After the prerequisites are passed the installation will continue and ask you to login in to a Azure Active Directory tenant. When the login is successful and you confirm you want to attach Azure Stack to this Azure Active Directory the installation of the Azure Stack POC will begin.    When the installation is done is will show the below message. On my VM the installation took about 4 hours.    Opening the Client VM and the Azure Stack Portal link will lead to the following URL: https://portal.azurestack.local/ with your own Azure Portal.    The base installation of Azure Stack can be extend with the PaaS (Platform as a Service) bits. The installation files and manuals can be found here:   Tools and PaaS services for Azure Stack  Other useful links for setting up a Azure Stack Environment are:   Nested Virtualization Azure Stack deployment tweaking Deploying Microsoft Azure Stack TP1 nested on Hyper-V end to end ","categories": ["Azure"],
        "tags": ["Azure","Azure Stack","Laptop"],
        "url": "/2016/03/installing-azure-stack-laptop-windows-10/",
        "teaser": null
      },{
        "title": "Attending Microsoft Build 2016",
        "excerpt":"I love attending conferences that contain great content and especially when this content is delivered by Program Managers or Evangelists from Microsoft. I have been to a few conferences  and one of the best conferences I attended was TechEd 2014 in Barcelona.  A conference I still wanted to attend was Build .  “Build is an annual conference event held by Microsoft, aimed towards software and web developers using Windows, Windows Phone, Microsoft Azure and other Microsoft technologies. First held in 2011, it serves as a successor for Microsoft's previous developer events, the Professional Developers Conference (an infrequent event which covered development of software for the Windows operating system) and MIX (which covered web development centering on Microsoft technology such as Silverlight and ASP.net).”  I was really disappointed that I got placed on the waiting list after just 5 minutes but a few weeks back I received a email that I had been registered. Meaning that at the end of this month I will be in San Francisco to attend Microsoft Build for the first time and I’m expecting a lot off great content.  On this blog and on twitter I will try to keep you posted on great announcements during the conference.  If you’re at Build and you follow me, be sure to send me a message or tweet and we’ll meet up!  ","categories": ["Event"],
        "tags": ["Build 2016;"],
        "url": "/2016/03/attending-microsoft-build-2016/",
        "teaser": null
      },{
        "title": "Securing an API app with an Application Identity - Part 2",
        "excerpt":"In the previous post the API app was configured and altered to support Application Identity security. The next step is to make a client application ready to call the API’s in a secured way.  In order to do this the Azure Active Directory needs to be aware of the application. For this the needs be added to Active Directory.  Azure Active Directory  Open the old Azure portal : https://manage.windowsazure.com. Open up the Active Directory and click on the “Applications” tab.    In the bar at the bottom of the screen a new application can be added by selecting new. Selecting “New” will open a wizard were in you can add the API application.    Select the option “Add an application my organization is developing”.    Enter the name of the application and since our application is an Web App select “Web Application and/or Web API” and click next.    Fill-in a Sign-On URL if you do not have a specific sign-in URL fill in your default URL for example “https://example.com” (Note: The URL needs to be secured with SSL).  The APP ID URI is a URL specific for your tenant, this URL will need to contain the tenant name. It needs to be constructed in the following way: https://[tenant-name]/[app-name]  Save the options, the configuration screen of the application will be opened.  On this screen we will need to do two things. Generate a App Key in order to authenticate the application and grant the application access to the API.    To generate a new App Key select a specific duration for example 2 years. After saving the configuration the App Key will be generated. Copy the App Key straight away because the App Key will not be visible after you navigate away from the page. If this happens a new key needs to be generated.    We now have to add a permission to other applications. For this click on “Add Application”.    In the dialog search for the application by first selecting “All Apps” in the show property and then selecting your application.    After saving the application is added to the list and the last step we need to take is to delegate the permissions to the API application, by selecting “Delegating Permissions”    Save the complete configuration. Now we need to make a few adjustments to the code in order to call the API.  Code Changes  First off all we need to add some configuration values to the web.config.  &lt;add key=\"auth:ClientId\" value=\"[ClientID]\" /&gt; &lt;add key=\"auth:AppKey\" value=\"[AppKey]\" /&gt; &lt;add key=\"auth:Tenant\" value=\"[Tenant]\" /&gt; &lt;add key=\"auth:AADInstance\" value=\"https://login.microsoftonline.com/{0}\" /&gt;   ClientID: Unique identifier for the application in Azure. AppKey:Credential for the application to authenticate to Azure AD. Tenant:The name of the Azure AD tenant in which you registered the application ([tenant].onmicrosoft.com) AADInstance:This is the instance of Azure, for example public Azure or Azure China.  With these properties in the web.config we can start making calls to the API in a secured way. The first step in doing this is accruing the bearer token from Azure Active Directory.  internal string GetBearerToken() {     string retVal = string.Empty;      AuthenticationContext authContext = new AuthenticationContext(AuthConfiguration.Authority);     ClientCredential clientCredential = new ClientCredential(AuthConfiguration.ClientId, AuthConfiguration.AppKey);      int retryCount = 0;     bool retry = false;     AuthenticationResult authResult = null;     do {         retry = false;         try {             authResult = authContext.AcquireToken(AuthConfiguration.ResourceId, clientCredential);         } catch (AdalException ex) {             //log or retry         }      } while ((retry == true) &amp;&amp; (retryCount &lt; 3));      retVal = authResult.AccessToken;     return retVal; }  The “AuthConfiguration” class is a object that retrieves the Authentication settings from the web.config.  When you required a bearer token you can try to “Get” information or “Post” information to the API.  public T GetObjectsSecured&lt;T&gt;(string endpoint) {     T retVal = default(T);      string token = GetBearerToken();      if (!string.IsNullOrEmpty(token)) {         using (WebClient webClient = new WebClient()) {             webClient.Encoding = Encoding.UTF8;             webClient.Headers.Add(HeaderConsts.ContentType, HeaderConsts.JsonContentType);             webClient.Headers.Add(HttpRequestHeader.Authorization, HeaderConsts.Bearer + \" \" + token);             string returnJson = webClient.DownloadString(endpoint);              retVal = JsonConvert.DeserializeObject&lt;T&gt;(returnJson);         }     }      return retVal; }  public bool UpdateObjectSecured&lt;T&gt;(T item, string endpoint) {      bool retVal = false;     Uri endpointUri = new Uri(endpoint);     string serialized = JsonConvert.SerializeObject(item);      string token = GetBearerToken();      if (!string.IsNullOrEmpty(token)) {         using (HttpClient webClient = new HttpClient()) {             webClient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(HeaderConsts.Bearer, token);             using (HttpRequestMessage inputMessage = new HttpRequestMessage()) {                 inputMessage.Content = new StringContent(serialized, Encoding.UTF8, HeaderConsts.JsonContentType);                 inputMessage.Headers.Accept.Add(new MediaTypeWithQualityHeaderValue(HeaderConsts.JsonContentType));                 HttpResponseMessage message = webClient.PostAsync(endpointUri, inputMessage.Content).Result;                  if (message.StatusCode == HttpStatusCode.OK) {                     retVal = true;                 }             }         } ","categories": ["Development"],
        "tags": ["AAD","API","Azure"],
        "url": "/2016/03/securing-api-app-application-identity-part-2/",
        "teaser": null
      },{
        "title": "Azure Web Application Fundamentals",
        "excerpt":"This weekend my employer Motion10 hosted one of the many Global Azure Bootcamp around the world. Where I presented about Azure Web Application.  This year there where 161 confirmed event locations, that held a one-day deep dive class to help thousands of people get up to speed on developing solutions for Azure.  During this day I had the honor to present about Azure Web Applications, and two colleagues of my presented about other parts of Azure: Tomasso Groenendijk (Azure MVP) and Paul Baars .  With this blog I want to share all the information I created and shared with the attendants of the event hoping to get you up to speed as well.  Labs  Within the package you con download at the bottom of the blog (available on MSDN samples) you will find three labs:   Lab 1 - Getting started with Azure Web Apps and ASP.NET Lab 2 - Azure Web Apps and Visual Studio Team Services Lab 3 - Azure Web Apps and Azure Table Storage  The sample package also contains a solution. In that solution the outcome of Lab 3 is added. The outcome of Lab 3 is a MVC Web Application that connects to Azure Table Storage and performs some basic CRUD operations.  Lab 1  &nbsp;  Lab 1 while give a short introduction on how to work with Azure Web Applications and Visual Studio. During this lab you will create a Azure Web Application and deploy source files from Visual Studio to Azure.  Lab 2  Lab 2 while give a introduction on how to work with Azure Web Applications and continuous integration with Visual Studio Team Services. By using the Azure Portal you will setup continuous integration with an account you create within Visual Studio Team Services.  Lab 3  Lab 3 while give you a introduction on how to work with Azure Table Storage and Azure Web Applications. You will define basic CRUD operations to Create, Read, Update and Delete Table storage data. The source files within the solution contain all the code to perform the basic CRUD operations.  download the package from MSDN Samples  download from MSFTPlayground  Presentation    Azure Web Application Fundamentals from Maik van der Gaag ","categories": ["Event"],
        "tags": ["Azure","GlobalAzure","Labs"],
        "url": "/2016/04/azure-web-application-fundamentals/",
        "teaser": null
      },{
        "title": "Connect Logic Apps with Azure Functions",
        "excerpt":"Azure functions is a component released a few weeks ago by Microsoft in Azure to easily perform code/function in Azure.  “Azure functions is a solution for easily running small pieces of code, or \"functions,\" in the cloud. You can write just the code you need for the problem at hand, without worrying about a whole application or the infrastructure to run it. This can make development even more productive, and you can get started with your first function in just minutes.”  If you want more information on Functions you can get this from the documentation site of Microsoft (https://azure.microsoft.com/en-us/documentation/articles/functions-overview/).  I could imagine a lot of situations were you would like to incorporate pieces of code within a Logic App. You already could achieve this by creating a web / api application but with the use of functions this is a lot easier and you can benefit off the dynamic loading of Azure functions.  To incorporate functions open the Logic App designer and press the + button.    Click “Add an Action”.    The designer will give a couple of options. If you try the search box you won’t find any functions. To incorporate the functions click on “Show Microsoft Managed APIs”.    This option gives several options that exist in the same region. One of the options is Azure Functions.    Clicking on the option “Show Azure Functions in the same region” will open the selection for the functions container. Select the right container.    With the right container selected the existing functions will be loaded. Besides that you can also create a new one.    With the function selected you can add the input payload for the function.    ","categories": ["Azure","Development"],
        "tags": ["Azure","Azure Functions","Logic Apps"],
        "url": "/2016/04/connect-logic-apps-azure-functions/",
        "teaser": null
      },{
        "title": "Some facts on Azure Functions",
        "excerpt":"A few weeks ago Azure Functions where released during Build 2016. This really wasn’t announced as a big thing. But if you start looking at what you can do with Azure Functions it really is something awesome.  Occurring to Microsoft Azure Functions is the following:  “Azure functions is a solution for easily running small pieces of code, or “functions,” in the cloud. You can write just the code you need for the problem at hand, without worrying about a whole application or the infrastructure to run it. This can make development even more productive, and you can get started with your first function in just minutes.”  For now Azure Functions is hosted in three locations:   West Europe Southeast Asia West US  Managing and creating Azure Functions can be done from two locations:   https://functions.azure.com https://portal.azure.com  The Functions application is completely build upon Azure App Services which means you can use all functions that Azure App Services provide as: Deployment Slots, Web Configuration, Hybrid Connections and Continuous Integration.  Some other key features are:   Choice of language - Write functions using C#, Node.js, Python, F#, PHP, batch, bash, Java, or any executable. Pay-per-use pricing model - Pay only for the time spent running your code. See the Dynamic App Service Plan option in the pricing section below. Bring your own dependencies - Functions supports NuGet and NPM, so you can use your favorite libraries. Integrated security - Protect HTTP-triggered functions with OAuth providers such as Azure Active Directory, Facebook, Google, Twitter, and Microsoft Account. Code-less integration - Easily leverage Azure services and software-as-a-service (SaaS) offerings. See the integrations section below for some examples. Flexible development - Modify your functions with an in-portal editor, or set up continuous integration and deploy your code through GitHub, Visual Studio Team Services, and more. Open-source - Functions is open-source and available on GitHub.  The Azure Functions team is working on the supported languages but for know these are the supported languages:  1st class support    Node / JavaScript C#  Experimental    Python F# PowerShell PHP Bash Batch  By default Azure Functions contains a lot of Triggers, Inputs and Outputs you can use to process information. The below table gives you an idea of what can be used in what kind of situation.     Type Service Trigger Input Output   Schedule Azure Functions ✔     HTTP (REST or webhook) Azure Functions ✔  ✔*   Blob Storage Azure Storage ✔ ✔ ✔   Queues Azure Storage ✔  ✔   Tables Azure Storage  ✔ ✔   Tables Mobile Apps Easy Tables  ✔ ✔   No-SQL DB Azure DocumentDB  ✔ ✔   Push Notifications Azure Notification Hubs   ✔    What you maybe are wondering is, can I use NuGet packages or maybe custom assemblies? Yes you can! NuGet packages can be added by adjusting the project.json file and uploading it trough the KUDU UI or FTP. Besides that you can also reference your own assemblies. If you need to reference a private assembly, you can upload the assembly file into a bin folder relative to your function and reference it by using the file name.  #r \"MyAssembly.dll\"  More information on working with “External Assemblies” can be found here:  https://azure.microsoft.com/en-us/documentation/articles/functions-reference-csharp/#referencing-external-assemblies  On of the things I found when working with Azure Functions that get triggered by a scheduler you will have to use Cron Expressions:  http://www.quartz-scheduler.org/documentation/quartz-2.x/tutorials/tutorial-lesson-06.html  Cron expressions is another kind of time expression that Azure Functions uses. In time a UI will be created to set the scheduler but for know you will have to use this format:  Run every 5 seconds: 0/5 * * * * *  The best thing of Azure Functions is that you do not have to implement logic for getting and saving the information. You only have to implement the logic for the actions you want to perform. If you for example want to resize images you can build a Azure Function that is triggered by Azure Blob Storage, and outputs the resized images to Azure Blob Storage. The implementation you need to make is only the logic for resizing the images.  Besides all these great features it can also use Dynamic Compute what means it will use up no resources when it isn’t triggered or running and it will scale up when the functions needs to.  ","categories": ["Azure"],
        "tags": ["Azure","Azure Functions"],
        "url": "/2016/05/facts-azure-functions/",
        "teaser": null
      },{
        "title": "Azure Functions and Microsoft Cognitive Services",
        "excerpt":"Microsoft is releasing services in very fast pace. This means you really need to keep up with everything. A few weeks back I read about Azure Functions (explained in this blog post) and Microsoft Cognitive services.  Cognitive Services are services that developers can use to easily add intelligent features – such as emotion and sentiment detection, vision and speech recognition, knowledge, search and language understanding – into their applications.  With these services you can now easily build intelligent applications. Microsoft offers API’s in the following categories:   Vision Speech Language Knowledge Search  All these categories contain a lot of services if you want to see the full list of API’s click on the following URL:   https://www.microsoft.com/cognitive-services/en-us/apis  With some services it is possible to retrieve information about images that are uploaded on your website. The service can be that intelligent to recognize what is on the image.  This can be handy in combination with for example Azure Functions. Azure Functions can be triggered by Blob (Image) and the function can then process the image and save what is on that image. This information can then be saved to a storage location of your choosing.  To implement this you will have to Subscribe for Cognitive services. To subscribe go to this URL: https://www.microsoft.com/cognitive-services/ and click on “Get Started for Free”.    You will be redirected to a information page, read the information on the page and click “Let’s go” and sign-in with your Live ID. You will be redirected again but this time to a page with your subscriptions. On this page you can retrieve and regenerate the keys that are needed for doing requests to the Cognitive services of your current subscription.  To get a new subscription click “Request new trials”. Select the services you want to try out. In this example we will use the “Computer Vision – Preview”.  Back on the subscription page click on “Show” to show one of your keys and save it for later because we will need it to implement the http call.    Using the Cognitive services is basically doing a HTTP Post to a specific URL and supplying your key and image in the post. Documentation on how to use the different services work can also be found on the Cognitive Service site under “Documentation” (https://www.microsoft.com/cognitive-services/en-us/documentation).  With the subscription in place we can start with the Azure Function. Within your Azure Functions container create a new Azure Function and choose the BlobTrigger.    Fill in the correct information and click on “Create”:    The Azure function will be created and you will be redirected to the development tab. On this tab add the following references. We will need these reference to create the Http Call and analyze the image.  #r \"System.Web\" using System.IO; using System.Net.Http; using System.Net.Http.Headers; using System.Web;  The next step is to adjust the default Run method to have a Stream input parameter instead of the string input.  public static void Run(Stream myBlob, TraceWriter log) {     //get byte array of the stream     byte[] image = ReadStream(myBlob);      //analyze image     string imageInfo = AnalyzeImage(image);      //write to the console window     log.Info(imageInfo); }  The we can create the method for analyzing the image.  private static string AnalyzeImage(byte[] fileLocation) {     var client = new HttpClient();     var queryString = HttpUtility.ParseQueryString(string.Empty);      client.DefaultRequestHeaders.Add(\"Ocp-Apim-Subscription-Key\", \"{subscription key}\");      queryString[\"maxCandidates\"] = \"1\";     var uri = \"https://api.projectoxford.ai/vision/v1.0/describe?\" + queryString;     HttpResponseMessage response;      using (var content = new ByteArrayContent(fileLocation)) {         content.Headers.ContentType = new MediaTypeHeaderValue(\"application/octet-stream\");         response = client.PostAsync(uri, content).Result;          string imageInfo = response.Content.ReadAsStringAsync().Result;          return imageInfo;     } }  In the method we create a Http Post to the Cognitive service URL: https://api.projectoxford.ai/vision/v1.0/describe? we also add a query string named “maxCandidates” that tells the service the maximum number of candidate descriptions to be returned. The default is value of this parameter is 1.  In the header value “Ocp-Apim-Subscription-Key” the subscription key is placed. The file we would like to analyze is added to the post as a Byte Array.  With this function we can start creating a new Azure Function.  The only method we still miss is a method to convert the stream into a byte array that is necessary to add in the HTTP Post (ReadStream).  public static byte[] ReadFully(Stream input) {     byte[] buffer = new byte[16 * 1024];     using (MemoryStream ms = new MemoryStream()) {         int read;         while ((read = input.Read(buffer, 0, buffer.Length)) &gt; 0) {             ms.Write(buffer, 0, read);         }         return ms.ToArray();     } }  With all these methods in place the function can be tested by uploading a image into the blob account.  In my test scenario’s I use a test image from Microsoft.    Analyzing this image will give the following result.  {   \"description\": {     \"tags\": [       \"grass\",       \"dog\",       \"outdoor\",       \"frisbee\",       \"animal\",       \"sitting\",       \"small\",       \"brown\",       \"field\",       \"laying\",       \"orange\",       \"white\",       \"yellow\",       \"green\",       \"mouth\",       \"playing\",       \"red\",       \"holding\",       \"park\",       \"blue\",       \"grassy\"     ],     \"captions\": [       {         \"text\": \"a dog sitting in the grass with a frisbee\",         \"confidence\": 0.66584959582698122       }     ]   },   \"requestId\": \"38e74ca2-b114-4ebb-b74d-457cc9a2adc2\",   \"metadata\": {     \"width\": 300,     \"height\": 267,     \"format\": \"Jpeg\" ","categories": ["Azure"],
        "tags": ["Azure","Azure Functions","Cognitive Service"],
        "url": "/2016/05/azure-functions-microsoft-cognitive-services/",
        "teaser": null
      },{
        "title": "Loving the Application Map in Application Insights",
        "excerpt":"After being at Build 2016 and hearing about the Application Map functionality in Application Insights, I did not had time to explore this functionality.  The Application Map automatically discovers your application topology in Azure and lays the performance information on top of it. It allows you to discover application dependencies on Azure Services. You can triage the problem by understanding if it is code related or dependency related and from a single place drill into related diagnostics experience.  All of this makes it easy to find a problem within your application. It will show you the errors on the specific component in your topology. This allows you to answer the following questions very quickly:   Is for your performance degrading on the SQL tier? Are the problems occurring in your front-end tier or service tier.  You can find the Application Map within you Application Insights application by clicking on “Settings and Diagnostics” and then on “Application Map” under “Investigate”.  &nbsp;    ","categories": ["Azure"],
        "tags": ["Application Insights","Application Map","Azure"],
        "url": "/2016/05/loving-application-map-application-insights/",
        "teaser": null
      },{
        "title": "Pass trough all SharePoint items from the &ldquo;Get Items&rdquo; action in Azure Logic Apps",
        "excerpt":"When constructing a Logic Apps that gets information from a SharePoint list you will use the SharePoint “Get Items” action. This action allows you to get items from a SharePoint list you connect the action to. You can then process the items by using for example an Azure Function. Constructing a flow this will give you the following Logic App.    By default Logic Apps will construct a foreach loop after the “Get Items” action that you cannot see in the designer view. By opening the code view you will see it (It will only appear if you added a value of the “Get Items” action to for example the input of the Azure Function).  {     \"$schema\": \"https://schema.management.azure.com/providers/Microsoft.Logic/schemas/2015-08-01-preview/workflowdefinition.json#\",     \"actions\": {         \"Get_items\": {             \"conditions\": [],             \"inputs\": {                 \"host\": {                     \"api\": {                         \"runtimeUrl\": \"https://logic-apis-westeurope.azure-apim.net/apim/sharepointonline\"                     },                     \"connection\": {                         \"name\": \"@parameters('$connections')['sharepointonline_1']['connectionId']\"                     }                 },                 \"method\": \"get\",                 \"path\": \"\"             },             \"type\": \"apiconnection\"         },         \"SharePointProcessing\": {             \"conditions\": [                 {                     \"dependsOn\": \"Get_items\"                 }             ],             \"foreach\": \"@body('Get_items')['value']\",             \"inputs\": {                 \"body\": \"@item()['Status']\",                 \"function\": {                     \"id\": \"\"                 }             },             \"type\": \"Function\"         }     },     \"contentVersion\": \"1.0.0.0\",     \"outputs\": {},     \"parameters\": {         \"$connections\": {             \"defaultValue\": {},             \"type\": \"Object\"         }     },     \"triggers\": {         \"recurrence\": {             \"recurrence\": {                 \"frequency\": \"Hour\",                 \"interval\": 1             },             \"type\": \"Recurrence\"         }     } }  This means that the Azure Function will be called for every item that is retrieved by the “Get Items” action meaning that if the action retrieves 100 items the function will be started 100 times.  In a Logic App a colleague (Andre de Ruiter) of mine was constructing he wanted to process all the items at ones in a Azure Function. By default this isn’t possible because the Logic Apps designer only gives you the option to add specific fields into the input value.    As you can see in the above screenshots the action retrieved all the fields available on the SharePoint item and does not give a option to add the complete body of the “Get Items” action.  If you want to send the complete body trough you will have to use de code view. The easiest way is to add one output from the “Get Items” activity into the input of your action and then click on “Code View”.  Within this view search for the Azure Function or the other action that is being called. You will notice that this action contains the foreach object.  \"SharePointProcessing\": {     \"conditions\": [         {             \"dependsOn\": \"Get_items\"         }     ],     \"foreach\": \"@body('Get_items')['value']\",     \"inputs\": {         \"body\": \"@item()['Deleted']\",         \"function\": {             \"id\": \"\"         }     },     \"type\": \"Function\" }  Remove the line with the foreach and in the “inputs” object change the body value to: “@body(‘Get_items’)”. Your action will know look as followed.  \"SharePointProcessing\": {     \"conditions\": [         {             \"dependsOn\": \"Get_items\"         }     ],     \"inputs\": {         \"body\": \"@body('Get_items')\",         \"function\": {             \"id\": \"\"         }     },     \"type\": \"Function\" }  Open the “Designer” again and notice that the Action now contains the “body” element of the “Get Items” action.    ","categories": ["Azure"],
        "tags": ["Azure","Logic Apps","SharePoint"],
        "url": "/2016/05/send-trough-all-sharepoint-items-from-the-get-items-action-in-azure-logic-apps/",
        "teaser": null
      },{
        "title": "Azure Active Directory B2C",
        "excerpt":"Azure Active Directory B2C is a feature released a couple of weeks ago in Azure and is still in preview. Azure Active Directory B2C is a consumer identity and access management in the could, some key features of this component are:   Improve connection with your consumers. Pay only for what you use. Scale to a large amount of consumers. Let consumers use their social media accounts. Customizable workflow for consumer interactions.  AAD B2C can be attached to the following Identity providers:   Amazon Google LinkedIn Microsoft Facebook    Setting this up takes just a couple of steps. Doing this requires you to login to the old old Azure Management portal:   https://manage.windowsazure.com  On the bottom of the management portal click on “New”, select “App Services” and then “Active Directory”.    Fill in all the information and make sure, you select the checkbox “This is a B2C directory”. When done press the button and the Active Directory will be created.    When it is created identity providers, policies and applications can be registered. All of these settings need to be done within the new Azure Portal by navigating to the Azure Active Directory Management Blade. You can get to the specific configuration page by using the link in the old portal from the Azure Active Directory dashboard.    &nbsp;  Identity providers can be added to active directory by using the Identity Providers option within the Azure Active Directory Management Blade. How the external Identity providers can be configured is described here: https://azure.microsoft.com/en-us/documentation/articles/active-directory-b2c-setup-msa-app/  Applications are the applications you will be using in combination with the Azure Active Directory B2C. These can be added by going to the configuration page of the new portal. Within the new portal, define a application by clicking on “Applications” and then “Add” give the application a name and specify that it is a “Web App / Web API”. Make sure you specify a redirect URI to make sure the providers knows were to redirect you (this needs to be the URL of your application).  Save the application to start with the definition of the policies. Policies are definitions that contain how certain actions will be performed and what data it will contain. Policies that can be created within the preview are:   Sign-Up Sign-In Sign-Up or Sign-In Profile editing Password reset  For these policies a couple of things can be configured as you can see in the below screenshot.     Identity providers: This specifies which Identity providers can make use of the policy. By default you have only one identity provider: Local Account (Azure Active Directory Account). Sign-up/in/edit attributes: This specifies which attributes will be used for the policy. Application claims: This specifies which attributes are added to the claim for your application. Token, session &amp; SSO config: Configuration for the sign-in policy. Multifactor authentication: Use multifactor authentication. Page UI customization: Use a customized page for your policy.  Create the policies you need for your application, for the sample application that can be found on GitHub (Link at the bottom of the page) you need three policies: Sign-Up, Sign-In and Profile editing.  When the policies are ready test them by opening the policies window and clicking on a specific policy you created.    Click “Run now” and the policy will be run in the context of the application you have selected. If everything works correctly the sample application can be downloaded and configured. The configuration that needs to be adjusted and extended is in the Web.Config file:  &lt;appSettings&gt;     &lt;add key=\"webpages:Version\" value=\"3.0.0.0\" /&gt;     &lt;add key=\"webpages:Enabled\" value=\"false\" /&gt;     &lt;add key=\"ClientValidationEnabled\" value=\"true\" /&gt;     &lt;add key=\"UnobtrusiveJavaScriptEnabled\" value=\"true\" /&gt;     &lt;add key=\"ida:Tenant\" value=\"[Enter the name of your B2C directory, e.g. contoso.onmicrosoft.com]\" /&gt;     &lt;add key=\"ida:ClientId\" value=\"[Enter the Application Id assigned to your app by the Azure portal, e.g.580e250c-8f26-49d0-bee8-1c078add1609]\" /&gt;     &lt;add key=\"ida:AadInstance\" value=\"https://login.microsoftonline.com/{0}{1}{2}\" /&gt;     &lt;add key=\"ida:RedirectUri\" value=\"https://localhost:44316/\" /&gt;     &lt;add key=\"ida:SignUpPolicyId\" value=\"[Enter your sign up policy name, e.g. b2c_1_sign_up]\" /&gt;     &lt;add key=\"ida:SignInPolicyId\" value=\"[Enter your sign in policy name, e.g. b2c_1_sign_in]\" /&gt;     &lt;add key=\"ida:UserProfilePolicyId\" value=\"[Enter your edit profile policy name, e.g. b2c_1_profile_edit]\" /&gt; &lt;/appSettings&gt;   Problems  Azure Active Directory is in preview and that is why you can run into specific problems. One problem that I encountered was the one in the screenshot below:   The provided application with ID ‘’ is not valid against this service. Please use an application created via the B2C portal an try again.    I checked the ID of the application and all looked fine. The problem was that I added the application to the Azure Active Directory B2C in the old Azure portal, the URL constructed for that type of applications are constructed differently (v1.0 vs. v2.0) then the way used in the Azure Portal Blade.  Recreating the application in the Azure Blade fixed the issue.  References:   Quick Start Guide Direct download sample application Microsoft Configure Identity providers  ","categories": ["Azure"],
        "tags": ["Azure Active Directory","B2C"],
        "url": "/2016/06/azure-active-directory-b2c/",
        "teaser": null
      },{
        "title": "Azure App Service and Client Certificate Authentication",
        "excerpt":"Azure App Services can make use of Client Certificate Authentication. The options for this are not available in the portal and need to be configured manually.  Activating Client Certificate Authentication  In the below blog post on the Azure documentation site is explained how you can configure your Azure Web App for client certificate authentication:   How To Configure TLS Mutual Authentication for Web App   The same way can also be used for for example an Azure API App. The below option is a option for setting the same property in a different way.   Open https://resources.azure.com/ and login with the same credentials as the Azure Portal.   Find your site. It will be under subscriptions – [your subscription] – resouceGroups – [your resource group] – providers – Microsoft.Web – sites –[your site]    &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;   Make sure the interface is set on “Read\\Write” mode.      Click the “Edit” button on the op of the screen.     &nbsp;  &nbsp;  &nbsp;   Find the property “clientCertEnabled” and set it to “true”.     &nbsp;  &nbsp;   Click the “PUT” button on top to save your changes.     &nbsp;  &nbsp;  &nbsp;  &nbsp;  With the Azure resource configured you need to make sure that your application is able to use Client Certificate Authentication.  Preparing your Application  Keep in mind that your Azure resource does not validate your Client Certificate! It only requires you to supply a certificate when opening the application. That requires you to write your own logic for validating the certificate. In the blog post mentioned above there is class for validating the certificate. Based on this example I have created a Certificate Authorization Attribute.  /// &lt;summary&gt; /// Certificate Authorization Attribute /// &lt;/summary&gt; /// &lt;seealso cref=\"System.Web.Http.AuthorizeAttribute\" /&gt; [AttributeUsage(AttributeTargets.All)] public class CertificateAuthorizationAttribute : AuthorizeAttribute {      /// &lt;summary&gt;     /// Indicates whether the specified control is authorized.     /// &lt;/summary&gt;     /// &lt;param name=\"actionContext\"&gt;The context.&lt;/param&gt;     /// &lt;returns&gt;     /// true if the control is authorized; otherwise, false.     /// &lt;/returns&gt;     protected override bool IsAuthorized(HttpActionContext actionContext) {         bool retVal = false;          var cert = actionContext.RequestContext.ClientCertificate;          if (cert != null) {             try {                 retVal = IsValidClientCertificate(cert);             } catch {                 //log exception             }         }         return retVal;     }      /// &lt;summary&gt;     /// Determines whether [is valid client certificate] [the specified certificate].     /// &lt;/summary&gt;     /// &lt;param name=\"certificate\"&gt;The certificate.&lt;/param&gt;     /// &lt;returns&gt;boolean&lt;/returns&gt;     private bool IsValidClientCertificate(X509Certificate2 certificate) {         //1. Check time validity of certificate         if (DateTime.Compare(DateTime.Now, certificate.NotBefore) &lt; 0 || DateTime.Compare(DateTime.Now, certificate.NotAfter) &gt; 0) {             return false;         }          //2. Check subject name of certificate         bool foundSubject = false;         string[] certSubjectData = certificate.Subject.Split(new char[] { ',' }, StringSplitOptions.RemoveEmptyEntries);         foreach (string s in certSubjectData) {             if (String.Compare(s.Trim(), \"[Subject Name]\") == 0) {                 foundSubject = true;                 break;             }         }         if (!foundSubject) return false;          //3. Check issuer name of certificate         bool foundIssuerCN = false;         string[] certIssuerData = certificate.Issuer.Split(new char[] { ',' }, StringSplitOptions.RemoveEmptyEntries);         foreach (string s in certIssuerData) {             if (String.Compare(s.Trim(), \"[Issuer]\") == 0) {                 foundIssuerCN = true;                 break;             }         }         if (!foundIssuerCN) return false;          // 4. Check thumbprint of certificate         if (String.Compare(certificate.Thumbprint.Trim().ToUpper(), \"[Thumbprint]\") != 0) return false;          return true;     } }  This attribute makes sure you call the application with a valid certificate. It checks the following properties of the certificate:   The time validity  The subject name  The issuer name  The thumbprint   With the attribute implemented it can be used on a Controller method for example.  [CertificateAuthorization] [Route(\"spitems/{clientId}\")] public HttpResponseMessage Get(string clientId) { }  Make sure you do not forget to add the following xml node to your web.config and call your application with https. If you do not do this your certificate will not be recognized in your request.  &lt;configuration&gt; &lt;!-- other settings --&gt;   &lt;system.webServer&gt;     &lt;security&gt;       &lt;access sslFlags=\"SslNegotiateCert\" /&gt;     &lt;/security&gt;   &lt;/system.webServer&gt; &lt;configuration&gt;  Testing your application  Testing your application can be done by using the HttpClient to perform a request.  private async Task&lt;string&gt; RunAsync() {     string retVal = string.Empty;     var handler = new WebRequestHandler();     handler.ClientCertificateOptions = ClientCertificateOption.Manual;     var cert = GetClientCert();     handler.ClientCertificates.Add(cert);      handler.ServerCertificateValidationCallback = delegate (object sender, X509Certificate certificate, X509Chain chain, SslPolicyErrors error) {         return true;     };     handler.UseProxy = false;      using (var client = new HttpClient(handler)) {         client.BaseAddress = new Uri(\"[Base Address]\");         client.DefaultRequestHeaders.Accept.Clear(); ","categories": ["Azure"],
        "tags": ["Azure","Azure App Services","Certificate"],
        "url": "/2016/06/azure-app-service-and-client-certificate-authentication/",
        "teaser": null
      },{
        "title": "Starting with Azure Resource Templates",
        "excerpt":"Since that Azure uses the Azure Resource Manager you have the ability to setup your own templates for deploying your applications. This can be handy because the infrastructure for your application is typically made up of many components – maybe a virtual machine, storage account, and virtual network, or a web app, database, database server, and 3rd party services. You do not see these components as separate entities, instead you see them as related and interdependent parts of a single entity. These components can be deployed and managed as a group using the Azure Resource Manager.&nbsp;   With the Resource Manager, you can create a simple template (in JSON format) that defines deployment and configuration of your application. This template is known as a Resource Manager template and provides a declarative way to define deployment. By using a template, you can repeatedly deploy your application throughout the app lifecycle and have confidence your resources are deployed in a consistent state.   Creating the Azure Resource Template  To start with a Azure Resource Template a new project of the template “Azure Resource Group” needs to be added to your Visual Studio Solution.     When clicking “Ok” a dialog will appear that allows you to pick a kind of resource for deployment. When the resource is chosen the project will be created. This will give a project with three folders:   Scripts: Folder for PowerShell scripts that are needed for the deployment. By default it will contain a default script for deploying your resource.   Templates: Folder for the Azure Resource Manager templates. By default it will contain the Resource Template it self and a parameters file.   Tools: Folder that contains tools that are needed for the deployment. By default it contains AzCopy.exe  Editing the Resource Template  Within Visual Studio there is a text editor for editing the resource template that basically is a large JSON file. This combined with the JSON Outline viewer gives you the flexibility to do many actions. Which actions you can perform depend on the API versions you use within the Resource Template and what actions are available. It could be possible that JSON Outline viewer does not container actions that the ARM API supports. When this occurs the actions need to be added manually.   From the JSON Outline viewer you can add additional resources to your template by clicking the right mouse button on the resources node.    Adding resources can also be done on another level. This will give you different options for example adding a a Web Deploy or Application Settings element to a Web Application.     Deploying a Azure API Application  Most of the time when building a Azure API Application it will not only consists of Azure App Services but also Application Insights and a Azure Storage account for example. To deploy these resources all at once the Azure Resource Manager is the way to go. But to get there you will have to keep a couple things in mind.   To help you with building templates make use of the Azure Resource Explorer:   Azure Resource Explorer  In the Azure Resource Explorer you will find all your resources you have created within the portal all in JSON representations.   Setting the Web App Type  When selecting a resource for the ARM template there is no option for a API application. The way to create a API Application is by adding a “Web App” resource and setting a specific property to “api” within the template. This property is called “kind”.  \"apiVersion\": \"2015-08-01\", \"name\": \"[parameters('apiName')]\", \"kind\" :  \"api\", \"type\": \"Microsoft.Web/sites\", \"location\": \"[resourceGroup().location]\", \"tags\": {   \"[concat('hidden-related:', resourceGroup().id, '/providers/Microsoft.Web/serverfarms/', parameters('hostingPlanName'))]\": \"Resource\",   \"displayName\": \"Website\" },  Adding application settings  When developing web applications there will be a need for saving settings within the web.config. These settings can be overridden within Azure via the Application Settings blade. Example for settings are:  AlwaysOn en Client Certificates (not possible trough the interface)  \"properties\": {   \"name\": \"[parameters('apiName')]\",   \"serverFarmId\": \"[resourceId('Microsoft.Web/serverfarms', parameters('hostingPlanName'))]\",   \"clientCertEnabled\": true,   \"siteConfig\": {     \"AlwaysOn\" :  true   } }  Many other settings are depended on other resources like: Application Insights and Azure Storage Accounts. To reference specific values from other resources you can use a specific string concatenation but the resource needs to be created first. When the template is setup in the correct way Azure Resource Manager will handle this dependencies to other resources.    Storage Account Connection String  The storage account connection string can be created with the default endpoint and a key from the Storage Account. These properties can be retrieved and set as connection string by using the below JSON.  \"properties\": {   \"StorageConnectionString\": \"[concat('DefaultEndpointsProtocol=https;AccountName=',variables('storageAccountName'),';AccountKey=',listKeys(variables('storageAccountName'),'2016-01-01').keys[0].value)]\" }  Application Insights Instrumentation Key  When making use of application insights the application has to know the instrumentation key in order to log the resources. An option for this is to register the instrumentation key in the web.config and register is it in the Application_Start event as you can read here: Application Insights configurable instrumentation key.  The instrumentation key is added to the web.config by setting the correct reference.  \"properties\": {   \"InstrumentationKey\": \"[reference(concat('Microsoft.Insights/components/', parameters('insightsName'))).InstrumentationKey]\" }  Deploying your Web Application  With Azure Resource Management templates there is also the option to deploy your application. This can be added to the template by adding the project you created as a reference and adding a “Web Deploy for Web Apps” resource.  To complete this option the PowerShell script and the template need to be altered. What you need to understand is that the zip file containing your sources has to be present within Azure in order to be able to deploy it to your web application. So you have to create a Storage account within Azure and add your zip package to the blob container.   With the package uploaded you need to add the ‘_artifactsLocation’ and ‘_artifactsLocationSasToken’ parameters to the resource provider in order for it to know where the deployment package is saved. I also altered the default PowerShell to append the date and time to the package name in order to save old deployment packages within the storage container.  The file name is specified in the ‘_packageFileName’ parameter that is also send to the resource provider by the use of the OptionalParameters. The complete files can be viewed in the following sections.  PowerShell deployment file  #Requires -Version 3.0 #Requires -Module AzureRM.Resources #Requires -Module Azure.Storage  Param(     [string] $ResourceGroupLocation = '[Location]',     [string] $ResourceGroupName = '[Resource Group Name',     [string] $WebApiPackage = '[Full path to pacakge]',     [string] $TemplateFile = '[Path to the template]',     [string] $TemplateParametersFile = 'Path to the parameters file]',     [string] $DeployContainer = 'deployment'  )  Import-Module Azure -ErrorAction SilentlyContinue Set-StrictMode -Version 3     #Login to the Azure Resource Management Account Login-AzureRmAccount  #Construct Azure Resource Manager parameters $OptionalParameters = New-Object -TypeName Hashtable $DateFormat = Get-Date -Format \"yyyyMMdd-HHmm\" $FileName = \"package-\" + $dateFormat + \".zip\" $TemplateFile = [System.IO.Path]::Combine($PSScriptRoot, $TemplateFile) $TemplateParametersFile = [System.IO.Path]::Combine($PSScriptRoot, $TemplateParametersFile)  #Construct deployment parameters $DeployStorageAccountName = $ResourceGroupName.ToLowerInvariant() + 'deployment' $DeployStorageAccountName = $DeployStorageAccountName -replace '-'   #Create Azure Resource Group Write-Host \"Creating Azure Resource Group: \" $ResourceGroupName \" in \" $ResourceGroupLocation -ForegroundColor Green New-AzureRmResourceGroup -Name $ResourceGroupName -Location $ResourceGroupLocation -Force -ErrorAction Stop   #Create Azure Storage Account for the deployment of the resources Write-Host \"Creating Azure Storage Account in Resource Group : \" $ResourceGroupName \" named \" $DeployStorageAccountName -ForegroundColor Green New-AzureRmStorageAccount -ResourceGroupName $ResourceGroupName -Name $DeployStorageAccountName -Type \"Standard_LRS\" -Location $ResourceGroupLocation  #Create storage container for the deployment files Write-Host \"Creating storage container for the deployment files named: \"  $DeployContainer -ForegroundColor Green $StorageAccountContext = (Get-AzureRmStorageAccount | Where-Object{$_.StorageAccountName -eq $DeployStorageAccountName }).Context $StorageContainer = Get-AzureStorageContainer -Name $DeployContainer -Context $StorageAccountContext -ErrorAction SilentlyContinue if($StorageContainer -eq $null){     New-AzureStorageContainer -Name $DeployContainer -Context $StorageAccountContext }  #Adding the package to the container Write-Host \"Adding the deployment package to the container with the name: \" $FileName -ForegroundColor Green Set-AzureStorageBlobContent -File $WebApiPackage -Blob $FileName -Container 'deployment' -Context $StorageAccountContext -Force  #Setting the optional parameters for the deployment file $ArtifactsLocation = $StorageAccountContext.BlobEndPoint + $DeployContainer $ArtifactsLocationSasToken = New-AzureStorageContainerSASToken -Container $DeployContainer -Context $StorageAccountContext -Permission r -ExpiryTime (Get-Date).AddHours(4) $ArtifactsLocationSasToken = ConvertTo-SecureString $ArtifactsLocationSasToken -AsPlainText -Force  $OptionalParameters = New-Object -TypeName Hashtable $OptionalParameters['_artifactsLocation'] = $ArtifactsLocation $OptionalParameters['_artifactsLocationSasToken'] = $ArtifactsLocationSasToken $OptionalParameters['_packageFileName'] = $FileName  #Deploying resources with the resource management file Write-Host \"Deploying resources to the Resource Group: \" $ResourceGroupName -ForegroundColor Green New-AzureRmResourceGroupDeployment -Name ((Get-ChildItem $TemplateFile).BaseName + '-' + ((Get-Date).ToUniversalTime()).ToString('MMdd-HHmm')) `                                    -ResourceGroupName $ResourceGroupName `                                    -TemplateFile $TemplateFile `                                    -TemplateParameterFile $TemplateParametersFile `                                    @OptionalParameters `                                    -Force   Parameters File  {     \"$schema\": \"http://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\",     \"contentVersion\": \"1.0.0.0\",   \"parameters\": {     \"hostingPlanName\": { \"value\": \"[Service Plan Name]\" },     \"skuName\": { \"value\": \"[Resource Level]\" },     \"skuCapacity\": { \"value\": [Capacity] },     \"resourceLocation\": { \"value\": \"[Resource Location]\" },     \"storageName\": { \"value\": \"[Storage Name]\" },     \"storageType\": { \"value\": \"[Storage Type]\" },     \"apiName\": { \"value\": \"[API Application Name]\" },     \"insightsName\": { \"value\": \"[Insights Name]\" },   } }  Azure Resource Template  {   \"$schema\": \"http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",   \"contentVersion\": \"1.0.0.0\",   \"parameters\": {     \"hostingPlanName\": {       \"type\": \"string\",       \"minLength\": 1     },     \"resourceLocation\": {       \"type\": \"string\",       \"defaultValue\": \"West Europe\"     },     \"skuName\": {       \"type\": \"string\",       \"defaultValue\": \"F1\",       \"allowedValues\": [ \"F1\", \"D1\", \"B1\", \"B2\", \"B3\", \"S1\", \"S2\", \"S3\", \"P1\", \"P2\", \"P3\", \"P4\" ]     },     \"skuCapacity\": {       \"type\": \"int\",       \"defaultValue\": 1,       \"minValue\": 1     },     \"storageType\": {       \"type\": \"string\",       \"defaultValue\": \"Standard_LRS\",       \"allowedValues\": [ \"Standard_LRS\", \"Standard_ZRS\", \"Standard_GRS\", \"Standard_RAGRS\", \"Premium_LRS\" ]     },     \"storageName\": {       \"type\": \"string\",       \"minLength\": 1     },     \"apiName\": {       \"type\": \"string\",       \"minLength\": 1     },     \"insightsName\": {       \"type\": \"string\",       \"minLength\": 1     },     \"_artifactsLocation\": {       \"type\": \"string\"     },     \"_artifactsLocationSasToken\": {       \"type\": \"securestring\"     },     \"_packageFileName\": {       \"type\": \"string\"     }   },   \"variables\": {     \"storageAccountName\": \"[parameters('storageName')]\",     \"storageid\": \"[resourceId('Microsoft.Storage/storageAccounts',variables('storageAccountName'))]\"   },   \"resources\": [     {       \"apiVersion\": \"2015-08-01\",       \"name\": \"[parameters('hostingPlanName')]\",       \"type\": \"Microsoft.Web/serverfarms\",       \"location\": \"[parameters('resourceLocation')]\",       \"tags\": {         \"displayName\": \"HostingPlan\"       },       \"sku\": {         \"name\": \"[parameters('skuName')]\",         \"capacity\": \"[parameters('skuCapacity')]\"       },       \"properties\": {         \"name\": \"[parameters('hostingPlanName')]\"       }     },     {       \"apiVersion\": \"2015-08-01\",       \"name\": \"[parameters('apiName')]\",       \"kind\" :  \"api\",       \"type\": \"Microsoft.Web/sites\",       \"location\": \"[resourceGroup().location]\",       \"tags\": {         \"[concat('hidden-related:', resourceGroup().id, '/providers/Microsoft.Web/serverfarms/', parameters('hostingPlanName'))]\": \"Resource\",         \"displayName\": \"Website\"       },       \"dependsOn\": [         \"[concat('Microsoft.Web/serverfarms/', parameters('hostingPlanName'))]\"       ],       \"properties\": {         \"name\": \"[parameters('apiName')]\",         \"serverFarmId\": \"[resourceId('Microsoft.Web/serverfarms', parameters('hostingPlanName'))]\",         \"clientCertEnabled\": true,         \"siteConfig\": {           \"AlwaysOn\" :  true         }       },       \"resources\": [         {           \"name\": \"appsettings\",           \"type\": \"config\",           \"apiVersion\": \"2015-08-01\",           \"dependsOn\": [             \"[concat('Microsoft.Web/sites/', parameters('apiName'))]\",             \"[concat('Microsoft.Storage/storageAccounts/', variables('storageAccountName'))]\"           ],           \"tags\": {             \"displayName\": \"AppSettings\"           },           \"properties\": {             \"StorageConnectionString\": \"[concat('DefaultEndpointsProtocol=https;AccountName=',variables('storageAccountName'),';AccountKey=',listKeys(variables('storageAccountName'),'2016-01-01').keys[0].value)]\",             \"InstrumentationKey\": \"[reference(concat('Microsoft.Insights/components/', parameters('insightsName'))).InstrumentationKey]\"           }         },           {               \"name\": \"MSDeploy\",               \"type\": \"extensions\",               \"location\": \"[resourceGroup().location]\",               \"apiVersion\": \"2015-08-01\",               \"dependsOn\": [                   \"[concat('Microsoft.Web/sites/', parameters('apiName'))]\"               ],               \"tags\": {                   \"displayName\": \"Deploy\"               },             \"properties\": {               \"packageUri\": \"[concat(parameters('_artifactsLocation'), '/', parameters('_packageFileName'), parameters('_artifactsLocationSasToken'))]\",               \"dbType\": \"None\",               \"connectionString\": \"\",               \"setParameters\": {                 \"IIS Web Application Name\": \"[parameters('apiName')]\"               }             }           }       ]     },     {       \"apiVersion\": \"2014-04-01\",       \"name\": \"[concat(parameters('hostingPlanName'), '-', resourceGroup().name)]\",       \"type\": \"Microsoft.Insights/autoscalesettings\",       \"location\": \"[resourceGroup().location]\",       \"tags\": {         \"[concat('hidden-link:', resourceGroup().id, '/providers/Microsoft.Web/serverfarms/', parameters('hostingPlanName'))]\": \"Resource\",         \"displayName\": \"AutoScaleSettings\"       },       \"dependsOn\": [         \"[concat('Microsoft.Web/serverfarms/', parameters('hostingPlanName'))]\"       ],       \"properties\": {         \"profiles\": [           {             \"name\": \"Default\",             \"capacity\": {               \"minimum\": 1,               \"maximum\": 2,               \"default\": 1             },             \"rules\": [               {                 \"metricTrigger\": {                   \"metricName\": \"CpuPercentage\",                   \"metricResourceUri\": \"[concat(resourceGroup().id, '/providers/Microsoft.Web/serverfarms/', parameters('hostingPlanName'))]\",                   \"timeGrain\": \"PT1M\",                   \"statistic\": \"Average\",                   \"timeWindow\": \"PT10M\",                   \"timeAggregation\": \"Average\",                   \"operator\": \"GreaterThan\",                   \"threshold\": 80.0                 },                 \"scaleAction\": {                   \"direction\": \"Increase\",                   \"type\": \"ChangeCount\",                   \"value\": 1,                   \"cooldown\": \"PT10M\"                 }               },               {                 \"metricTrigger\": {                   \"metricName\": \"CpuPercentage\",                   \"metricResourceUri\": \"[concat(resourceGroup().id, '/providers/Microsoft.Web/serverfarms/', parameters('hostingPlanName'))]\",                   \"timeGrain\": \"PT1M\",                   \"statistic\": \"Average\",                   \"timeWindow\": \"PT1H\",                   \"timeAggregation\": \"Average\",                   \"operator\": \"LessThan\",                   \"threshold\": 60.0                 },                 \"scaleAction\": {                   \"direction\": \"Decrease\",                   \"type\": \"ChangeCount\",                   \"value\": 1,                   \"cooldown\": \"PT1H\"                 }               }             ]           }         ],         \"enabled\": false,         \"name\": \"[concat(parameters('hostingPlanName'), '-', resourceGroup().name)]\",         \"targetResourceUri\": \"[concat(resourceGroup().id, '/providers/Microsoft.Web/serverfarms/', parameters('hostingPlanName'))]\"       }     },     {       \"apiVersion\": \"2014-04-01\",       \"name\": \"[concat('ServerErrors ', parameters('apiName'))]\",       \"type\": \"Microsoft.Insights/alertrules\",       \"location\": \"[resourceGroup().location]\",       \"dependsOn\": [         \"[concat('Microsoft.Web/sites/', parameters('apiName'))]\"       ],       \"tags\": {         \"[concat('hidden-link:', resourceGroup().id, '/providers/Microsoft.Web/sites/', parameters('apiName'))]\": \"Resource\",         \"displayName\": \"ServerErrorsAlertRule\"       },       \"properties\": {         \"name\": \"[concat('ServerErrors ', parameters('apiName'))]\",         \"description\": \"[concat(parameters('apiName'), ' has some server errors, status code 5xx.')]\",         \"isEnabled\": false,         \"condition\": {           \"odata.type\": \"Microsoft.Azure.Management.Insights.Models.ThresholdRuleCondition\",           \"dataSource\": {             \"odata.type\": \"Microsoft.Azure.Management.Insights.Models.RuleMetricDataSource\",             \"resourceUri\": \"[concat(resourceGroup().id, '/providers/Microsoft.Web/sites/', parameters('apiName'))]\",             \"metricName\": \"Http5xx\"           },           \"operator\": \"GreaterThan\",           \"threshold\": 0.0,           \"windowSize\": \"PT5M\"         },         \"action\": {           \"odata.type\": \"Microsoft.Azure.Management.Insights.Models.RuleEmailAction\",           \"sendToServiceOwners\": true,           \"customEmails\": [ ]         }       }     },     {       \"apiVersion\": \"2014-04-01\",       \"name\": \"[concat('ForbiddenRequests ', parameters('apiName'))]\",       \"type\": \"Microsoft.Insights/alertrules\",       \"location\": \"[resourceGroup().location]\",       \"dependsOn\": [         \"[concat('Microsoft.Web/sites/', parameters('apiName'))]\"       ],       \"tags\": {         \"[concat('hidden-link:', resourceGroup().id, '/providers/Microsoft.Web/sites/', parameters('apiName'))]\": \"Resource\",         \"displayName\": \"ForbiddenRequestsAlertRule\"       },       \"properties\": {         \"name\": \"[concat('ForbiddenRequests ', parameters('apiName'))]\",         \"description\": \"[concat(parameters('apiName'), ' has some requests that are forbidden, status code 403.')]\",         \"isEnabled\": false,         \"condition\": {           \"odata.type\": \"Microsoft.Azure.Management.Insights.Models.ThresholdRuleCondition\",           \"dataSource\": {             \"odata.type\": \"Microsoft.Azure.Management.Insights.Models.RuleMetricDataSource\",             \"resourceUri\": \"[concat(resourceGroup().id, '/providers/Microsoft.Web/sites/', parameters('apiName'))]\",             \"metricName\": \"Http403\"           },           \"operator\": \"GreaterThan\",           \"threshold\": 0,           \"windowSize\": \"PT5M\"         },         \"action\": {           \"odata.type\": \"Microsoft.Azure.Management.Insights.Models.RuleEmailAction\",           \"sendToServiceOwners\": true,           \"customEmails\": [ ]         }       }     },     {       \"apiVersion\": \"2014-04-01\",       \"name\": \"[concat('CPUHigh ', parameters('hostingPlanName'))]\",       \"type\": \"Microsoft.Insights/alertrules\",       \"location\": \"[resourceGroup().location]\",       \"dependsOn\": [         \"[concat('Microsoft.Web/serverfarms/', parameters('hostingPlanName'))]\"       ],       \"tags\": {         \"[concat('hidden-link:', resourceGroup().id, '/providers/Microsoft.Web/serverfarms/', parameters('hostingPlanName'))]\": \"Resource\",         \"displayName\": \"CPUHighAlertRule\"       },       \"properties\": {         \"name\": \"[concat('CPUHigh ', parameters('hostingPlanName'))]\",         \"description\": \"[concat('The average CPU is high across all the instances of ', parameters('hostingPlanName'))]\",         \"isEnabled\": false,         \"condition\": {           \"odata.type\": \"Microsoft.Azure.Management.Insights.Models.ThresholdRuleCondition\",           \"dataSource\": {             \"odata.type\": \"Microsoft.Azure.Management.Insights.Models.RuleMetricDataSource\",             \"resourceUri\": \"[concat(resourceGroup().id, '/providers/Microsoft.Web/serverfarms/', parameters('hostingPlanName'))]\",             \"metricName\": \"CpuPercentage\"           },           \"operator\": \"GreaterThan\",           \"threshold\": 90,           \"windowSize\": \"PT15M\"         },         \"action\": {           \"odata.type\": \"Microsoft.Azure.Management.Insights.Models.RuleEmailAction\",           \"sendToServiceOwners\": true,           \"customEmails\": [ ]         }       }     },     {       \"apiVersion\": \"2014-04-01\",       \"name\": \"[concat('LongHttpQueue ', parameters('hostingPlanName'))]\",       \"type\": \"Microsoft.Insights/alertrules\",       \"location\": \"[resourceGroup().location]\",       \"dependsOn\": [         \"[concat('Microsoft.Web/serverfarms/', parameters('hostingPlanName'))]\"       ],       \"tags\": {         \"[concat('hidden-link:', resourceGroup().id, '/providers/Microsoft.Web/serverfarms/', parameters('hostingPlanName'))]\": \"Resource\",         \"displayName\": \"LongHttpQueueAlertRule\"       },       \"properties\": {         \"name\": \"[concat('LongHttpQueue ', parameters('hostingPlanName'))]\",         \"description\": \"[concat('The HTTP queue for the instances of ', parameters('hostingPlanName'), ' has a large number of pending requests.')]\",         \"isEnabled\": false,         \"condition\": {           \"odata.type\": \"Microsoft.Azure.Management.Insights.Models.ThresholdRuleCondition\",           \"dataSource\": {             \"odata.type\": \"Microsoft.Azure.Management.Insights.Models.RuleMetricDataSource\",             \"resourceUri\": \"[concat(resourceGroup().id, '/providers/Microsoft.Web/serverfarms/', parameters('hostingPlanName'))]\",             \"metricName\": \"HttpQueueLength\"           },           \"operator\": \"GreaterThan\",           \"threshold\": 100.0,           \"windowSize\": \"PT5M\"         },         \"action\": {           \"odata.type\": \"Microsoft.Azure.Management.Insights.Models.RuleEmailAction\",           \"sendToServiceOwners\": true,           \"customEmails\": [ ]         }       }     },     {       \"apiVersion\": \"2014-04-01\",       \"name\": \"[parameters('insightsName')]\",       \"type\": \"Microsoft.Insights/components\",       \"location\": \"Central US\",       \"dependsOn\": [         \"[concat('Microsoft.Web/sites/', parameters('apiName'))]\"       ],       \"tags\": {         \"[concat('hidden-link:', resourceGroup().id, '/providers/Microsoft.Web/sites/', parameters('apiName'))]\": \"Resource\",         \"displayName\": \"AppInsightsComponent\"       },       \"properties\": {         \"applicationId\": \"[parameters('apiName')]\"       }     },     {       \"name\": \"[variables('storageAccountName')]\",       \"type\": \"Microsoft.Storage/storageAccounts\", ","categories": ["Azure"],
        "tags": ["ARM","Azure","Deployment","JSON"],
        "url": "/2016/06/starting-azure-resource-templates/",
        "teaser": null
      },{
        "title": "SharePoint Rest API Handler",
        "excerpt":"SharePoint contains a lot of Rest API’s that can be used for many scenario’s. You could use them for example in desktop and windows phone applications. When using these API’s you need to make sure authentication is handled before calling the API. For authenticating to these API’s there a couple of options:   Authenticate as a User. This is a registered user with a license within your Office 365 tenant or your on-premise AD.   Authenticate as a Application. You can register a application within an SharePoint site by using the “AppRegNew.aspx” page and perform actions on behalf of a application.  Register SharePoint Add-ins 2013     The token that is received from the authentication request needs to be supplied in the header of the API call. In order to retrieve the token you can make use of the “TokenHelper” class that is added by default to SharePoint App projects.  With the token the API call can be made, this needs do be done in different ways. In the following paragraphs there are examples for Getting Items, Uploading documents, Updating, Getting documents by Id.  Getting Items from a SharePoint List  Getting items from a SharePoint list can be achieved by using a Http Get to the “GetItems” API method and supplying a Caml query in the URL.   [siteurl]/_api/web/lists/GetByTitle('[listname]')/GetItems(query=@v1)?@v1={\"ViewXml\":\"[camlquery]\"}   Example for a Caml query is displayed below. Make sure you supply a query with the “View” and “Query” tag included.  &lt;View&gt;  &lt;ViewFields&gt;   &lt;FieldRef Name='LinkFilename' /&gt;  &lt;/ViewFields&gt;  &lt;Query&gt;   &lt;OrderBy&gt;    &lt;FieldRef Name='Created' /&gt;   &lt;/OrderBy&gt;  &lt;/Query&gt; &lt;/View&gt;  &nbsp;  In C# the API call can be made in the following way:  public string GetItems(string siteUrl, string list, string camlQuery) {     string retVal = string.Empty;      string data = camlJson.Replace(\"{0}\", camlQuery);     camlQuery = \"(query=@v1)?@v1={\\\"ViewXml\\\":\\\"{2}\\\"}\".Replace(\"{2}\", camlQuery);     string url =string.Format(CultureInfo.InvariantCulture, \"{0}/_api/web/lists/GetByTitle('{1}')/GetItems{2}\", siteUrl, list, camlQuery);      SPAPIHandler handler = new SPAPIHandler(ClientId, ClientSecret);     retVal = handler.Post(url);      return retVal; }   Getting a Document by Id  Getting a specific item from a SharePoint list can be achieved by using a Http Get to the “Items” API method and supplying with specific select and filter string.   [siteurl]/_api/web/lists/getByTitle('[listname]')/items?$select=EncodedAbsUrl&amp;$filter=Id eq [Id]   The select query in the above method retrieves the “EncodedAbsUrl” this is the URL to the specific document. Within C# you can use the following method to retrieve the specific document.  public string GetDocument(string siteUrl, string list, string Id) {          string retVal = string.Empty;     string url = string.Format(CultureInfo.InvariantCulture, \"{0}/_api/web/lists/getByTitle('{1}')/items?$select=EncodedAbsUrl&amp;$filter=Id eq {2}\", siteUrl, list, Id);      SPAPIHandler handler = new SPAPIHandler(ClientId, ClientSecret);     retVal = handler.Get(url);      return retVal; }   Uploading a Document  Uploading documents to a SharePoint library can be achieved by using a Http Post to the “Add” API method. When uploading a document it is not possible to also add metadata directly. This needs to be done within a separate call.   [siteurl]/_api/web/lists/getByTitle('[listname]')/RootFolder/Files/Add(url='[filename]', overwrite=true)   The document itself needs to be added to the content of the post message. In C# the complete method looks like this:  public string PostDocument(string siteUrl, string list, Document document) {     string retVal = string.Empty;      string url = string.Format(CultureInfo.InvariantCulture, \"{0}/_api/web/lists/getByTitle('{1}')/RootFolder/Files/Add(url='{2}', overwrite=true)\", siteUrl, list, document.FileName);     SPAPIHandler handler = new SPAPIHandler(ClientId, ClientSecret);     retVal = handler.PostDocument(url, document.DocumentByteArray);      JavaScriptSerializer serializer = new JavaScriptSerializer();     dynamic item = serializer.Deserialize&lt;object&gt;(retVal);     string relativeUrl = item[\"d\"][\"ServerRelativeUrl\"];      UpdateDocument(siteUrl, relativeUrl, document);      return retVal; }  Updating a Document  As mentioned in the above paragraph updating a document is done in a separate Http method that also needs some additional headers.   [siteurl]/_api/web/GetFileByServerRelativeUrl('[file relative url]')/ListItemAllFields   The relative URL of the file can be retrieved from the result of the upload request. This can be seen in the paragraph above where the result message is serialized in a dynamic object and the “ServerRelativeUrl” property is retrieved.  The complete C# method for updating a document is:  public string UpdateDocument(string siteUrl, string relativeUrl, Document document) {     string retVal = string.Empty;          string url = string.Format(CultureInfo.InvariantCulture, \"{0}/_api/web/GetFileByServerRelativeUrl('{1}')/ListItemAllFields\", siteUrl, relativeUrl);     SPAPIHandler handler = new SPAPIHandler(ClientId, ClientSecret);          retVal = handler.Merge(url, document);      return retVal; }    SharePoint API Handler  As you can see in the examples above they all make use of the “SPAPIHandler” class. This class takes care of the HTTP calls and authenticates the requests.  public class SPAPIHandler {      private string contenttype = \"application/json;odata=verbose\";      private string acceptHeader = \"application/json;odata=verbose\";      /// &lt;summary&gt;     /// Gets or sets the client identifier.     /// &lt;/summary&gt;     /// &lt;value&gt;     /// The client identifier.     /// &lt;/value&gt;     private string ClientId { get; set; }      /// &lt;summary&gt;     /// Gets or sets the client secret.     /// &lt;/summary&gt;     /// &lt;value&gt;     /// The client secret.     /// &lt;/value&gt;     private string ClientSecret { get; set; }      public SPAPIHandler(string clientId, string clientSecret) {         ClientId = clientId;         ClientSecret = clientSecret;     }      public string Get(string url) {         var accessToken = GetAccessTokenResponse(url);          HttpWebRequest request = (HttpWebRequest)HttpWebRequest.Create(url);         request.Method = \"GET\";         request.Accept = acceptHeader;         request.Headers.Add(\"Authorization\", accessToken.TokenType + \" \" + accessToken.AccessToken);          HttpWebResponse response = (HttpWebResponse)request.GetResponse();           if (response.StatusCode == HttpStatusCode.OK || response.StatusCode == HttpStatusCode.NoContent) {             using (var reader = new System.IO.StreamReader(response.GetResponseStream())) {                 return reader.ReadToEnd();             }         }          return null;     }      public string Post(string url) {         var accessToken = GetAccessTokenResponse(url);          HttpWebRequest request = (HttpWebRequest)HttpWebRequest.Create(url);         request.Method = \"POST\";         request.ContentLength = 0;         request.Accept = acceptHeader;         request.Headers.Add(\"Authorization\", accessToken.TokenType + \" \" + accessToken.AccessToken);          HttpWebResponse response = (HttpWebResponse)request.GetResponse();           if (response.StatusCode == HttpStatusCode.OK || response.StatusCode == HttpStatusCode.NoContent) {             using (var reader = new System.IO.StreamReader(response.GetResponseStream())) {                 return reader.ReadToEnd();             }         }          return (null);     }      public string Merge(string url, Document doc) {          var accessToken = GetAccessTokenResponse(url);          var postData = new JObject {             [\"__metadata\"] = new JObject { [\"type\"] = \"SP.File\" }         };          foreach(string key in doc.MetaData.Keys) {             postData.Add(key, doc.MetaData[key]);         }          byte[] listPostData = Encoding.ASCII.GetBytes(postData.ToString());          HttpWebRequest request = (HttpWebRequest)HttpWebRequest.Create(url);         request.Method = \"POST\";         request.ContentLength = postData.ToString().Length;         request.ContentType = contenttype;         request.Accept = acceptHeader;         request.Headers.Add(\"Authorization\", accessToken.TokenType + \" \" + accessToken.AccessToken);         request.Headers.Add(\"If-Match\", \"*\");         request.Headers.Add(\"X-Http-Method\", \"MERGE\");          Stream listRequestStream = request.GetRequestStream();         listRequestStream.Write(listPostData, 0, listPostData.Length);         listRequestStream.Close();          HttpWebResponse response = (HttpWebResponse)request.GetResponse(); ","categories": ["Development"],
        "tags": ["API","Rest","Samples","SharePoint"],
        "url": "/2016/07/sharepoint-rest-api-handler/",
        "teaser": null
      },{
        "title": "ESP8266 based temperature monitor with Azure IOT",
        "excerpt":"Motion10 hosted a summerschool about Azure IOT. For the summerschool me and my colleagues (Jesse Gorter and Wesley Bakker) created several labs and presentations for temperature monitoring with a ESP8266.  The Azure IOT presentation is shared on SlideShare. All other information is shared on GitHub.  Presentation    Labs and other Presentations:  https://github.com/webbes/Summerschool  ","categories": ["Azure"],
        "tags": ["Azure","IoT","Labs"],
        "url": "/2016/08/esp8266-based-temperature-monitor-azure-iot/",
        "teaser": null
      },{
        "title": "Setting the API Definition URL and CORS value with ARM",
        "excerpt":"When working with Azure services, you will combine services together. For one of mine integration cases I had to create a LogicApp where in I needed to use one of my own API’s in order to perform some actions.  In order for the Logic apps designer to parse your Swagger and notice your API as connector, it's necessary that you enable CORS and set the APIDefinition properties of the web application you want to use. Using the portal this is very easy to set.    Open the settings blade of your Web App Under the API section set the 'API Definition' to the URL of your swagger.json file (this is usually https://{name}.azurewebsites.net/swagger/docs/v1). Add a CORS policy for '*' to allow for requests from the Logic apps Designer.  These settings can also be set when you use a Azure Resource Management template to deploy your resources.  It’s very simple as well you need to add the “apiDefinition” “url” property and the “cors” property to the web config section.  {   \"name\": \"web\",   \"type\": \"config\",   \"apiVersion\": \"[variables('defaultApiVersion')]\",   \"dependsOn\": [     \"[concat('Microsoft.Web/sites/', parameters('api_Name'))]\"   ],   \"tags\": {     \"displayName\": \"WebSettings\"   },   \"properties\": {     \"cors\": {       \"allowedOrigins\": [         \"*\"       ]     },     \"apiDefinition\": {       \"url\": \"[concat('https://', reference(concat('Microsoft.Web/sites/', parameters('api_Name'))).defaultHostName, '/swagger/docs/v1')]\"     }   } }  In de code snippet above check the value of the “url” property of the “apiDefinition” it is a concatenation of parameters and created resources within my resource template making the URL property dynamic.  ","categories": ["Azure"],
        "tags": ["ARM","Azure"],
        "url": "/2016/08/setting-api-definition-url-cors-value-arm/",
        "teaser": null
      },{
        "title": "Azure and Development Resources",
        "excerpt":"While browsing the internet and reading blog posts I usually find very useful links or great resources that I could use in many situation. The problem is that over time a lose these resources, so it is a good time to write them down and also share them with my readers.  When I find new resources I will update this post with the correct links and descriptions.  Temporary CORS Pass trough  Online there is a Azure Web App build that enables CORS pass trough for several situations. 1 example is storing your Swagger metadata file within Blob Strorage.  https://corspassthrough.azurewebsites.net/api/path?uri=https://.blob.core.windows.net//.swagger  The URI property can contain any URL you want to temporary enable CORS on.  Azure Developer Guide  Guide for Azure Developers on how to develop on Azure from day one using common app design scenarios.  https://azure.microsoft.com/en-us/campaigns/developer-guide/  Azure CDN Endpoint Interactive Map  The Microsoft Azure CDN (Content Delivery Network) Interactive Map shows the location of each of the Microsoft Azure CDN endpoints plotted over a map using the Bing Maps v8 JavaScript SDK.  This project is hosted as open source on Github:   https://github.com/crpietschmann/azure-cdn-map  This interactive map is created by: Chris Pietschmann  http://map-cdn.buildazure.com/  Microsoft Azure Region Map  The Microsoft Azure Region Map shows the location of each Microsoft Azure Region plotted over a map using the Bing Maps v8 JavaScript SDK.  Region locations can be retrieved by using a Azure CLI command:  azure location list --json  This project is hosted as open source on Github:   https://github.com/crpietschmann/azure-region-map  This interactive region map is created by: Chris Pietschmann  http://map.buildazure.com/  Markdown Cheat sheet  When writing your documentation in markdown it is great to have a cheat sheet for the correct syntax. This one is intended as a quick reference and showcase. For more complete info, you can view the original specs John Gruber's original spec and the Github-flavored Markdown info page.  https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet  Cheat sheet Collection  A great collection of cheat sheet for several scenario’s for example:   C# .Net HTML XML Wordpres and many more  http://www.cheat-sheets.org/  ","categories": ["Azure","Development"],
        "tags": ["Azure","CDN","Development"],
        "url": "/2016/09/azure-development-resources/",
        "teaser": null
      },{
        "title": "Escape Single Quotes within Azure Resource Template",
        "excerpt":"In some situations you will get into problems when creating a Azure Resource Template. I was creating a solution for a customer and had to add a Logic App to our template. That isn’t that hard I thought but when adding a Logic App to your template and making it parameterized and depending on other resources you can come into scenario’s were you will have to escape single quotes.  Looking into the documentation about azure resource templates you will not find any solution (I haven’t) so I tried a couple of escape characters and they didn’t work.  The last resort was a solution that isn’t that pretty but it works, I hope that in new releases this will be fixed.  Create a Variable  The first step for the solution is to create a variable in the template that contains a single quote.  \"variables\": {   \"singlequote\": \"'\" }  Use the variable  After you created the variable you can use this to combine you string values.   \"uri\": \"[concat('https://', reference(concat('Microsoft.Web/sites/', parameters('api_Name'))).defaultHostName, '/api/Item/', '@{encodeURIComponent(body(',variables('singlequote'), 'HTTP_GetItem',variables('singlequote'), ')?[', variables('singlequote'),'itemid_value', variables('singlequote'), '])}')]\"  As you can see in the above code snip it we set the “uri” property of a Logic App component. In this specific snipt it we dynamically get the hostname of a Azure Web Site the Logic App depends on. Further up in the “uri” we need single quotes as you can see by adding the variable ‘singlequote’.  ","categories": ["Azure DevOps"],
        "tags": ["Azure","Azure Resource Template","Development","Escape"],
        "url": "/2016/10/escape-single-quotes-within-azure-resource-template/",
        "teaser": null
      },{
        "title": "Custom Self Signed Certificate Identity Server",
        "excerpt":"For Identity server to be able to sign the login request you can add a Test certificate from the Identity Server it self or you are able to generate a certificate your self.  Generate certificate  To generate a certificate compatible for Identity Server your self you can perform the following command in a “Developer Command Prompt”.  C:\\Temp&gt;makecert -r -pe -n \"CN=MSFTPlayground\" -b 01/01/2015 -e 01/01/2099 -eku 1.3.6.1.5.5.7.3.3 -sky signature -a sha256 -len 2048 -ss Personal -sr LocalMachine  The command will generate a self signed certificate within your local computer certificate store.    Once generated you can export the certificate including the private key with the MMC-snapin.   Open a Command Prompt window, in Administration Mode.  Type mmc and press the ENTER key.  On the File menu, click Add/Remove Snap In.  Click Add.  In the Add Standalone Snap-in dialog box, select Certificates.  Click Add.  In the Certificates snap-in dialog box, select Computer account and click Next.  In the Select Computer dialog box, click Finish. (by default it will use “Local Computer”)  In the Add Standalone Snap-in dialog box, click Ok.   Within the Personal folder open the Certificates. Within that container you can see your certificate.   PowerShell  As many of you may know the makecert application is deprecated. The new option to generate certificates is using PowerShell. With the method “New-SelfSignedCertificate” it is quite hard / pretty impossible to generate this kind of certificates.  Vadims Podans created a helpful PowerShell extension for creating advanced certificates. The PowerShell extension can be downloaded from here:   Self-signed certificate generator (PowerShell)  This will leaves you with implementing the following PowerShell script.  .\\New-SelfSignedCertificateEx.ps1  New-SelfsignedCertificateEx -Subject \"CN=MSFTPlayground\" -EKU 1.3.6.1.5.5.7.3.3 -KeySpec \"Signature\" -KeyUsage \"DigitalSignature\" -SignatureAlgorithm \"SHA256\" -KeyLength 2048 -FriendlyName \"MSFTPlayground Code Signing\" -NotAfter $([System.DateTime]::Now.AddYears(15)) -Exportable  ","categories": ["Development"],
        "tags": ["Certificate","Identity Server"],
        "url": "/2016/10/custom-self-signed-certificate-identity-server/",
        "teaser": null
      },{
        "title": "Using Certificates in Azure App Services",
        "excerpt":"In different kind of situations you need to use a certificate for authentication or signing. I needed on because I was setting up a Identity Server the Identity Server V3 (https://identityserver.io) to be exact.  Within this Identity Server a certificate is used for signing. For testing purposes you can download one from their GitHub repository. But if you would like to buy one or generate one you will have to do some extra work.  If you want to generate a certificate you can read my post from last week:   Custom Self Signed Certificate Identity Server  The certificate that’s needed can be saved within the application or a more manageable solution by saving it within Azure.  On a blog page from Microsoft I found out how you can save the certificate within Azure and use it with your application. As this is a guide made with the old portal I rewrote the guide with steps that need to be down within the new Azure portal (https://portal.azure.com).  Upload the Certificate  The first step is to upload the certificate. To accomplish this follow the following steps:   Open the Azure portal: https://portal.azure.com Navigate to your created Azure App Service for example a Azure Web App. In the menu blade pick the option “SSL Certificates” under the “Settings” section. In the SSL Certificates blade upload your certificate and supply the password.    Adjusting the Web App Settings  When the certificate is uploaded the Web application it self needs to be instructed to load the certificate. This can be done by adding a application settings called “WEBSITE_LOAD_CERTIFICATES” and adding the thumbprint of the certificate you want to be loaded as the value. If you want to load multiple certificates you need to supply the value as a comma-separated list.   Open the Azure portal: https://portal.azure.com Navigate to your created Azure App Service for example a Azure Web App. In the menu blade pick the option “Application Settings” under the “Settings” section. In the newly opened blade scroll down to the section called “App Settings” and add the settings.     Before publishing your application to the cloud you can also add the value to your web.config.  &lt;add key=\"WEBSITE_LOAD_CERTIFICATES\" value=\"2697505afae56f3ac23a53716d2ff3029903d542\"/&gt;  Load the certificate within your application  With the previous steps done you are able to load the certificate within the application. By simply adding the following C# method:  public X509Certificate2 GetCertificate(string thumbprint) {      if (string.IsNullOrEmpty(thumbprint))         throw new ArgumentNullException(\"thumbprint\", \"Argument 'thumbprint' cannot be 'null' or 'string.empty'\");      X509Certificate2 retVal = null;      X509Store certStore = new X509Store(StoreName.My, StoreLocation.CurrentUser);     certStore.Open(OpenFlags.ReadOnly);      X509Certificate2Collection certCollection = certStore.Certificates.Find(X509FindType.FindByThumbprint, thumbprint, false);      if (certCollection.Count &gt; 0) {         retVal = certCollection[0];     }      certStore.Close();      return retVal; }  This method can simply be called by supplying the thumbprint of the certificate that you for example have saved within the web.config/ app settings.  //load the thumbprint from the web.config string thumbprint = WebConfigurationManager.AppSettings[WebConfigurationKeys.Thumbprint]; X509Certificate2 identityCertificate = GetCertificate(thumbprint); ","categories": ["Azure"],
        "tags": ["Azure","Azure App Services","Certificate","Identity Server"],
        "url": "/2016/11/using-certificates-azure-app-services/",
        "teaser": null
      },{
        "title": "Alerts on Failed Azure Logic Apps",
        "excerpt":"When working on and maintaining Logic Apps, you want to be notified if a certain action or complete Logic App (workflows) fails. Setting this up can be done by using Azure Monitoring.  Create Alert  By following the below steps you will create an Alert on your Logic App and specify when this alert will be triggered.   Open the Azure portal and navigate to the Logic App for which an alert needs to be specified. Within the menu blade click on “Alert Rules”.     A list of currently configured alerts on the resource will be opened. For specifying a new alert click on “Add metric alert” at the top of the screen.     On the “Add Rule” blade that appears you will need to fill in the correct information. The most important information is prefilled. Prefilling is done regarding the context we created the alert in. Besides the resource that is prefilled it is important to fill in the following information:     Metric: Example of metrics that are available:  Run latency: The average time between when a run starts an when it ends Run rate: How many runs start per second Run success latency: The average time between start and end, but only for successful runs Run success percentage: The number of successful runs divided by the total Runs cancelled: The number of runs you've cancelled Runs completed: The number of completed runs, irrespective of status Runs failed: The number of runs that failed Runs started: The number of runs that started in that minute Runs succeeded: The number of runs that completed successfully Actions failed: The number of actions failed.   Condition: Set it to “Greater than or equal to” this will make sure you will receive an alert when there is 1 failure. Threshold: Specify the value 1 Period: The period of time in which you would like to check the condition.       Press “OK”. To save the rule.  Now that the rule is saved the list of alerts will be shown. From this list you can monitor the alerts and check the status as shown in the image below.    The Alert  As a result of configuring the rule the filled in users and the owners will receive a alert when it is triggered. The alert it self will look similar to the below e-mail.    ","categories": ["Azure"],
        "tags": ["Azure","Azure Monitoring","Logic Apps","Monitoring"],
        "url": "/2016/11/alerts-failed-azure-logic-apps/",
        "teaser": null
      },{
        "title": "Recommendations on Azure with Azure Advisor",
        "excerpt":"Since last week “Azure Advisor” made it to public preview. Azure Advisor is a recommendation engine that recommends steps to take to optimally configure your Azure resources.  By analyzing the resource configuration and usage data it is able to make recommendations according to the best practices that in the long run will reduce costs and better the performance.  Start using the Azure Advisor  You can start using the Azure Advisor by following these steps:   Open the Azure Portal: https://portal.azure.com Click on “More services” in the services blade.     In the “More services” search for: Azure Advisor.     Clicking on “Azure Advisor” will open up the Azure Advisor blade.     Click on “Let’s get started!” Azure Advisor will start working and show you a message “We're refreshing recommendations for your subscriptions. This could take some time...” After some time (for me not more then a minute) it will show you the results.    Azure Advisor Results  After all the information is analyst the Azure Advisor blade will show some recommendations.   By clicking on the main recommendation you can see the recommendation per resource.     On the resource blade you can see a severity to show how important it is to do something regarding the recommendation. By clicking trough on the recommendation you will get more information. After clicking again it is also possible that it will show the blade to configure the the resource where the recommendation is referring to.    &nbsp;  The Azure Advisor will recommend on the following categories.     Category Description   High Availability Azure Advisor will inspect the resource configuration and provide recommendations that improve the availability of your application. For example, as a best practice, Azure Advisor will recommend that you move your single instance VMs into an availability set. This configuration ensures that during either a planned or unplanned maintenance event, at least one virtual machine will be available and meet the Azure VM SLA.   Security Azure Advisor integrates with Azure Security Center to show you the security related recommendations, so you have a unified Azure recommendation experience.   Performance Each Azure service typically has its own set of performance optimization recommendations. For example, the Redis cache advisor provides performance recommendations for Redis clusters. Instead of navigating to each Azure service used by your application, you can leverage Azure Advisor for a convenient and consolidated view of all your Redis, SQL DB, and webapps performance recommendations.   Cost Azure Advisor can look at your resource utilization and suggest ways to save money! For example, you may have VM’s that are underutilized. Azure Advisor will show you the estimated cost of running the VM and recommend either stopping the VM or downsize the VM.       Dismiss Recommendations  Besides reading the recommendations you also have the option to dismiss them.   Click on the main recommendation. Click on the dots next to the specific recommendation. Click on “Dismiss”.    Dismissing an recommendation will not remove it, but will only filter it out from the results. You can view the dismissed recommendations by allowing the “dismissed” items within the filter on the top left corner of the blade.    When showing dismissed items the view will show the severity icon as a grey icon.  My Opinion  Azure was missing a recommendation service like this. It really helps you with a better setup of your Azure environment. But it will not make your Solution / Solution Architecture better.  Before you begin with Azure or a new Solution on Azure you still need to define the correct Solution Architecture. The service will definitely help you with recommendation on you already running resources to reduce costs, and make sure your solution has a good performance, security.  ","categories": ["Azure"],
        "tags": ["Advisor","Azure"],
        "url": "/2016/11/recommendations-on-azure-with-azure-advisor/",
        "teaser": null
      },{
        "title": "Define Azure Resource Manager Policies",
        "excerpt":"Azure Resource Manager policies provide you with the ability to manage risk within you Azure environment. You can write policies to enforce certain situations.   A policy setting is default set to allow. Policies are described by policy definitions in a policy definition language (if-then conditions). You create polices with JSON formatted files.  Policies that you have defined can be assigned to certain scopes:   Subscription Resource Group Resource type  Within the definition of the policies you can define the below actions:   Deny: Blocks the resource request Audit: Allows the request but adds a line to the activity log. These can be used to start action within Azure Automation. Append: Adds specified information to the resource. For example tagging the resource with useful information.  I started to create Azure Resource Manager Policies and created a GitHub repository to save them and share them.  Within this repository you have the option to add policies and work together on them to get a default set of policies.  The repository also contains a script files to assign the policies to specific resources.  Implementation  One of the policies within the repository is a policy to ensure that resources are created within the Europe regions.  The policy is described in if-then conditions, if the resource is not created in West-Europe (westeurope) or North-Europe (northeurope) the creation of the resource will be declined (deny).  {     \"$schema\": \"http://schema.management.azure.com/schemas/2015-10-01-preview/policyDefinition.json\",     \"if\": {         \"not\": {             \"field\": \"location\",             \"in\" : [\"northeurope\" , \"westeurope\"]         }     },     \"then\": {      \"effect\": \"deny\"        } }  In the policy file the schema file specified to get type-ahead functionality within JSON editors.  This policy can be assigned to a resource with executing two PowerShell commands:   New-AzureRmPolicyDefinition New-AzureRmPolicyAssignment  With the first command the definition is created, and saved to a PowerShell object.  $policy = New-AzureRmPolicyDefinition -Name [Policy Name] -Description [Policy Description] -Policy [Path to Policy JSON File]  The second command is then used to assign the policy to a certain scope.  New-AzureRmPolicyAssignment -Name [Policy Name] -PolicyDefinition $policy -Scope [Scope]  Putting everything together and making the script as generic as possible you have the following script to assign a policy to a resource group.  $policyName = Read-Host \"Specify the name of the policy\"; $policyDescription = Read-Host \"Specify the description of the policy\" $policyFile = Read-Host \"Path to json policy file\"; $resourceGroup = Read-Host \"Specify the resource group\";  #Login to the Azure Resource Management Account Login-AzureRmAccount  #Let the user choose the right subscrition Write-Host \"---------------------------------------------------------------------\" Write-Host \"Your current subscriptions: \" -ForegroundColor Yellow Get-AzureRMSubscription Write-Host \"Enter the Subscription ID to deploy to: \" -ForegroundColor Green $sub = Read-Host  Set-AzureRmContext -SubscriptionId $sub clear  $subId = (Get-AzureRmContext).Subscription.SubscriptionId $subName = (Get-AzureRmContext).Subscription.SubscriptionName  Write-Host \"Policy is applied to the resource group: $resourceGroup in subscription: $subName\" $policy = New-AzureRmPolicyDefinition -Name $policyName -Description $policyDescription -Policy $policyFile;  #Assign the Azure Policy New-AzureRmPolicyAssignment -Name $policyName -PolicyDefinition $policy -Scope \"/subscriptions/$sub/resourceGroups/$resourcegroup\"  After assigning the policy and trying to create a resource in another region will result in a error message. Sometimes this can still be a really descriptive message as shown in the image below.    ","categories": ["Azure"],
        "tags": ["Azure","Governance","Policy"],
        "url": "/2016/11/define-azure-resource-manager-policies/",
        "teaser": null
      },{
        "title": "Running a Single Instance of a Azure Logic App",
        "excerpt":"In certain scenarios you want to run a Single Instance off a Azure Logic App every time. Scenarios were you need this is when containers are created within your flow and the other messages are also depending on the same containers.  Solution  Together with a colleague of mine we searched for a solution and finally found the answer after having contact with Microsoft. The trigger component within a Logic App has a property called “operationOptions”. This property can be set to “SingleInstance” this will make sure the Logic App will only be trigged per message.  Example  \"triggers\": {   \"When_a_message_is_received_in_a_topic_subscription\": {     \"type\": \"ApiConnection\",     \"inputs\": {       \"host\": {         \"api\": {           \"runtimeUrl\": \"https://logic-apis-westeurope.azure-apim.net/apim/servicebus\"         },         \"connection\": {           \"name\": \"[name]\"         }       },       \"method\": \"get\",       \"path\": \"[path]\"     },     \"operationOptions\": \"SingleInstance\",     \"recurrence\": {       \"interval\": 1,       \"frequency\": \"Minute\"     }   } }  The code snip-it above is a Service Bus trigger for new messages within a topic. By adding the “operationsOptions” this Logic App will only be triggered once per message.  Disadvantage  When using the “operationsOptions” property you have to keep in mind that only one instance will run within your interval. For example when there are 5 messages within the topic, and the Logic App is triggered once every minute it will get one message from the topic every minute and will not run a new flow if the preceding message is processed. In this example it will take about 5 minutes to process the 5 messages depending on how long your Logic Apps runs.  ","categories": ["Azure"],
        "tags": ["Azure","Logic App"],
        "url": "/2016/11/running-single-instance-azure-logic-app/",
        "teaser": null
      },{
        "title": "Azure Resource Templates and Deployment Slots",
        "excerpt":"In some situations you may want to start using Deployment Slots in combination with your Azure App Service. This means you will have separate deployment slots instead of only the default production slot when running the App Service in Standard or Premium plan mode.  Deployment Slots  Deployment slots are actually live web apps with their own hostnames. Web app content and configurations elements can be swapped between deployment slots, including the production slot. Deploying your application to a deployment slot has the following benefits:   Validate web app changes in a staging deployment slot before swapping it with the production slot. Eliminates downtime when you deploy your web app. The traffic redirection is seamless, and no requests are dropped as a result of swap operations. Swap back, If the changes swapped into the production slot are not as you expected, you can perform the same swap immediately to get your \"last known good site\" back.   Resource Templates  This raises the question of how you are able to deploy deployment slots by using Azure Resource Templates.  A default App Service (web application) looks like the below snip-it.  {   \"apiVersion\": \"2015-08-01\",   \"name\": \"[parameters('siteName')]\",   \"type\": \"Microsoft.Web/sites\",   \"location\": \"[variables('location')]\",   \"tags\": {     \"[concat('hidden-related:', resourceGroup().id, '/providers/Microsoft.Web/serverfarms/', parameters('hostingPlanName'))]\": \"Resource\",     \"displayName\": \"Website\"   },   \"dependsOn\": [     \"[concat('Microsoft.Web/serverfarms/', parameters('hostingPlanName'))]\"   ],   \"properties\": {     \"name\": \"[parameters('siteName')]\",     \"serverFarmId\": \"[resourceId('Microsoft.Web/serverfarms', parameters('hostingPlanName'))]\"   },   \"resources\": [] },  Deployment slots are in fact resources of the App Service it self and can be deployed as such. This means you can specify them within the resources array of your web application / App Service.  {   \"apiVersion\": \"2015-08-01\",   \"name\": \"[parameters('tstSlot')]\",   \"type\": \"slots\",   \"tags\": {     \"displayName\": \"TST Slot\"   },   \"location\": \"[variables('location')]\",   \"dependsOn\": [     \"[resourceId('Microsoft.Web/Sites', parameters('siteName'))]\"   ],   \"properties\": {   },   \"resources\": [] }  The above snip-it adds a Deployment Slot to the App Service called “tst”. This name is defined within the parameters file of the template. When deployed the host name for the Deployment Slot will be:   https://[App Service Host Name]-[Slot Name].azurewebsites.net  Defining application settings on a Deployment Slot works the same as defining them on the application it self, and that is by adding a resource of the name “appsettings” to the resources array of the Deployment Slot.  {   \"name\": \"appsettings\",   \"type\": \"config\",   \"apiVersion\": \"2015-08-01\",   \"dependsOn\": [     \"[concat('Microsoft.Web/sites/', parameters('siteName'))]\",     \"[concat('Microsoft.Insights/components/', parameters('appInsightsName-tst'))]\"   ],   \"tags\": {     \"displayName\": \"AppSettings\"   },   \"properties\": {     \"insights:InstrumentationKey\": \"[reference(concat('Microsoft.Insights/components/', parameters('appInsightsName-tst'))).InstrumentationKey]\"   } }  This snip-it configures a application setting that is called “insights:InstrumentationKey” to the Application Insights component for Test that is created within the same Resource Template.  Slot Setting  As you may know a Deployment Slot setting has a marking option called “Slot Setting”. This means that the setting is sticky to the slot that gives you the option to keep settings specific for a specific environment. Take for example your production connection strings.  Within the “AppSettings” section in the Azure Resource Template you do not have an option to specify it.  To enable those markings another needs to be added to the App Service it self. This resource needs to be called “slotConfigNames” and must be of the type “config”. Within this resource you have to specify a property called “appSettingNames” that is an array of string items representing the application setting names.  {   \"name\": \"slotConfigNames\",   \"type\": \"config\",   \"apiVersion\": \"2015-08-01\",   \"dependsOn\": [     \"[concat('Microsoft.Web/sites/', parameters('siteName'))]\"   ],   \"tags\": {     \"displayName\": \"slotConfigNames\"   },   \"properties\": {     \"appSettingNames\": [       \"insights:InstrumentationKey\"     ]   } }  In the above snip-it the application setting “insight:InstrumentationKey” is specified as a “Slot Setting” in order to keep it sticky with the slots.  If you are interested in the complete resource template you can take a look at my GitHub repository:   https://github.com/MaikvanderGaag/MSFT-Azure-Deployment-Templates  ","categories": ["Azure DevOps"],
        "tags": ["Azure","Azure Resource Template","Deployment"],
        "url": "/2016/12/azure-resource-templates-deployment-slots/",
        "teaser": null
      },{
        "title": "2016 review of MSFTPlayground",
        "excerpt":"One of the things I like to do is sharing knowledge with the Community. I started my blog in 2008 and changed the platform used a couple of times from WordPress to SharePoint and BlogEngine.Net to SharePoint to WordPress again.  The last two years I have been using WordPress and I will continue to use WordPress in 2017. In Q4 of 2015 I started to blog more actively and that is still going strong.  The blog grew from 166 post to 201 post in 2016 and the views on my pages grew enormous. With atop of 440 views on one day.  Beside my blog I also started some other things in 2016:   Twitter: Started to use twitter more actively Facebook: I started a Facebook page for my blog. GitHub: I started a GitHub repository to share code and knowledge. YouTube Channel: For sharing video’s hope to have time to make some more in 2017.  Users and Pageviews  In 2016 the blog had 54,151 unique users and had 75,296 pageviews.    &nbsp;  Top 10 Pages of 2016  The top 10 pages of 2016 are except two post posts out of other years. (I really have to do announce my new posts better )    &nbsp;  GEO  The locations my blog is visited from did not change very much from 2015.    &nbsp;  Browser Use  This year edge is making its way to the top, but still needs to beat some contestants.    &nbsp;  &nbsp;  Thank you for visiting my blog in 2016. I hope to see you back in 2017, I will keep writing blog posts and will try to boost the quality even further.  &nbsp;  Happy New Year!  ","categories": ["General"],
        "tags": ["MSFTPlayground","Review"],
        "url": "/2016/12/2016-review-msftplayground/",
        "teaser": null
      },{
        "title": "Azure Management",
        "excerpt":"Azure Management and Governance is hard because Azure services are growing at a fast pace. Try to imagine that you are a Azure Administrator for a large company and that you need to maintain the Azure Subscription.  If you start thinking about it sounds really hard and that makes you wonder about options that you have to manage and keep it running as it should be.  For managing your Azure environment there are a couple of capabilities. Ad the time of writing there are the following build-in options:   Azure Policy &amp; Audit Role Based Access Control Azure Automation Resource Locks Azure Security Center Resource Groups Azure Advisor  Role Based Access Control is a huge one here. This was in place since the beginning of the new Azure Portal. This made sure you did not have to make someone Co-Administrator on your complete subscription. Besides the build-in options there is also a need to define a policy regarding your Azure Environment also called a Governance document. In the document you need to specify naming conventions that are used for creating and naming of services. The options described above can help you maintain that Governance.  Azure Policy &amp; Audit  Azure Policies and Audits help you manage risks within your Azure Environment / Subscription. Therefore it will help you by restricting, enforcing or auditing events.  Some key facts for policies and audits in Azure are:   It is a default allow system. By default everything is allowed. You will need to define deny or audit actions. Policies are created by Policy definitions, which are if-then conditions. Policy definitions are defined in JSON Deployed and Maintained via the Resource Manager.  Policies within Azure can be build on three types of actions:   Deny: Stop / Cancel the request. Audit: Add a specific message to the Audit log. Append: Add information to the resource. For example add a specific tag.  The defined policies can be enforced within the following scopes:   Subscription Resource group Resource type  My article about Azure Policies published a few weeks ago shares some more information and also contains information about the implementation:   Define Azure Resource Manager Policies  Besides the article I also maintain a GitHub repository with policies I created:   https://github.com/MaikvanderGaag/MSFT-Azure-Policy  Alongside the policies you also have the activity (audit) log that gives insight in all activities in and around Azure.    Resource Locks  Resource locks support another type of management on your resources. With resource locks you for example are able to make a resource read-only. Which makes it possible to secure resources in another way. At the time of writing this article Resource Locks support two actions:   CanNotDelete: You will not be able to delete the resources with this lock applied. ReadOnly: You only be able to read the resources with this lock applied, and will not be able to edit specific properties.  With a Resource Lock applied you can make sure someone does not accidently delete a business critical application, or for example the virtual network with the connection to the on-premise systems.   Resource Tags  Within Azure you have the option to add metadata to your resources in the form of tags. Tags are key value pairs that enable you to group and aggregate resources in various ways. With the use of Azure Policies you can also make sure that resources are tagged automatically with for example the Cost Center. Examples for tags are:   Cost Center Department Owner Project    &nbsp;  In my opinion you should always tag your resources groups with useful information and tag the resources within with the tags that are appropriate.  Resource Groups  A Resource group is a logical container for resources. You can resources by project or application for example.  The resource group gives you the ability to manage the following:   Delete resources at once. Move resources at once. Assign rights for the resource group.  Because of the new model of Azure you can also assign rights within a Resource Group like Role-based access control to give access to resources.  Azure Automation  This service provides a way for people to automate the manual, long-running, error-prone, and frequently repeated tasks that are commonly performed in a enterprise environment.  With Azure Automation you create so called Runbooks that are sets of tasks that are needed to be performed. You can use Runbooks created by the community within you Azure Environment.   Azure Automation: Script resources for IT professionals Runbook and module galleries for Azure Automation  Examples for Runbooks are:   Automatically shut down Azure VM’s Start and run specific services.  Azure Security Center  As Microsoft states Security Center helps you prevent, detect, and respond to threats with increased visibility into and control over the security of your Azure resources. It provides integrated security monitoring and policy management across your Azure subscriptions, helps detect threats that might otherwise go unnoticed, and works with a broad ecosystem of security solutions.  In the most simple form Azure Security Center is a service that gives you recommendation about the security of your services. Besides that it is also able to monitor the services in order to detect threats within your environment.    Policies within Security center can be turned off when they are not seen as threats and based on specific settings you can also define alerts.  Azure Advisor  Since last month “Azure Advisor” made it to public preview. The service is a recommendation engine that recommends steps to optimally configure your Azure resources. By analyzing the resource configuration and usage data it is able to make recommendations according to the best practices that in the long run will reduce costs and better the performance.  The Azure Advisor will give recommendation on four categories:   High Availability Security Performance Costs  You may noticed that the list also contains a Security category this category is a integration with Azure Security Center.  For information and how to start using the service read the following article:   Recommendations on Azure with Azure Advisor  Useful links   Azure subscription and service limits, quotas, and constraints Recommended naming conventions for Azure resources Use Policy to manage resources and control access Audit operations with Resource Manager Using tags to organize your Azure resources Lock resources with Azure Resource Manager Azure Automation overview Runbook and module galleries for Azure Automation Introduction to Azure Security Center Introduction to Azure Advisor  ","categories": ["Administration","Azure"],
        "tags": ["Azure;Management;Governance"],
        "url": "/2017/01/azure-management/",
        "teaser": null
      },{
        "title": "Undo Checkout within TFS or VSTS",
        "excerpt":"This post is written as a reminder for myself and for others who keep forgetting. The thing I keep forgetting is where to find the option to undo a check out on a file checked out by someone within TFS.  It sometime happens that a file is kept checked out, because someone needed to leave early or even because they deleted the workspace they used. In most project that means PIE the next day.  Microsoft Visual Studio Team Foundation Server 2015 Power Tools makes the tasks of undoing the checked out item easy if you are a administrator on a project.  Steps   Install “Microsoft Visual Studio Team Foundation Server 2015 Power Tools”. Open Visual Studio. Within Visual Studio open the Source Control Explorer. Find the file that is checked out by someone else. Right click the file and select “Find” and then “Find by Status”. (This option is added by the TFS Power Tools mentioned in Step 1).     Click on \"Find\" in the Dialog that appears.     In the search result window right click the check out from the appropriate person and select “Undo”.     In the dialog after clicking on “Undo” select “Ok” to make the action definite.    ","categories": ["DevOps"],
        "tags": ["Reminder","TFS","VSTS"],
        "url": "/2017/01/undo-checkout-within-tfs/",
        "teaser": null
      },{
        "title": "Practice for working with Deployment Slots within Azure App Services",
        "excerpt":"When you make use of Azure App Services you can deploy to different deployment slots.  You can choose to work with three deployment slots (dev, tst, acc) and use the default one as production, but what can you do best regarding this functionality.  Deployment slots  Deployment slots are live web apps with their own hostnames. Web app content and configurations elements can be swapped between two deployment slots, including the production slot. Deploying your application to a deployment slot has the following benefits:   You can validate web app changes in a deployment slot before swapping it with the another slot. Eliminate downtime on deployment, and automate the swapping. Easy to swap a bad deployment back.  &nbsp;  Note:”The app (App Service Plan) must be running in the Standard or Premium mode in order for you to enable multiple deployment slots. Everything described in this article is for Azure App Services and then specific Azure Web Apps, Azure API Apps and Azure Mobile Apps.”  Practice  Deployment slots can be deployed/ created easy by making use of the Azure Portal or Azure Resource Templates. Information on deploying Azure Deployment Slots via Azure Resource Templates can be found on a blog post I wrote a few weeks ago:   Azure Resource Templates and Deployment Slots  Within the Azure Portal (https://portal.azure.com) you need to take the following steps:   In the Azure Portal, open the blade of your Azure App Service. Click Settings, and then click Deployment slots. In the “deployment slots” blade click on Add Slot. In the “Add a slot” blade, give the slot a name, and select whether to clone web app configuration from another existing deployment slot.  Before creating deployment slots you need to have a useful pattern. In my opinion it is best to use a deployment slot as a pre production environment. This will lead to the following solution architecture.    This solution architecture makes sure that the development/ test / acceptation environment is completely separated from production. Via the pre production deployment slot you are able to test your application against the production SQL database. With this setup you can't alter any data in pre-production this is why there is a dotted line between the pre production web app and the test / acceptation database. That you could use to test all your CRUD operations. Besides this is also makes sure that the pre production is as close to production as needed for a clear test results.  Considerations  When making use of deployment slots there are a considerations and disadvantages to be aware of:   The web application and deployment slots are created within the same environment (This is also the case when you create a Azure Web App within the same App Service Plan). This means resources between the two instances are shared, So if you want to do a stress test on pre-production you are certain that it will effect the performance of the production instance.  Besides the above disadvantage also consider the settings that can and can't be swapped:  Swapped:   General settings - such as framework version, 32/64-bit, Web sockets. App settings (you can configure it to stick to a slot). Connection strings (you can configure it to stick to a slot). Handler mappings. Monitoring and diagnostic settings. WebJobs content.  Not swapped:   Publishing endpoints. Custom Domain Names. SSL certificates and bindings. Scale settings. WebJobs schedulers.  ","categories": ["Azure"],
        "tags": ["App Services","Azure","Deployment Slots"],
        "url": "/2017/01/practice-for-deployment-slots/",
        "teaser": null
      },{
        "title": "Adding your Client IP to the Azure SQL server firewall",
        "excerpt":"One of the first things that need to be done when you create a Azure SQL database is adding the Client IP address to the firewall of the Azure SQL server. By default all traffic between Azure services are allowed but traffic from another machine is disallowed. Connecting to the server with for example SQL Management Studio will result in the following error:  &nbsp;  “Cannot open server [servername] requested by the login. Client with IP address [IP Address] is not allowed to access the server.”  &nbsp;    Steps  When you open the Azure SQL Database within Visual Studio it will prompt you the question if you would like to add your Client IP to the firewall in order to access the database.  If you prefer another tool like SQL Server Management studio you will have to configure this within the Azure Portal (as mentioned in the error):   Open the Azure Portal: https://portal.azure.com. Click on Resource Groups and then the resource group of the SQL server. In the Resource Group blade click on the SQL server. Within the “Security” Category click on “Firewall”. Add your Client IP within this blade. Click on save to save the settings.  &nbsp;    &nbsp;  Until today I navigated to the website: https://www.whatismyip.com/ to retrieve my external IP and add it via the steps mentioned above but today I noticed another option (I think it already was there for quite some time). Within the firewall blade you can click on “Add client IP” to add your current client IP.  &nbsp;    ","categories": ["Azure"],
        "tags": ["Azure","IP","SQL"],
        "url": "/2017/01/adding-your-client-ip-to-the-azure-sql-server-firewall/",
        "teaser": null
      },{
        "title": "Creating your first Precompiled Azure Function",
        "excerpt":"Since a couple of weeks it is possible to create Precompiled Azure Functions.  This enables the use of .NET assemblies containing the function implementation, and bypassing the dynamic compilation process.  Precompiled Function  In order to use precompiled functions, you need to create a .Net assembly and adjust the function.json file. First of create a class library within Visual Studio  and add the following class:  using Microsoft.Azure.WebJobs.Host;  namespace MsftPlayground.Function {     public class LoggingFunction     {         public static void Run(string input, TraceWriter log) {             log.Info($\"Manually triggered PRECOMPILED function: {input}\");         }     } }  For this to work and compile the “Microsoft.Azure.WebJobs” NuGet packages needs to be added to your project.  Once the function method it self is able to compile we can create the “function.json” file. Important for this file is:   scriptFile: The file that contains the function. entryPoint: The fully qualified method name.  All of these options together create the following “function.json” file for a manually triggered function.  {   \"scriptFile\": \"MsftPlayground.Function.dll\",   \"entryPoint\": \"MsftPlayground.Function.LoggingFunction.Run\",   \"disabled\": false,   \"bindings\": [     {       \"type\": \"manualTrigger\",       \"direction\": \"in\",       \"name\": \"input\"     }   ] }  Azure Function  No that we have the correct files in place we need to get this up and running in Azure.   Open the Azure Portal (https://portal.azure.com) Navigate to the resource group where in you want to create the Function or create a new one. In the resource group click “Add” to create a “Function App”.     In the created Function App create a new function. In this example a C# Manually triggered function will be created.     In the develop section of the function click on “View files”.     In the file browser delete the “run.csx” file and adjust the “function.json” file as mentioned above. Now use the upload button to upload the compiled file. Click the “Run” button so see if your compiled function works.    The functionality looks very promising and make sure you bypass the dynamic compilation process. But there are a few things to keep in mind when the Azure Function:   When you make manual adjustments within the portal the function.json file will be refreshed (both scriptFile and entryPoint properties will be removed). Compiled files will only be loaded the first time you run the function. When you make changes to the compiled file you will need to restart the function app before changes will be applied.  Restarting the complete Azure Function App can be done via:   Open the Function App. Click on Function App settings. In the settings window click “Go to App Service Settings” In the overview blade click on “Restart”.  All source files are available on GitHub:   https://github.com/MaikvanderGaag/MSFT-PrecompiledFunction  ","categories": ["Azure"],
        "tags": ["Azure","Azure Functions"],
        "url": "/2017/01/creating-your-first-precompiled-azure-function/",
        "teaser": null
      },{
        "title": "Speaking at the Global Azure Bootcamp",
        "excerpt":"On 22 April I will be speaking at the Global Azure Bootcamp of Motion10. This year the Global event will be held for the fifth time. It is a one-day deep dive class to help thousands of people get up to speed on developing solutions for Azure.  Motion10 (Rotterdam, Netherlands) will be one of the location in 2017. Me, Tomasso Groenendijk (Azure MVP), Bobby Bregman and Jesse Gorter will prepare the presentations and hand-on labs.  All information that I prepare will be shared on this blog after the bootcamp, and register for the event when you are in the area on 22 April.   Register More Information (Dutch) Global Azure Bootcamp Event  Agenda     09:30 - 09:45 Opening - Gijs in ’t Veld   09:45 - 10:45 Identity Management with Azure Active Directory – Bobby Bregman   10:45 - 12:00 Safely saving data – Jesse Gorter   12:00 - 12:30 Quiz, let’s put your knowledge to the test!   12:30 - 13:00 Let’s eat! Lunchtime   13:00 - 14:20 API for the data – Maik van der Gaag   14:30 - 15:50 API Management - Tomasso Groenendijk (Azure MVP)   15:50 - 16:00 Closing   16:00 Drinks    &nbsp;  What is Global Azure Bootcamp 2017?  Learning about Azure is a massive undertaking for anyone. The Azure Cloud Platform is really big and offers a very long list of great services. How do you get a grip on it all? Together! Community leaders around the world know very well how challenging it is to find the right presenters, get good material and keep up to date with the fast moving Azure Platform.  Call it shooting at a moving target or drinking from the fire hose, we all feel it! Together we can focus our combined strengths, find resources, local user groups and connect to each other to learn smarter! Let’s build a list of resources and enable ways to connect the whole year and not only during Global Azure Bootcamp! Any user group leader can be a part of this if they wish!  More about this story will unfold as we get closer to the event. First of all, right now, sign up to create your own Global Azure Bootcamp event! Welcome to GAB 2017!  ","categories": ["Development"],
        "tags": ["Bootcamp","Global Azure"],
        "url": "/2017/02/speaking-global-azure-bootcamp/",
        "teaser": null
      },{
        "title": "Trace listeners (Logging) with Application Insights",
        "excerpt":"For every application that is build a logging framework needs to be present. Tracing and Application are framework that can be used in almost every situation.  Application Insights is an extensible Application Performance Management (APM) service for web developers on multiple platforms. It can be used to monitor your live web application. It can automatically detect performance anomalies and includes powerful analytics tools to help you diagnose issues and to understand what users actually do with your app.  Trace listeners (Tracing) on the other end are objects that get tracing information from the trace class and output the data to a medium that is configured. For instance you can write trace information to a UI, file or a windows event log.  Trace information can be send to Application Insights by making use of the Application Insights trace listener.  Custom Tracing class  Write a custom tracing class for your application. This class can be a static class because we do not want to initiate the class and it needs to be constructed only once. In the class methods need to be created for every event severity.  public static class AppTrace {     private static TraceSource traceSource { get; set; }      static AppTrace() {         traceSource = new TraceSource(\"TraceLogging\");     }      public static void Verbose(string message, int id = 16, [CallerMemberName]string memberName = \"\", [CallerFilePath] string filePath = \"\", [CallerLineNumber]int lineNumber = 0) {         traceSource.TraceEvent(TraceEventType.Verbose, id, Format(message, memberName, filePath, lineNumber));     }      public static void Error(string message, int id = 2, [CallerMemberName]string memberName = \"\", [CallerFilePath] string filePath = \"\", [CallerLineNumber]int lineNumber = 0) {         traceSource.TraceEvent(TraceEventType.Error, id, Format(message, memberName, filePath, lineNumber));     }      public static void Information(string message, int id = 8, [CallerMemberName]string memberName = \"\", [CallerFilePath] string filePath = \"\", [CallerLineNumber]int lineNumber = 0)         traceSource.TraceEvent(TraceEventType.Information, id, Format(message, memberName, filePath, lineNumber));     }      public static void Critical(string message, int id = 1, [CallerMemberName]string memberName = \"\", [CallerFilePath] string filePath = \"\", [CallerLineNumber]int lineNumber = 0) {         traceSource.TraceEvent(TraceEventType.Critical, id, Format(message, memberName, filePath, lineNumber));     }      public static void Warning(string message, int id = 4, [CallerMemberName]string memberName = \"\", [CallerFilePath] string filePath = \"\", [CallerLineNumber]int lineNumber = 0) {         traceSource.TraceEvent(TraceEventType.Warning, id, Format(message, memberName, filePath, lineNumber));     }      public static void Start(string service, int id = 256, [CallerMemberName]string memberName = \"\", [CallerFilePath] string filePath = \"\", [CallerLineNumber]int lineNumber = 0) {         traceSource.TraceEvent(TraceEventType.Start, id, Format(\"Starting - \" + service, memberName, filePath, lineNumber));     }      public static void Stop(string service, int id = 512, [CallerMemberName]string memberName = \"\", [CallerFilePath] string filePath = \"\", [CallerLineNumber]int lineNumber = 0) {         traceSource.TraceEvent(TraceEventType.Stop, id, Format(\"Stoping - \" + service, memberName, filePath, lineNumber));     }      private static string Format(string message, string memberName, string filePath, int lineNumber) {         return $\"Message: {message}, MemberName: {memberName}, FilePath: {filePath}, LineNumber: {lineNumber}\";     } }   In the constructor the source name is configured this makes it possible to bind the trace listeners within the application configuration.  Application Insight trace listener  For sending trace information to Application Insights a reference needs need to be added to the project by adding \"Microsoft.ApplicationInsights.TraceListener\" NuGet package.  From the “Package Manager Console”  Install-Package Microsoft.ApplicationInsights.TraceListener  From the “Package Manager UI”    Configure the trace listener  To bind trace listeners to the trace source they need to be configured within the web.config or app.config. This can be done by adding the below configuration section within the configuration tag.  &lt;system.diagnostics&gt;   &lt;sources&gt;     &lt;source name=\"TraceLogging\" switchName=\"Verbose\"&gt;       &lt;listeners&gt;         &lt;add name=\"appinsights\" type=\"Microsoft.ApplicationInsights.TraceListener.ApplicationInsightsTraceListener, Microsoft.ApplicationInsights.TraceListener\"/&gt;         &lt;add name=\"console\" type=\"System.Diagnostics.ConsoleTraceListener\" /&gt;       &lt;/listeners&gt;     &lt;/source&gt;   &lt;/sources&gt; &lt;/system.diagnostics&gt;  Within our class the trace listeners are referenced by using the source name “TraceLogging”.  Within the configuration of the “Source” the listeners are configured. In the snip-it above it has a listener for Application Insights and for the Console Window. This means that depending on the “TraceLevel” of the message the message will be send to Application Insights and to the Console.  Which messages are send to the listeners depend on the switch. In the \"source\" tag it is configured by the \"swithName\" property. The switch is something you would like change when you find a problem within your application.  Azure  At the moment it is not possible to change tracing configuration within the Azure Portal that will reflect the above settings. To be able to make this configurable you can add a specific app setting to be able to set the switch in code.  Besides the switch you should also make the Id (\"InstrumentationKey\") of the Application Insights service configurable.  Additions to keep it configurable  Web.config  Add an application setting for the instrumentation key and trace switch.  &lt;appSettings&gt;   &lt;add key=\"InstrumentationKey\" value=\"[key]\"/&gt;   &lt;add key=\"TraceSwitch\" value=\"All\"/&gt; &lt;/appSettings&gt;  Trace Class  To make use of these settings we also need to adjust the constructor of our class. Make the following adjustments to the constructor.  static AppTrace() {     traceSource = new TraceSource(\"TraceLogging\");     traceSource.Switch.Level = (SourceLevels)Enum.Parse(typeof(SourceLevels), ConfigurationManager.AppSettings[\"TraceSwitch\"], true);     TelemetryConfiguration.Active.InstrumentationKey = ConfigurationManager.AppSettings[\"InstrumentationKey\"]; } Write information to the Trace Log  With everything in place it is easy to write monitoring information to any source of your choosing, and depending on the level specified in the settings information will be send to the log.  class Program {     static void Main(string[] args) {         AppTrace.Verbose(\"Test Verbose\");         AppTrace.Error(\"Test Error\");         AppTrace.Warning(\"Test Warning\");         AppTrace.Information(\"Test Information\");         AppTrace.Critical(\"Test Critical\");          Console.ReadKey();     } }  Starting this console application will show messages within in the console and Application Insights:  Console Output    Application Insights Output    &nbsp;  As we change the “TraceSwith” setting to for example “Critical” you will see within the console window that less information is send by the trace source.    ","categories": ["DevOps"],
        "tags": ["Application Insights","Azure","Logging","Monitoring","Trace"],
        "url": "/2017/02/trace-listeners-logging-azure-application-insights/",
        "teaser": null
      },{
        "title": "Azure Function CI &ndash; Overview",
        "excerpt":"Azure Functions provide event-based serverless computing that make it easy to develop and scale your application. By using Azure Functions CI you can setup continious integration for your functions. Since a couple of months Visual Studio contains a preview with the ability to create a function project in Visual Studio, add functions using any supported language, run them locally, and publish them to Azure. But this project template does not make it completely possible to use the template with a CI scenario.  In the next couple of weeks I will publish a couple of post to get you started with Azure Function and Continuous Integration within Visual Studio Team Services.  Posts  The following posts can be found within the Azure Functions CI series, when not published links will link to a 404 page:   Creating a Pre-Compiled Azure Function. Create a CI Build for the Azure Function. Deploying the Azure Function from the Build within VSTS.  Prerequisites  The series of blog post is created by using the Azure Function Visual Studio template, prerequisites for the posts are listed below.   Azure Function Tools Information Visual Studio Team Services  &nbsp;  Screenshots and examples are created with the “New Build Editor” preview enabled within Visual Studio Team Services.    ","categories": ["DevOps"],
        "tags": ["Azure","CI","VSTS"],
        "url": "/2017/03/azure-functions-ci-overview/",
        "teaser": null
      },{
        "title": "Azure Function CI &ndash; 1. Creating a Pre-Compiled Azure Function",
        "excerpt":"As mentioned in the overview Azure Function provide event-based serverless computing that make it easy to develop and scale your application, paying only for the resources your code consumes during execution. Since a couple of months Visual Studio contains a preview with the ability to create a function project in Visual Studio, add functions using any supported language, run them locally, and publish them to Azure. But this project template does not make it completely possible to use the template with a CI scenario.  Especially running the Azure Function locally offers a great functionality and gives you the option to debug the function before publishing it to Azure.  Prerequisites  This blog post is totally written using the Azure Function Visual Studio template. Information and the Azure Function tools download are listed below:   Azure Function Tools Information  Getting Started  When you have the Azure Function tools installed we can start creating a new project with Visual Studio. For this project make use of the “Azure Function (Preview)” template that is added when you have the Azure Function Tools installed.    &nbsp;  Give the project a appropriate name and also do this for your solution. Click “Ok”to create the project and solution. When the solution is created right click on the solution and add a new project. This time add a project of the “Class Library” template. We will use this project for the pre-compiled function.    If the project is created rename the “Class1” file to “MsftHttpTrigger” and also rename the class itself.  Next step is to add the required NuGet packages in order to run the function itself, add the following NuGet Packages to the class library project:   “Microsoft.Azure.WebJobs” “Microsoft.AspNet.WebApi.Client” “Microsoft.AspNet.WebApi.Core”  &nbsp;    &nbsp;  For this example we will use the default “Run” method you normally get within the “run.csx” copy and past it within the MsftHttpTrigger class.  using Microsoft.Azure.WebJobs.Host; using System.Linq; using System.Net; using System.Net.Http; using System.Threading.Tasks;  namespace Implementation {     public class MsftHttpTrigger     {         public static async Task&lt;HttpResponseMessage&gt; Run(HttpRequestMessage req, TraceWriter log) {             log.Info($\"C# HTTP trigger function processed a request. RequestUri={req.RequestUri}\");              string name = req.GetQueryNameValuePairs()                 .FirstOrDefault(q =&gt; string.Compare(q.Key, \"name\", true) == 0)                 .Value;              dynamic data = await req.Content.ReadAsAsync&lt;object&gt;();             name = name ?? data?.name;              return name == null ? req.CreateResponse(HttpStatusCode.BadRequest, \"Please pass a name on the query string or in the request body\"): req.CreateResponse(HttpStatusCode.OK, \"Hello \" + name);         }     } }  Function Project  With the pre-compiled function ready the adjustments can be made to the Azure Functions project.  We need to start by adding a new function to the function project by right clicking on the project and selecting “Add” – “New Azure Function”.    This option will open a dialog with a lot off different Azure Function options. Within this example we will use a HttpTrigger – C# function.  &nbsp;    After clicking “Create” the function will be created and we can delete the files we don’t need. The files that can be deleted are:   “sample.dat” “run.csx” “readme.md”  The other file within the function (function.json) needs to be changed to refer and load the pre-compiled function. This can be done by adding a “scriptfile” property and “entrypoint” that refers to the method within the “scriptfile”.  {   \"disabled\": false,   \"scriptFile\": \"bin\\\\Implementation.dll\",   \"entryPoint\": \"Implementation.MsftHttpTrigger.Run\",   \"bindings\": [     {       \"authLevel\": \"function\",       \"name\": \"req\",       \"type\": \"httpTrigger\",       \"direction\": \"in\"     },     {       \"name\": \"res\",       \"type\": \"http\",       \"direction\": \"out\"     }   ] }  As specified in the code snip-it above the “scriptfile” (binary) is looked within the bin folder of the function.  Copy DLL  As specified within the function.json file the pre-compiled function resides within the “bin” folder of the function. This means we will need to populate the bin folder with the dll of the class library project. In order to get the binary file in place we will make use of the build events that can be specified in a project.  Right click the project and select properties. Open the build events and add a post build event to copy the output to the functions bin folder.  SET FILEPATH=$(SolutionDir)MSFT.Function\\MsftHttpTrigger\\bin  IF NOT EXIST \"%FILEPATH%\" (         GOTO MakeDir ) ELSE (        GOTO CopyFiles )  :MakeDir mkdir \"%FILEPATH%\" :CopyFiles ","categories": ["DevOps"],
        "tags": ["Azure","Azure Functions","CI","VSTS"],
        "url": "/2017/03/azure-function-ci-1-creating-a-pre-compiled-azure-function/",
        "teaser": null
      },{
        "title": "Preview of the Policy Management UX",
        "excerpt":"Azure Resource Manager policies provide&nbsp;the ability to manage risks within a Azure environment. To implement this&nbsp;so called&nbsp;policies can be created&nbsp;to enforce specific circumstances.  One of my&nbsp;previous post: “Define Azure Resource Manager Policies” describe&nbsp;what policies are. Some key features are:   Polices are&nbsp;a settings that are default set to allow. Policies are described by policy definitions in a policy definition language (if-then conditions). Polices are defined with JSON formatted files.  Policy Management UX  As of today policies can be managed within the &nbsp;Preview Azure Portal. Before this&nbsp;you needed to use PowerShell (If you prefer PowerShell you can read my previous post: “Define Azure Resource Manager Policies”). For a couple situations you still need to use PowerShell because certain functions are not yet available within Azure.  With the Policies UX you can view the policy assignments on two levels:   Subscription Resource Group  A option \"Policies\" is added to the settings blade of the specified levels. Clicking on the setting&nbsp;will show the assigned policies.    By clicking “Add assignment” you can add policy assignments. The policy UX&nbsp;offers&nbsp;a couple of build-in options:   SQL Server version 12.0 required. Allowed resource types. Not allowed resource types. Storage account SKU’s. Storage account encryption required. Virtual Machines SKU’s. Allowed locations.  The build-in options provide you with the ability&nbsp;to add the specific allowed items directly from the interface for example the SKUs of Virtual Machines.    &nbsp;  Besides the build-in policies the drop down also shows custom policies that are uploaded via PowerShell.  Conclusion  The policy management UX is a great starting point for doing more with policies. Policies are not used that often and this UX will help to get started. In the near future the UX should be extended because it is lacing some commonly used functionalities. Besides the UX I hope&nbsp;that they enhance the policy definitions. In my opinion some enhancements should be:   Better error messages within the portal, that reference the policies. A way to edit the policies within the portal. Management across subscriptions. Tagging with policy definitions. Uploading custom policies. Regular expression possibility within policies. This can come in use&nbsp;for naming conventions.  ","categories": ["Azure"],
        "tags": ["Azure","Management","Policy"],
        "url": "/2017/03/public-preview-of-the-policy-management-ux/",
        "teaser": null
      },{
        "title": "Azure Function CI &ndash; 2. Create a CI Build for the Azure Function",
        "excerpt":"This post is the second one in a series of three posts and will help you by creating a CI build for a Azure Function.  Prerequisites  This blog post is totally written using the Azure Function Visual Studio template. Information and the Azure Function tools download are listed below:   Finished Part 1 - Creating a Pre-Compiled Azure Function Azure Function Tools Information Visual Studio Team Services  Source Control  The solution created in the previous steps needs to be added and committed to Source Control. When it is added open Visual Studio Team Services in the browser and navigate to the build tab.  &nbsp;    &nbsp;  &nbsp;  When you do not have build within your project VSTS will show a large button to add a new build definition. Click this button (“New Definition” ) or use the smaller button within the top bar.    Next up is the template selection for the build definition. Select the Visual Studio Template.    This will create a Build definition with some default build steps. Save the the template as your build definition by clicking on “Save &amp; Queue”  &nbsp;    &nbsp;  A queue build window will open with the default settings. Keep these settings to create a release build for every CPU type.    You will notice that this build will fail. This is because the “.funproj” is not known by the build agent (mainly because the template is still in preview). You can notice this by the following error.  MSFTPlayground Function\\MSFT.Function\\MSFT.Function.funproj(8,3): Error MSB4019: The imported project \"C:\\Program Files (x86)\\MSBuild\\Microsoft\\VisualStudio\\v14.0\\AzureFunctions\\Microsoft.AzureFunctions.Props\" was not found. Confirm that the path in the &lt;Import&gt; declaration is correct, and that the file exists on disk.  The “.funproj” itself does not need to be build so to make the Build server run correctly we can and will exclude the project from the build. In order to get this done perform the following steps.   Open Visual Studio and right click the solution. In the menu click on “Configuration Manager” Deselect the build checkbox for your Functions project. Do this for the debug and release configuration.    &nbsp;  When the Function project is exclude from the build Commit the sources and queue the build again and you will see that this build will run successful.  Next post will be on: Deploying the Azure Function from the Build within VSTS.  ","categories": ["DevOps"],
        "tags": ["Azure","Azure Functions","CI","DevOps","VSTS"],
        "url": "/2017/03/azure-function-ci-2-create-ci-build-azure-function/",
        "teaser": null
      },{
        "title": "Azure Function CI – 3. Deploying the Azure Function from the Build within VSTS",
        "excerpt":"This post is the third and last one in a series of posts and will help you by deploying a CI build for a Azure Function.  Prerequisites  This blog post is totally written using the Azure Function Visual Studio template. Information and the Azure Function tools download are listed below:   Finished Part 2 - Create a CI Build for the Azure Function Azure Function Tools Information Visual Studio Team Services  In order to also make it deployable we need to take some steps.  Project changes  With all parts in place we can take the last steps to deploy the function. As there are still some small issues with the Preview Azure Function Tools for Visual Studio we need to use a ASP.Net Web Application project to create the deployment package. Right click the solution to add the new project of the type ASP.Net Web Application.    In the next screen click the empty ASP.Net Web Application template because we do not need all the extra features that are included in the other templates.    As mentioned above the project will be used to build a deployment package for the function. In order to get all files in place within the project we will configure a Pre Build event by opening the properties of the newly created project.  SET SOURCEPATH=$(SolutionDir)MSFT.Function\\MsftHttpTrigger SET FILEPATH=$(ProjectDir)MsftHttpTrigger  IF NOT EXIST \"%FILEPATH%\" ( GOTO MakeDir ) ELSE ( GOTO CopyFiles )  :MakeDir mkdir \"%FILEPATH%\" :CopyFiles xcopy /e /y \"%SOURCEPATH%\" \"%FILEPATH%\"  Now build the project, this will make sure all files are retrieved and placed within the project folder structure. When the build succeeds include the files within the project except the “project.json” and “project.lock.json”. By including the files within the project we make sure the files can be packaged by “msbuild”.&nbsp; In the functions folder also include a dummy “run.csx” file in order to use the precompiled function within the Azure Portal. Last step for now is to delete the “web.config” file.  The solution now looks as followed:    Now check / change the build order of the solution to make sure everything is build in the appropriate order to have the latest files in place within the deployment project.&nbsp; Right click the solution and click on “Project Build Order”.    Make sure the projects are build in the following sequence:   Azure Function Project Implementation Project Deployment Project  When we would package the project at this time we would get a correct package but would also have some files we do not want to include in our package. This is the package.config file and the default “bin” folder of a web project. In order to exclude them unload the project in Visual Studio by right clicking and selecting “Unload Project”. When the project is unloaded right click it and click on “Edit [Project file name]”    Add the following xml section within the configuration XML of the build configurations.  &lt;ExcludeFoldersFromDeployment&gt;bin&lt;/ExcludeFoldersFromDeployment&gt; &lt;ExcludeFilesFromDeployment&gt;package.config&lt;/ExcludeFilesFromDeployment&gt;   When added the Property groups will look like:  &lt;PropertyGroup Condition=\" '$(Configuration)|$(Platform)' == 'Debug|AnyCPU' \"&gt;   &lt;DebugSymbols&gt;true&lt;/DebugSymbols&gt;   &lt;DebugType&gt;full&lt;/DebugType&gt;   &lt;Optimize&gt;false&lt;/Optimize&gt;   &lt;OutputPath&gt;bin\\&lt;/OutputPath&gt;   &lt;DefineConstants&gt;DEBUG;TRACE&lt;/DefineConstants&gt;   &lt;ErrorReport&gt;prompt&lt;/ErrorReport&gt;   &lt;WarningLevel&gt;4&lt;/WarningLevel&gt;   &lt;ExcludeFoldersFromDeployment&gt;bin&lt;/ExcludeFoldersFromDeployment&gt;   &lt;ExcludeFilesFromDeployment&gt;package.config&lt;/ExcludeFilesFromDeployment&gt; &lt;/PropertyGroup&gt; &lt;PropertyGroup Condition=\" '$(Configuration)|$(Platform)' == 'Release|AnyCPU' \"   &lt;DebugType&gt;pdbonly&lt;/DebugType&gt;   &lt;Optimize&gt;true&lt;/Optimize&gt;   &lt;OutputPath&gt;bin\\&lt;/OutputPath&gt;   &lt;DefineConstants&gt;TRACE&lt;/DefineConstants&gt;   &lt;ErrorReport&gt;prompt&lt;/ErrorReport&gt;   &lt;WarningLevel&gt;4&lt;/WarningLevel&gt;   &lt;ExcludeFoldersFromDeployment&gt;bin&lt;/ExcludeFoldersFromDeployment&gt;   &lt;ExcludeFilesFromDeployment&gt;package.config&lt;/ExcludeFilesFromDeployment&gt; &lt;/PropertyGroup&gt;  Save the changes and reload the project by right clicking the it and selecting “Reload Project”. Commit all the changes you made and queue a new&nbsp; build.  Deploy Azure Function  When the build succeed the build template needs to be adjust in order to deploy the Azure Function. Edit the build template and add so called “msbuild” arguments to the build action of the solution to make sure the package is created when the solution is build. In order to achieve this open the Build template editor of your previously created build and click on the “Build solution” action to add the following build arguments.  /p:DeployOnBuild=true /p:WebPublishMethod=Package /p:PackageAsSingleFile=true /p:SkipInvalidConfigurations=true /p:PackageLocation=\"$(build.artifactstagingdirectory)\\\\\"    &nbsp;  Next up is adding a task to the build template for deploying the resources to Azure. This can be be done by adding the task:&nbsp; Azure App Services Deploy    Configure this build step with the correct subscription, and Azure resource (the function were you would like to deploy to). Make sure you set a different package location, so that the action can find the deployment package created within the previous action:  $(build.artifactstagingdirectory)/**/*.zip  &nbsp;    &nbsp;  Save and queue a new build. If this build succeeds your pre-compiled Azure Function is deployed to your Azure resource.  Note: This build template requires the Azure Function Host to be present within Azure if there is enough interest I will also create a series of post to deploy the Function by using ARM templates within the same pipeline.   Enable CI  Finally Continuous Integration needs to be enabled. To do this edit the build definition and click on the “Triggers” tab.    ","categories": ["DevOps"],
        "tags": ["ALM","Azure","Azure Functions","DevOps","VSTS"],
        "url": "/2017/03/azure-function-ci-3-deploying-the-azure-function-from-the-build/",
        "teaser": null
      },{
        "title": "Pass-Through Authentication with Azure Active Directory, Azure SQL, Azure API and a Console Application",
        "excerpt":"In situations you need to login to an application and use that identity to access an API (pass-through identity) and also get data from Azure SQL Server. With the new feature to use Azure Active Directory users within Azure SQL Server this is know is possible.  In this series of blog post I will cover the following topics:   Part 1 – Azure SQL Database with Azure Active Directory Authentication Part 2 – Azure API Application to query the Azure SQL Database Part 3 – Console application to call the API  The blog post will appear online in the upcoming weeks.  Prerequisites  During the series of blog post we will use Visual Studio 2017 and other applications.   Cloud Tools Azure Subscription  Complete source code can be found on GitHub at the end of Part 3.  ","categories": ["Azure"],
        "tags": ["App Services","Azure","Azure Active Directory","Azure SQL Server","Development","Identity"],
        "url": "/2017/04/pass-through-authentication-with-azure-active-directory-azure-sql-and-azure-api-application/",
        "teaser": null
      },{
        "title": "Global Azure Bootcamp - Pass-through authentication with an Azure API Application",
        "excerpt":"This weekend my employer Motion10 hosted one of the many Global Azure Bootcamp around the world. Where I presented about API Applications and .Net Core.  This year there where 244 confirmed event locations, that held a one-day deep dive class to help thousands of people get up to speed on developing solutions for Azure.  During this day I had the honor to present about Azure API Applications, and two colleagues of my presented about other parts of Azure: Tomasso Groenendijk (Azure MVP), Bobby Bregman and Jesse Gorter.  With this blog I want to share all the information I created and shared with the attendants of the event hoping to get you up to speed as well.  Labs  You can download a package at the bottom of this blog. In this package there are three labs:   Lab 1 - Getting started with the Azure API application Lab 2 - Extend the Azure API App to pass through authentication to Azure SQL Database Lab 3 - Configure Azure Active Directory and the API Lab 4 - Test Application  Lab 1  Lab 1 while give a short introduction on how to work with Azure API Applications and Visual Studio. During this lab you will create an Azure API Application and deploy source files from Visual Studio to Azure  Lab 2  Lab 2 extends the API application to pass-through the identity of the current user.  Lab 3  Lab 3 while configure Azure Active Directory to allow pass-through authentication  Lab 4  During Lab 4 creates a Test application to test the application  Labs  Presentation    Download  All content from the complete day can be downloaded from GitHub:  https://github.com/MaikvanderGaag/GAB-2017-Motion10  ","categories": ["Event"],
        "tags": ["Authentication","Azure","GlobalAzure"],
        "url": "/2017/04/global-azure-bootcamp-pass-through-authentication-with-an-azure-api-application/",
        "teaser": null
      },{
        "title": "Part 1 &ndash; Azure SQL Database with Azure Active Directory Authentication",
        "excerpt":"This post is the first post in a series of three posts and will help you with the creation of identity pass-through authentication from a client application to an API and then to an Azure SQL Database. In this post we setup an Azure SQL Database and enable Azure Active Directory authentication on the database.  Create Azure SQL Database  First step is to create a new Azure SQL Database.   Open the azure portal (https://portal.azure.com) Click on “New” and search for SQL Database.     Click on SQL Database. In the next blade that appears click “Create” In the “New” SQL Database blade fill in the correct properties. For this example you could use the AdventureWorks database. Wait for the Azure SQL Database deployment to be done.  Enable Azure Active Directory Authentication   With the Azure SQL Database that is created you also create an Azure SQL Server or you have chosen to use an existing one. Within the portal navigate to the Azure SQL Server. In the Azure SQL Server blade click on “Active Directory admin” under “Settings”     Click on \" “Set admin” in the “Active Directory Admin” blade.     Add the Active Directory user that you want to use as admin and click on “Select”. In the Active Directory Admin blade click on “Save” to save the settings.  Add user to the Azure SQL Database  With the Active Directory Admin set for the Azure SQL Server you are able to login to the SQL server with SQL Server Management Studio. On all client machines, from which your applications or users connect to Azure SQL Database or Azure SQL Data Warehouse using Azure Active Directory  users / applications, you must install the following software:   .NET Framework 4.6 or later from https://msdn.microsoft.com/library/5a4x27ek.aspx. Azure Active Directory Authentication Library for SQL Server (ADALSQL.DLL) is available in multiple languages (both x86 and amd64) from the download center at Microsoft Active Directory Authentication Library for Microsoft SQL Server.  You can meet these requirements by:   Installing either SQL Server 2016 Management Studio or SQL Server Data Tools for Visual Studio 2015 meets the .NET Framework 4.6 requirement. SSMS installs the x86 version of ADALSQL.DLL. SSDT installs the amd64 version of ADALSQL.DLL. The latest Visual Studio from Visual Studio Downloads meets the .NET Framework 4.6 requirement, but does not install the required amd64 version of ADALSQL.DLL.  If you have installed the software and you met the requirements take the following steps to add other users.   Make sure your Client IP address is added to the allowed addresses within the firewall of the Azure SQL Server. (Adding your Client IP to the Azure SQL server firewall) Open SQL Server Management Studio as Administrator. In the connect window fill in the Azure SQL Server and select “Active Directory Universal Authentication” as the authentication method.     Clicking on “Connect” will make a authentication dialog appear, fill in the right credentials (the credentials for the Active Directory Admin) and click on login. Open a new Query window in SQL Management studio and perform the following query.  Create user [User] from external provider   This query will add a User, Application or Group out of Azure Active Directory to the SQL Server Users. You can use the below SQL query to check if the user is added.  SELECT * FROM sys.sysusers;   By adding the user it  does not have read rights on the Azure SQL Database. Perform the next query to give the user the data reader role.  ALTER ROLE db_datareader ADD MEMBER [User]   You can now login with the added user by using SQL Server Management studio but make sure you set the initial database for the connection by clicking options.    If you do not configure the initial database you will get the following exception: Login failed for user “NT AUTHORITY\\ANONYMOUS LOGIN”. (Microsoft SQL Server, Error: 18456).    In the next post we will  create a Azure API Application to query the Azure SQL Database.  ","categories": ["Administration"],
        "tags": ["Azure","Azure Active Directory","Azure SQL Server","Identity"],
        "url": "/2017/04/part-1-azure-sql-database-with-azure-active-directory-authentication/",
        "teaser": null
      },{
        "title": "Part 2 &ndash; Azure API Application to query the Azure SQL Database",
        "excerpt":"This post is the second in a series of three posts and will help you with the creation of identity pass-through authentication from a client application to API and then to an Azure SQL Database. In this post we will create an Azure API Application with .Net Core to query the Azure SQL Database.  Previous Post :   Part 1 - Azure SQL Database with Azure Active Directory Authentication  Basics  To get everything up and running we will need to perform some basic tasks. These tasks include:   Create a .Net Core API application Setting up application settings. Adding HttpContext  Create a .Net Core API Application   Open Visual Studio 2017 Create a new project with the template: “ASP.Net Core Web Application (.Net Framework)”.  Note: We create a ASP.Net Core Web Application with .Net Framework because the needed libraries for this solution are not build yet. This means this application will only run on Windows-based machines.   Select the “Web API” option and make sure you enable authentication in “Work or School Accounts” mode and choose your Azure Active Directory domain. Check in the project properties if you are using the target framework “.Net Framework 4.6.2” if not set it to this version. Open the NuGet Package manager and add the following package:  Microsoft.IdentityModel.Clients.ActiveDirectory    Setting up Application settings and adding HttpContext.  With the empty project setup we can start by adding basic things we will need in our application. The first thing is application settings in order to authenticate against Azure Active Directory and the Azure SQL Resource.   Add a new class and call it for example “ApplicationSettings” in this class we will add the properties we need in our configuration file.  public string ClientId { get; set; }  public string ClientSecret { get; set; }  public string AadInstance { get; set; }  public string Domain  { get; set; }  public string Audience { get; set; }  public string TenantId { get; set; }  public string AzureSqlResource { get; set; }  public string ConnectionString { get; set; }  &nbsp;  Below is the description of the properties:     Property Description   ClientId The client id of the current application registered in Active Directory.   ClientSecret The secret of the current client application.   AadInstance The login URL of Azure Active Directory   Domain The Domain of Azure Active Directory   Audience The application resource App ID URL   TenantId The Tenant Id of Azure Active Directory   AzureSqlResource The resource Id of Azure SQL Databases, by default this is “https://database.windows.net”   ConnectionString The Connection String to out Azure SQL Database     To load these settings within the application we will also need to place them in the .Net Core application file “appsettings.json” in this file add the following section.  \"AuthenticationSettings\": {   \"AadInstance\": \"https://login.microsoftonline.com/\",   \"Audience\": \"{App ID URL Application}\",   \"ClientId\": \"{ClientId}\",   \"ClientSecret\": \"{ClientSecret}\",   \"Domain\": \"{Azure Active Directory Domain}\",   \"TenantId\": \"{Domain Tenant Id}\",   \"ConnectionString\": \"Data Source={Azure SQL Server};Initial Catalog={Azure SQL Database};Pooling=False;MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;\",   \"AzureSqlResource\": \"https://database.windows.net/\" }   Loading these settings need to our settings object need to be configured first.&nbsp;Therefor we need to adjust the “ConfigureService” method in the StartUp.cs file. In this method we also add the&nbsp;loading of the “HttpContextAccessor”, this is all done by using Dependency Injection.  public void ConfigureServices(IServiceCollection services) {     services.AddMvc();     //Loads the configuration section into out Authentication Settings Object     services.Configure&lt;ApplicationSettings&gt;(Configuration.GetSection(\"AuthenticationSettings\"));     //Make sure that we have a context within out ASP.Net Core Project     services.AddSingleton&lt;IHttpContextAccessor, HttpContextAccessor&gt;(); }   Because we do not want duplicate settings in our settings file we will adjust the \"Configure\" methods as well.  public void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory loggerFactory) {     loggerFactory.AddConsole(Configuration.GetSection(\"Logging\"));     loggerFactory.AddDebug();      app.UseJwtBearerAuthentication(new JwtBearerOptions     {         Authority = Configuration[\"AuthenticationSettings:AadInstance\"] + Configuration[\"AuthenticationSettings:TenantId\"],         Audience = Configuration[\"AuthenticationSettings:Audience\"]     });      app.UseMvc(); }   Move the values from the “Authentication” – “AzureAd” section in the “appsettings.json”file to the newly added “AuthenticationSettings” section and delete the complete “Authentication” section and save the file.  Setup a Authentication Helper  First up we will create a Authentication helper class that will handle the authentication actions. To get the identity of the logged in user we will retrieve the access token from the authorization header in the request.   Add a new class file to the project and call it “AuthenticationHelper”. Add the below method, this method will retrieve the access token from the authorization header and construct a “UserAssertion” (The UserAssertion object is a credential type representing user credentials) object that&nbsp;is needed to authenticate against Azure Active Directory.  private static UserAssertion GetUserAssertion(IHttpContextAccessor httpContextAccessor) {     UserAssertion retVal = null;      string accessToken = httpContextAccessor.HttpContext.Request.Headers[\"Authorization\"][0];     string userAccessToken = accessToken.Substring(accessToken.LastIndexOf(' ')).Trim();      Claim upn = httpContextAccessor.HttpContext.User.Claims.First(c =&gt; c.Type == \"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/upn\");      if (upn != null) {         retVal = new UserAssertion(userAccessToken, \"urn:ietf:params:oauth:grant-type:jwt-bearer\", upn.Value);     }      return retVal; }   With the UserAssertion object that represents the current user we can retrieve a AccessToken from Azure Active Directory for Azure SQL Databases.  public static AuthenticationResult GetAuthenticationResult(IHttpContextAccessor httpContextAccessor, ApplicationSettings authSettings) {      AuthenticationResult retVal = null;      UserAssertion userInfo = GetUserAssertion(httpContextAccessor);     ClientCredential clientCred = new ClientCredential(authSettings.ClientId, authSettings.ClientSecret);     AuthenticationContext authContext = new AuthenticationContext(authSettings.AadInstance + authSettings.Domain);      bool retry = false;     int retryCount = 0;      do {         retry = false;         try {             retVal = authContext.AcquireTokenAsync(authSettings.AzureSqlResource, clientCred, userInfo).Result;          } catch (AdalException ex) {             if (ex.ErrorCode == \"temporarily_unavailable\") {                 retry = true;                 retryCount++;             }         }     } while ((retry == true) &amp;&amp; (retryCount &lt; 1));      return retVal; } The token retrieved by this method&nbsp;will be used&nbsp;as an access token for our Azure SQL Database  Create a API controller to query the database   Add a new controller to the controller folder and add the following fields and constructor in order to have everything in place (settings and httpcontext).  private IHttpContextAccessor httpContextAccessor;  private ApplicationSettings authSettings { get; set; }  public CustomerController(IHttpContextAccessor httpContextAcc, IOptions&lt;ApplicationSettings&gt; settings) {     httpContextAccessor = httpContextAcc;     authSettings = settings.Value; }   Next up is implementing the “Get” method of the controller to retrieve the data from the SQL server.  [HttpGet] public JsonResult Get() {     JsonResult retVal = null;      AuthenticationResult authResult = AuthenticationHelper.GetAuthenticationResult(httpContextAccessor, authSettings);      if (authResult != null) {         string queryString = \"SELECT * FROM SalesLT.Product\";          using (SqlConnection connection = new SqlConnection(authSettings.ConnectionString)) {             connection.AccessToken = authResult.AccessToken;             try {                 connection.Open();                 SqlCommand command = new SqlCommand(queryString, connection);                 SqlDataAdapter adapter = new SqlDataAdapter(command);                  DataTable table = new DataTable();                   adapter.Fill(table);                  retVal = new JsonResult(table);              } catch (SqlException ex) {             }         }     }     return retVal; }  In the method the access token is retrieved by the authentication helper and passed through to the connection (the “AccessToken” properties is only available in the .Net Framework 4.6.2 or higher) then a select all query is performed on the SQL database to retrieve all the data and return it as a JSON string.  Delegate Permissions  ","categories": ["Azure"],
        "tags": ["API","Azure","Azure App Services"],
        "url": "/2017/05/part-2-azure-api-application-to-query-the-azure-sql-database/",
        "teaser": null
      },{
        "title": "Attending Microsoft Build 2017",
        "excerpt":"I love attending conferences that contain great content and especially when this content is presented by Program Managers or Evangelists from Microsoft. During the last years I have attended a few conferences. The best conferences I attended was TechEd 2014 in Barcelona.  Last year I also attended Build in San Francisco, from which I came back a little disappointed especially the venue disappointed me.  Seattle  This year Build is held in Seattle, moving to Seattle made me decide to attend Build for the second time.  “Build is an annual conference event held by Microsoft, aimed towards software and web developers using Windows, Windows Phone, Microsoft Azure and other Microsoft technologies. First held in 2011, it serves as a successor for Microsoft's previous developer events, the Professional Developers Conference (an infrequent event which covered development of software for the Windows operating system) and MIX (which covered web development centering on Microsoft technology such as Silverlight and ASP.net).”  Registering for Build was a lot easier that last year. Last year we were placed on the waiting list and needed to wait for a month if we could join Build.  This year Build will kick off on May 10. I will be traveling to Seattle on May 6th and will enjoy some free time in Seattle.  I'm very exited and I'm expecting a lot off great content. On this blog and on twitter I will keep you posted on announcements (#MSBuild).  If you’re at Build and you follow me, be sure to send me a message or tweet and we’ll meet up!  Microsoft (Live)  “Software continues to transform the world in remarkable ways and developers are at the center of it. At Microsoft Build 2017 we invite you to join us in downtown Seattle to learn about latest new technologies and exciting plans on the horizon.  As always, Microsoft Build is filled with strong technical sessions as well as opportunities to meet and learn from others in the industry. We’ll make sure the schedule is filled with solid content, and will strive to deliver some fun surprises along the way.  Not able to attend Microsoft Build? Keynotes and select sessions will be streamed live here!”  ","categories": ["Event"],
        "tags": ["Build","MSBuild"],
        "url": "/2017/05/attending-microsoft-build-2017/",
        "teaser": null
      },{
        "title": "Part 3 &ndash; Console application to call a API with Azure Active Directory Authentication",
        "excerpt":"This post is the third and last in a series of three posts and will help you with the creation of identity pass-through authentication from a client application to a API and then to an Azure SQL Database. In this post we will create a console application to query the API published in Azure.  Previous Posts:   Part 1 – Azure SQL Database with Azure Active Directory Authentication Part 2 – Azure API Application to query the Azure SQL Database  Add new Application to Azure Active Directory  In order to call our API we need to have a registered application within Azure Active Directory that has delegated permissions for the API application.   Navigate to the Azure Portal (https://portal.azure.com/) Open the “Azure Active Directory” blade. In the blade click on “App Registrations”. In the “App Registrations” blade click on “New application registration”. Fill in a Name (like ConsoleApplication), Application Type: Native and Redirect Url: https://localhost. With everything filled in click on “Create”.     With the application registration created click on the registered application in the application list. In this window copy the ClientId of the application and click on “Required permissions”.     Click “Add” in the Required permissions blade to give the console application delegated permissions on the API we created. In the “Select an API” search for your created API application and select it. The permission step will open, make sure you select your application under “Delegated Permissions” and click “Select”. In the steps blade click “Done”.  Create console application   Open Visual Studio and create a new Console Application. Open the NuGet Package manager and add the following package:  Microsoft.IdentityModel.Clients.ActiveDirectory – Version: 2.22.302111727    Note: Make sure you install version: 2.22.302111727. This version contains the option to prompt for user credentials.  Client authentication with authentication prompt   In the “Program.cs” file add the below methods.  private static void TestApi(string url) {      var authResult = GetToken();     string token = authResult.AccessToken;     if (token != null) {          HttpClient client = new HttpClient();         client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", token);          var response = client.GetAsync(new Uri(url));          string content = response.Result.Content.ReadAsStringAsync().Result;         Console.WriteLine(content);     } }  private static AuthenticationResult GetToken() {      string aadInstance = \"https://login.windows.net/{0}\";     string ResourceId = ConfigurationManager.AppSettings[\"ResourceId\"];     string tenantId = ConfigurationManager.AppSettings[\"TenantId\"];     string clientId = ConfigurationManager.AppSettings[\"ClientId\"];     string replyAddress = ConfigurationManager.AppSettings[\"ReplyAddressConfigured\"];     AuthenticationContext authenticationContext =       new AuthenticationContext(string.Format(aadInstance, tenantId));      AuthenticationResult authenticationResult = authenticationContext.AcquireToken(ResourceId, clientId, new Uri(replyAddress), PromptBehavior.RefreshSession);      return authenticationResult; }   The methods will call the API with the URL as specified in the “url” parameter but also retrieve a token for the user that logs in to the prompt. The&nbsp;“GetToken” method makes sure this prompt is shown by the \"PromptBehavior\" enumeration.   With the methods in the “Program.cs” file we need to adjust the main method.  static void Main(string[] args) {      string url = ConfigurationManager.AppSettings[\"Url\"];      if (string.IsNullOrEmpty(url)) {         Console.WriteLine(\"Please fill in your URL:\");         url = Console.ReadLine();     }      Console.WriteLine(\"Calling url: \" + url);      TestApi(url);     Console.WriteLine(\"Done processing, press any key to close....\");     Console.ReadKey(); }    As you may have seen in the previous methods we will save the settings in&nbsp;the configuration file. Open the file and add those settings.  &lt;appSettings&gt; ","categories": ["Development"],
        "tags": ["API","Azure App Services","Azure SQL Server","Identity"],
        "url": "/2017/05/part-3-console-application-call-api/",
        "teaser": null
      },{
        "title": "Build 2017 announcements that effect your work",
        "excerpt":"A couple of minutes ago the keynote of Build 2017 ended. The keynote contained a lot of announcements that I was not expecting.  IoT Edge  Microsoft announced Azure IoT Edge, which is technology that's meant to extend devices. It's a cross-platform runtime that runs on both Windows and Linux, and it will work on devices that are smaller than a Raspberry Pi. Devices would normally send telemetry data to an IoT hub, and analytics would be sent back to the device. Now, all of this can be handled by Azure. Logic for these devices can be developed and tested in the cloud, and then it can be packaged into a Docker container and exported to the machine itself.  This technology certainly will effect the way you will handle information from and to you devices within an IoT scenario.  Visual Studio 2017  Main announcements for Visual Studio 2017 were:   VS2017 support for Azure Functions and Logic Apps Azure Application insights for Azure Functions  The support for Azure Functions within VS2017 and Application Insights will make it a lot easier to integrate it within a Continuous Delivery Pipeline and watch your serverless applications.  Azure  A lot of the time during the keynote was used to talk about Azure, what is totally logical as the mission of Microsoft still is Cloud and Mobile first. The new things for Azure were:   Azure Cloud Shell: Bash shell within the Azure Portal, PowerShell will follow soon. Azure Mobile Application for IOS and Android Visual Studio Snapshot debugging for Azure (demo was from within the Application map within Azure Application Insights) Visual Studio for the MAC Azure Database Migration Service: Service for the migration of existing on premise databases (also Oracle) to Azure SQL Server. PostgreSQL as a Service MySQL as a Service  Azure Cosmos DB  The Azure Cosmos DB is a new service that is a globally distributed database service that is multi model and multi Api. Some keys fact about this service are:   Globally Distributes: Locations can be altered from the Azure Portal Multi model  Key-Value Document Column Family Graph   Multi API  DocumentDB MongoDB Table Storage Gremlin Graph Spark   Scale to any need Comprehensive SLA  Cognitive Services  Microsoft is getting better the last years with Artificial Intelligence and services to support this like the cognitive services. For the cognitive services there were some announcements regarding new Api’s. The most important one was the possibility for custom solutions. With custom solutions you can think of learning software for image recognition for example called the Custom Vision Service. The demonstration that was given during the keynote was about the recognition of leaf types.  &nbsp;  All of these announcements will affect our work and how we will do our work and certainly how we design our solutions.  ","categories": ["Event"],
        "tags": ["Azure","MSBuild","Serverless","VS2017"],
        "url": "/2017/05/build-2017-announcements-that-effect-your-work/",
        "teaser": null
      },{
        "title": "The Build announcement I got from Build 2017 Day 1",
        "excerpt":"This year is my second year at Build. Most announcement during Build are given on Day 1. This blog post is a summary of announcement that I heard of during my sessions.  The sessions I followed on day 1 are:   Keynote DevOps for every Team Asp.Net Core 2.0 Continuous Delivery on Microsoft Azure Azure Compute: New Features and roadmap  Keynote  Information about the keynote and the announcements made their I have already shared in the following blog post:  Build 2017 announcements that effect your work  DevOps for every Team  DevOps was one of the buzz words of Build 2017. At the keynote every attendee got a folder regarding the DevOps sessions. This session was not a real DevOps sessions but showed more the capabilities Continuous Integration / Continuous Delivery options within Visual Studio Team Services. Most of the functionalities for this are build completely within the Azure Portal itself.  Key facts from this session were:   From the Continuous Delivery tab within the Azure Portal you can now also create a new VSTS account and setup Continuous Delivery. You can branch within Git version control from a work item. WhiteSource extension for build templates that does an Open Source Security check. Create a new Git version control project within Visual Studio Team Services based on a Team Foundation Source Control. New Release Pipeline visualization. Package management support. Deployment groups to support deploying to machines within the same role.  This functionality also adds the ability  to set to which machines the new version needs to be deployed to check the release,   Release Plans for VSTS is General Available and is included within your subscription  Asp.Net Core 2.0  The Asp.Net Core 2.0 sessions was a really good and funny session from Scott Hanselman and Daniel Roth about the announced preview of Asp.Net Core 2.0. Within this session they showed the new capabilities and possibilities of .Net Core 2.0. Executable can be downloaded from the following place:  https://www.microsoft.com/net/core/preview#windowscmd  Note: Installation Preview 3 of Visual Studio 2017 can be done side by side your production version.  &nbsp;  Key facts were:   More functionality build in to the runtime of .Net Core 2.0 examples are:  Diagnostics logging. Configuration, can be added by default dependency injection.   Preview of .Net Core 2.0 25% faster than the 1.1 version. Improvements are mostly made during Startup. Razor Pages can be created and used within .Net Core 2.0. Environment configuration. For example for your configuration files. (appsettings.dev.json) Azure Active Directory B2C integration.  Continuous Delivery on Microsoft Azure  The session was about the Continuous Delivery options for Microsoft Azure. Options that were shown are the default options that are within Visual Studio 2017, command line and VSTS. The other features and announcements were more summarized and described shortly below.     Azure IaaS DevOps with VSTS   Deployment Groups  Agent Based deployments based on tags   Scale set deployment  VHD based deployments using Packer. Integration into the Azure Portal //Coming soon   Azure Key Vault //Coming soon  Consume keys during your deployment    &nbsp;  Besides the above announcement Microsoft is also focusing on Container DevOps and announced a lot of things as you can see in the below picture.    Another great announcement that will be coming soon was Pipeline as Code this basically is a definition file of your release pipeline based on YALM.  Azure Compute: New Features and roadmap  During this session new features and roadmap information was given about everything about Azure Compute divided into three sections: Infrastructure, Containers and Platform, were mainly two of them were very important to my opinion.  Infrastructure  A lot of new IaaS sizes were announced were two of them were special. The Dv3 en Ev3 were discussed and shown especially because they support nested virtualization. Nested virtualization is the virtualization of machines within a virtual machines. Besides that Corey Sanders gave a demonstration of Azure PowerShell within the Azure Portal that is coming soon and is in preview. The best feature of Azure PowerShell within the Azure Portal was the ability to browse your Azure subscriptions and resources in a file based format using the “dir” and “cd” commands. If your would like to sign up for the preview go to:   https://aka.ms/PsCloudSignUp  Another announcement was a new service called: “Instance Metadata Service” that runs on the virtual machines within Azure that is a rest Api on: https://169.254.169.254 and shows information of the machine for example maintenance windows and scheduled reboots.   Platform  An announcement for platform was about Azure batch called Low Priority Batch VMs which are evitable instances which will give you to 80% discount. It really means that Microsoft can get back instances of that you define as Low Priority Batch VMs in order to use for other type of usage that will result in a large discount for you.  The preview of the Azure Function Runtime was also an announcement that makes it possible to run the Azure Function runtime almost everywhere like within Azure IoT Edge. You can download it from here: https://aka.ms/azafr  Last but not least they announced a feature that Seals azure services as a sealed app in other to distribute  them and keep management over the applications that is called Azure Managed Applications (should be available today).  ","categories": ["DevOps","Event"],
        "tags": [".Net Core","CD","CI","DevOps","MSBuild","VSTS"],
        "url": "/2017/05/build-announcement-got-build-2017-day-1/",
        "teaser": null
      },{
        "title": "Extensions and Tips for deploying with Azure Resource Templates",
        "excerpt":"Working with Azure Services in different subscriptions means that the Azure Services need to run in different subscriptions. This often occurs when we talk about DTAP environments (Development, Test, Acceptance and Production). Making sure the resources run on different subscriptions can be done by hand but that is very time-consuming task, this is where the Azure Resource Manager comes in.  Azure Resource Manager  The Azure Resource Manager is a technology within Azure that makes deployments and actions within Azure tools consistent. It is the layer that the tooling around Azure uses (Portal, Command Line and Visual Studio) to do tasks within, the “New” Azure Portal. This portal is fully build on top of the Azure Resource Manager.  &nbsp;    &nbsp;  Deployments of Azure services that have associations and the same lifetime are mostly put in the same Resource Group. A Resource Group is a logical container of Azure Services that you would like to manage together,  deploying a complete Resource Group with multiple services needs to be done using the Resource Manager and Resource Templates.  Visual Studio  In this blog post we will not get into creating complete resource templates but we will mention how to get started together with Visual Studio. Within Visual Studio you get started by creating a project with the template called “Azure Resource Group”.    Creating a project with this template will give you three files:   ResourceTemplate.parameters.json: Parameters file for the resource template. ResourceTemplate.json : The resource template. Deploy-AzureResourceGroup.ps1: PowerShell script that will get you started with deploying the resource template.  As mentioned above the “ResourceTemplate.json” file is the real resource template. In this file all Azure resources are defined but also the configurations of those Azure resources. Visual Studio offers a viewer for the resource template file called the “JSON Outline”.    As you can see in the above screenshot the “JSON Outline” displays the structure of the resource template.   Tip  Add a tag called “displayName” to the resource file in the Azure resource as in the below snip-it. With this tag the “JSON Outline” will use this name to display the resource in the viewer instead of the “name” property that is usually a non saying reference to a specific parameter.  {   \"apiVersion\": \"[variables('defaultApiVersion')]\",   \"name\": \"[parameters('api_Name')]\",   \"kind\": \"api\",   \"type\": \"Microsoft.Web/sites\",   \"location\": \"[variables('location')]\",   \"tags\": {     \"displayName\": \"InternalAPI\"   },   \"dependsOn\": [     \"[concat('Microsoft.Web/serverfarms/', parameters('hosting_Plan_Name'))]\"   ],   \"properties\": {     \"name\": \"[parameters('api_Name')]\",     \"serverFarmId\": \"[resourceId('Microsoft.Web/serverfarms', parameters('hosting_Plan_Name'))]\",     \"siteConfig\": {       \"AlwaysOn\": true     }   } }  PowerShell  The PowerShell added to the project is a great starting point to start with the deployment. But as mentioned above accounts are mostly associated with multiple Azure Subscriptions. The script does not support this situation and will always deploy to the default subscription that is associated with your account.  To support this scenario add a line of PowerShell to make sure that the user needs to login.  #Login to the Azure Resource Manager Login-AzureRmAccount   This line will open the login window to the Azure Resource Manager. Next we will retrieve all subscriptions the user is associated to.  Write-Host \"---------------------------------------------------------------------\" Write-Host \"Your current subscriptions: \" -ForegroundColor Yellow Get-AzureRMSubscription  The “Get-AzureRMSubscription” will display specific information about all the subscriptions the user is associated with. Information that is shown is the subscription id and subscription name. To set the context of the Azure Resource Manager to the right subscription the following lines need to be added.  Write-Host \"Enter the Subscription ID to deploy to: \" -ForegroundColor Green $sub = Read-Host  Set-AzureRmContext -SubscriptionId $sub clear The above lines will ask the user to fill in the subscription id of the subscription were they would like to deploy the Azure Resource Template. The function “Set-AzureRMContext” will then make sure that the context of the resource manager is set to the correct subscription.  ","categories": ["Azure DevOps"],
        "tags": ["Azure","Powershell","Resource Manager"],
        "url": "/2017/05/extensions-and-tips-for-deploying-with-azure-resource-templates/",
        "teaser": null
      },{
        "title": "Configure access to a private network for a Azure App Services",
        "excerpt":"On-Premise connections for Azure App Services can be created by using Hybrid Connections. Hybrid connections do not need any development or re-configuration of your application. It only needs a small client service in the private network (downloadable from the Azure Portal) and the hybrid connection configured in the Azure Portal.  Hybrid Connection  Hybrid Connections give an easy and fast way to connect to on-premise resources for Azure App Services and Azure Mobile Apps.    Hybrid connections can be used in different scenario's:   .NET framework access to SQL Server .NET framework access to HTTP/HTTPS services with WebClient PHP access to SQL Server, MySQL Java access to SQL Server, MySQL and Oracle Java access to HTTP/HTTPS services  The connections are secured using  Shared Access Signature (SAS) authorization between Azure applications and the on-premises Hybrid Connection Manager to the Hybrid Connection. Separate connection keys are created for the application and the on-premises Hybrid Connection Manager. These connection keys can be rolled over and revoked independently.  Hybrid Connections provide for seamless and secure distribution of the keys to the applications and the on-premises Hybrid Connection Manager.  The previous Hybrid Connections (classic) relied on BizTalk Services and used multiple TCP ports for connectivity and were susceptible to firewall issues and performance issues. The new Azure Relay Hybrid Connections use web sockets and communication takes place over a single port; 443.  Configuration  A great advantage from hybrid connections is that the client application does not need to be altered, as an example you can use the same connection string as you would when deploying the application in the private network. In order to set up a hybrid connection follow the below steps:   Open the Azure Portal and find the application that needs to be connected to the private network. In the application blade click on “Networking”.     In the networking blade click on “Configure your hybrid connection endpoints”. This will open the hybrid connections blade,  in this blade select “Add Hybrid Connection”.     The “Add Hybrid connection” blade will show a list of all available hybrid connections within your Azure Subscription. If you do not have a hybrid connection you will be able to add a new hybrid connection. To do this select “Create new Hybrid Connection”.     In the “Create Hybrid Connection” blade the correct information for the hybrid connection needs to be filled in.       Property Description   Endpoint Name The endpoint name for the hybrid connection.   Endpoint Host The  hostname of the on-premise system.   Endpoint Port The port for the on-premise connection   Location Location for the servicebus used for the hybrid connection   Name Name for the servicebus used for the hybrid connection.    &nbsp;   With all the correct values entered click on “Create” to create the hybrid connection. When the hybrid connection is created it will show up in the list of all available hybrid connections. In this blade select the hybrid connection you want to use and click on “Add selected hybrid connection”.     Adding the hybrid connection is not the last step. In order to make it work a small application needs to be installed on a on-premise system to route the traffic. To download this application click on “Download connection manager” in the hybrid connection blade.     When the connection manager is installed it will ask for a connection string to the hybrid connection configured in the Azure Portal. This connection string can be found on the hybrid connection detail page when you click on it from the hybrid connection overview blade.     If the connection manager is correctly configured it will show a connected status on the hybrid connection overview page.  &nbsp;  Note: Make sure you replace all classic hybrid connections by 31-05-2018 because Microsoft then stops with Azure BizTalk services: https://azure.microsoft.com/en-au/updates/azure-biztalk-services-simplifying-our-azure-offerings/  ","categories": ["Azure"],
        "tags": ["Azure","Azure App Services","Hybrid Connection"],
        "url": "/2017/06/configure-access-to-a-private-network-for-a-azure-app-services/",
        "teaser": null
      },{
        "title": "Restricting access to your Azure Web Application",
        "excerpt":"As you may know almost everything that is deployed to Azure is publicly available. As with Azure SQL Database you do not have a firewall available for Azure Web Applications. That means other options need to be used to restrict access to Azure Web Application.  Restrict access by IP  A possible option is to restrict access to your application by IP addresses. The IP addresses can be added as a allowed IP address within the web.config of your application. All other IP addresses will get a 403 Forbidden response from Azure.  &lt;system.webServer&gt;     &lt;security&gt;       &lt;ipSecurity allowUnlisted=\"false\"&gt;         &lt;clear /&gt;         &lt;add ipAddress=\"99.99.99.99\" allowed=\"true\" /&gt;       &lt;/ipSecurity&gt;     &lt;/security&gt; &lt;/system.webServer&gt;   Restrict access for specific Users  Another option is to restrict access by enabling Authentication on the web application. This can be done for several Authentication Providers like: Azure Active Directory, Google, Facebook, Twitter and Microsoft. The below steps will help you with the configuration of Azure Active Directory as a authentication provider.   In the Azure Portal navigate to the blade of the web application.     Click on “Authentication/Authorization” and select “On”. Activating this option will give you several options for Authentication Providers. For know we will select Azure Active Directory.     Because there isn’t a pre-configured application select the “Express” option. This option will register the Enterprise application within Azure Active Directory for us, or let you select a existing.     Clicking “Save” on this blade will register the application within Azure Active Directory. From there users can be granted access to the application. To grant users access to the application open the Azure Active Directory blade within the Azure Portal and select Enterprise Applications.     In the Enterprise Applications blade select “All Applications” to see a list of all applications that are registered within Azure Active Directory.     From this list select the application. This will open the blade of the specific application. In the blade select “Users and Groups”.     In the “Users and Groups” blade all users are shown that are granted access to the application. From here you can a add users to give them access.  Restrict Crawling  When you are developing or testing a site that has anonymous content you probably want the content not to be crawled by spiders or bots because many traffic can come from bots and spiders. Stopping the crawling can be done by placing a file called “robots.txt” at the root of your web application with the following content.  #robots.txt User-agent: *   Disallow: /   ","categories": ["Azure"],
        "tags": ["Azure","Azure Web Apps"],
        "url": "/2017/06/restricting-access-to-your-azure-web-application/",
        "teaser": null
      },{
        "title": "Removing the Classis Hybrid Connections from Azure (Azure BizTalk Service)",
        "excerpt":"As you know the classic hybrid connections that are build upon Azure BizTalk Services are deprecated. These connection will have to be replaced by the new hybrid connections based upon Azure Relay Services   Configure access to a private network for an Azure App Services  Replacing the connections will need to take place before 31 may 2018. Last week I was performing these actions for a client and noticed that I had a BizTalk service (new portal) in one of our production resource groups.  &nbsp;  To clean up nicely I replaced the hybrid connections and wanted to drop the BizTalk service, in the new portal this wasn’t possible (It only redirects you to the old portal) and clicking on it navigated me to the old portal (https://manage.windowsazure.com). For some reason the portal wasn’t able to load correctly so I still wasn’t able to drop the service.  &nbsp;  Since deleting the resource group isn’t a option (we had other azure services within the group) I though of another solution that I wanted to share with you.  Steps   Navigate to : https://resources.azure.com and login with the Azure Administrator and make sure you choose the right subscription.    https://resources.azure.com will open the Azure Resource Explorer which is a website/tool that you use to explore the Azure Resource Manager APIs and do API calls on your Azure subscription.   With the Azure Resource Explorer open navigate to the subscription and find your Resource group in which the Azure BizTalk service resides.     Click open the providers and look for the provider “Microsoft.BizTalkServices”. This provider has al the BizTalk Services in the resource group. Click on the BizTalk service that need to be removed and click on it.     Information of the Azure BizTalk Service is loaded and in the top bar you are able to open a “Actions” tab. Opening the tab will give you the option to delete the classic BizTalk Service from your Azure Subscription.     Click on “Delete” to do the delete API call. Processing the call will take a couple of seconds and after that the Azure BizTalk Service is removed.  ","categories": ["Azure DevOps"],
        "tags": ["ARM","Azure","BizTalk"],
        "url": "/2017/08/removing-classis-hybrid-connections-azure-azure-biztalk-service/",
        "teaser": null
      },{
        "title": "Azure Event Grid with Custom Events",
        "excerpt":"As of yesterday (16-8-2017) the public preview of Azure Event Grid is live. Azure Event Grid is a fully managed event routing service. Azure Event Grid greatly simplifies the development of event-based applications and simplifies serverless workflow creation. Using a single service, Azure Event Grid manages all routing of events from any source, to any destination, for any application.  Microsoft explains Azure Event Grid as follows:  “Azure Event Grid allows you to easily build applications with event-based architectures. You select the Azure resource you would like to subscribe to, and give the event handler or WebHook endpoint to send the event to. Event Grid has built-in support for events coming from Azure services, like storage blobs and resource groups. Event Grid also has custom support for application and third-party events, using custom topics and custom webhooks. ”  As explained above there is support for custom topics and webhooks. Setting up custom events needs the following within Azure.  &nbsp;    The Event Grid Topic is the Azure Service were the events are sent to, and the Subscriptions are the applications that need to receive the events. So let's get this to work within Azure and Visual Studio.  Create the Event Grid Topic  First create the Event Grid Topic in Azure. With this topic we know were we need to send the events.   Open the Azure Portal (https://portal.azure.com) Click on the “+” icon and search for “Event Grid Topic”. Click on “Create” to create the topic. Fill in the correct information and click on “Create”    You now have a Topic were events can be send too.  Sending Events to the Event Grid Topic  With the Topic in place the events can be send. In order to send events we need two things: the endpoint and the key from the topic. Both things are available on the Azure Event Grid Topic Blade.   Open the Event Grid Topic Blade to find the Endpoint and Key of the Topic.    The Endpoint is used to Post a request to and The key as an authentication to the topic and is placed in the header with the key: “aeg-sas-key”.   The event itself needs should be in the form of a json message with specific properties. On the documentation site of Azure Event Grid you will find information on the schema: Event Grid event schema. Good to notice on the schema and the example provided is that the event is an array.  [   {     \"topic\": \"/subscriptions/{subscription-id}/resourceGroups/Storage/providers/Microsoft.EventGrid/topics/myeventgridtopic\",     \"subject\": \"/myapp/vehicles/motorcycles\",         \"id\": \"b68529f3-68cd-4744-baa4-3c0498ec19e2\",     \"eventType\": \"recordInserted\",     \"eventTime\": \"2017-06-26T18:41:00.9584103Z\",     \"data\":{       \"make\": \"Ducati\",       \"model\": \"Monster\"     }   } ]    subject string Path to the event subscription it is used for filtering and routing the requests   eventType string The event type   eventTime string The time the event is generated based on the provider's UTC time. During testing I  needed to give a date time offset.   id string Unique identifier for the event. Can be a Guid provided as string value.   data object Your specific event data.     Open Visual Studio and Create a new Console Application Project. In this project add the below code.  static void Main(string[] args) {     var eventNew = MakeRequestEvent();     eventNew.Wait();                 Console.WriteLine(eventNew.Result.Content.ReadAsStringAsync().Result);      Console.ReadKey(); }  private static async Task&lt;HttpResponseMessage&gt; MakeRequestEvent() {     string topicEndpoint = \"[endpoint]\";     var httpClient = new HttpClient();     httpClient.DefaultRequestHeaders.Add(\"aeg-sas-key\", \"[key]\");      List&lt;CustomEvent&lt;Account&gt;&gt; events = new List&lt;CustomEvent&lt;Account&gt;&gt;();     var customEvent = new CustomEvent&lt;Account&gt;();     customEvent.EventType = \"eventgridtest\";     customEvent.Subject = \"/msft/test\";     customEvent.Data = new Account() { Name = \"Maik\", Gender = \"Male\" };     events.Add(customEvent);      string jsonContent = JsonConvert.SerializeObject(events);     Console.WriteLine(jsonContent);      var content = new StringContent(jsonContent, Encoding.UTF8, \"application/json\");     return await httpClient.PostAsync(topicEndpoint, content);             } The code above make an http post call to the topic endpoint with the key in the “aeg-sas-key” header.  The event (json) is placed in the body which is a serialized version of the CustomEvent object I created myself. This object has the default properties for an event. For the data every object could be used as it based on generics. The Accounts class used is a basic class with only a Name and Gender property.  public class CustomEvent&lt;T&gt; {      public string Id { get; private set; }      public string EventType { get; set; }      public string Subject { get; set; }      public string EventTime { get; private set; }      public T Data { get; set; }      public CustomEvent(){         Id = Guid.NewGuid().ToString();          DateTime localTime = DateTime.Now;         DateTime utcTime = DateTime.UtcNow;         DateTimeOffset localTimeAndOffset = new DateTimeOffset(localTime, TimeZoneInfo.Local.GetUtcOffset(localTime));         EventTime = localTimeAndOffset.ToString(\"o\");      } } If the event is posted successfully the response will be empty.  Subscribing application  The subscribing application will be a simple web application with one API method. This method will add a custom event in Application Insights.   Create a new project within Visual Studio make this a ASP.Net Web application. Use a API project as the template for the application. Add the Application Insights nuget package. Add a EventsController and add the below code.  public class EventsController : ApiController {     // POST api/events     public void Post([FromBody]List&lt;CustomEvent&lt;Account&gt;&gt; value) {         foreach(var eventValue in value) {             string data = JsonConvert.SerializeObject(eventValue);             Dictionary&lt;string, string&gt; allData = new Dictionary&lt;string, string&gt;();             allData.Add(\"event id\", eventValue.Id);             allData.Add(\"eventdata\", data);              TelemetryClient client = new TelemetryClient();             client.TrackEvent(\"Event Received\", allData);         }     } } This method reads the event from the body and places the data within Application Insights with the name Event Received.   Publish the application to Azure and remember the endpoint of the API method [url]/api/events.  Configure the Event Grid Subscriber  With all applications in place the Event Grid Topic needs to know were to send the events to. For this an Event Grid Subscriber needs to be configured.   Open the Event Grid Blade and click on Event Subscription.     Fill in the correct properties. The EventType property can be used to filter on the eventtype of the event. The prefix and suffix properties can be used to filter on the subject.     Click on create to create the subscription.    The Test  With everything in place we can start using the Event Grid.   Fire up the console application the application will give you the json message as output. If something goes wrong during the event you will see the error message within the console window. ","categories": ["Development"],
        "tags": ["ASP.net","Azure","Event Grid"],
        "url": "/2017/08/azure-event-grid-custom-events/",
        "teaser": null
      },{
        "title": "Protect your master Branch in VSTS",
        "excerpt":"You can set up permissions and policies to control who can read and update code in a branch on your Git repo within VSTS. You can set permissions for individual users and groups, and inherit and override permissions as needed.  &nbsp;  Within regular projects you want to set up specific permissions and policies on your master branch. Only specific users should add code to your master branch. This is done by permissions and policies  &nbsp;  Adjust security on your master branch  To remove access to a branch we can remove the permissions for the developers. In default situations your developers will reside in the default “Contributors” group. First step is to remove the inheritance from the branch because branch security is set up with inheritance.  &nbsp;   Go to your project within VSTS Click on the settings icon and then Version Control Select the branch you want to stop the inheritance of. Click on inheritance and select “off”.     With the inheritance off VSTS security groups can be removed from the branch.    &nbsp;  Branch Policy  If the permissions are setup correctly on the branch a policy needs to be configured in the branch in order for developers to be able to push code to the master branch, and that approval is configured on the so-called pull request.   Go to your project within VSTS Click on the settings icon and then Version Control Select the branch Select “Branch Policies”    &nbsp;   On the branch policy page setup the policy as you want, you should for example configure a required approver. I also love the feature of a linked work items that are required.    ","categories": ["Azure DevOps"],
        "tags": ["Configuration","Security","VSTS"],
        "url": "/2017/08/protect-master-branch-vsts/",
        "teaser": null
      },{
        "title": "Listing Azure Services within a CSV file",
        "excerpt":"In some situations you will look into a current Azure Environment and the setup/governance of it and need to migrate or move resources around.  The below script will help you with exporting the Azure resources into a CSV file, from that CSV file you can join the data within Excel to make plans.  Script  &nbsp;  ################################################################################### ## ## PowerShell script for exporting Azure Resources within a subscription. ## Creator: Maik van der Gaag ## ###################################################################################   Login-AzureRmAccount $path = Read-Host \"Enter the full path to save the export file to\"  $subsciptions = Get-AzureRmSubscription  Write-Host \"Subscriptions\" Write-Host \"--------------\" foreach($sub in $subsciptions){      Write-Host ($sub | Select -ExpandProperty \"Name\") }  Write-Host \"\"  $name = Read-Host \"Please enter the subscription names for which you want to export the Azure Services devided by (,)\"  $names = $name.Split(\",\");  foreach($subName in $names){ ","categories": ["Azure"],
        "tags": ["Azure","Governance","Powershell"],
        "url": "/2017/09/exporting-azure-services-csv-file/",
        "teaser": null
      },{
        "title": "My first public VSTS Extension",
        "excerpt":"A couple of days ago I started developing extensions for Build and Release pipelines of Visual Studio Team Services (VSTS). One of these extensions is finished and I wanted to share it with the community.  &nbsp;  VSTS Extension  The extension that I created is a build or release tasks that can be used to extract the raw file content and place that content within a variable. The task itself only needs two properties:   File path: Path to the file from which you want to extract the file content. Variable Name: The name of the variable wherein you want to place the file content.  &nbsp;    &nbsp;  Use Case  An example use case for this extension is saving the content of a query (during a project we extracted the query from a “.asaql” file) into a variable and used that variable to override a parameter within our Azure Resource Management Template. With this setup we configured a Release Pipeline for our Stream Analytics service.  Links  &nbsp;  GitHub: https://github.com/MaikvanderGaag/msft-vsts-extensions  Marketplace: https://marketplace.visualstudio.com/items?itemName=maikvandergaag.maikvandergaag-file-to-variable  Issues: https://github.com/MaikvanderGaag/msft-vsts-extensions/issues  ","categories": ["DevOps"],
        "tags": ["ALM","Extension","VSTS"],
        "url": "/2017/09/my-first-public-vsts-extension/",
        "teaser": null
      },{
        "title": "VSTS build or release agent on a self hosted machine in Azure or on premise",
        "excerpt":"You can think of many situations where you would like to create a self hosted machine for Visual Studio team services (VSTS). Installing and configuring a self hosted machine requires only a few steps.  Before you begin make sure the system meets the pre-requisites:   Windows 10 and Beyond (64-bit): No known system prerequisites at this time. Windows 7 to Windows 8.1, Windows Server 2008 R2 SP1 to Windows Server  : PowerShell 3.0 or higher Visual Studio: Not technically required by the agent, but many build scenarios need Visual Studio.  Besides the system pre-requisites there is also an account pre-requisite:   Account with Personal Access Token.  Create a Personal access token  Creating a Personal Access Token (PAT) requires the following steps:   Navigate to the VSTS tenant were you want to add an agent. In the top bar click on your icon and click on “Security”.     Within the security window click on Personal Access Token.     Click on “Add”  to generate a new personal access token and make sure you select the following scopes:  Agent Pools (read, manage) Deployment group (read, manage)       Save the generated token.  &nbsp;  Install VSTS agent  With all the pre-requisites in place we can start with the agent itself.   Logon to the server that will be the self hosted machine. Navigate to the VSTS tenant were you want to add an agent. Go to Setting –&gt; Agent Pools  and click on “Create new Pool”  The pool is a logical container for the agents that you add to VSTS. You can name the pool after the company of define another logical name.  &nbsp;    &nbsp;   Create a folder on the server to install the agent.  It is a good practice to keep the folder naming as short as possible (D:\\a or D:\\agent).    Download the agent and extract it to the folder you created.     Open a PowerShell window as administrator and navigate to the created folder. Run the below command:  .\\config.cmd   During the execution you need to fill in specific information about your tenant.    &nbsp;  At the end of the script the agent is started. The installed agent should now be visible within the agent pool in VSTS.    ","categories": ["DevOps"],
        "tags": ["ALM","Azure","DevOps","VSTS"],
        "url": "/2017/12/vsts-build-release-agent-self-hosted-machine-azure-premise/",
        "teaser": null
      },{
        "title": "Publish PowerBI files with Visual Studio Team Services",
        "excerpt":"Today I published an extension for Visual Studio Team Services (VSTS) that gives you the ability to deploy PowerBI files (pbix) to PowerBI.com. It uses the PowerBI API that has the following operations (at the time of writing this article):   Dataset operations: Get and create Datasets. Table operations: Get Tables and update Table schema. Row operations: Add Rows and Delete Rows. Group operations: Get Groups. Import operations: Create Import, Get Imports, Get Import from GUID, and Get Import by File Path. Dashboard operations: Get Dashboards and Get Tiles.  All other information about the API can found on MSDN: https://msdn.microsoft.com/en-us/library/mt147898.aspx    Start using the extension in VSTS  To start with the extension you will need to meet the prerequisites:   Account with access to PowerBI Azure Active Directory application with access to the PowerBI API  This means you will have to register an application within Azure Active Directory to be able to use the PowerBI API. The user that publishes the dashboards from VSTS should have access to the application created in Azure Active Directory.  The below figure gives an example for an Azure Active Directory application.    &nbsp;  The application itself needs to be configured with the following permissions.    Future Releases  This is the first release, if you have any suggestions or remarks please let me know. I will try to add those within a new release.  Other information about the extension:   GitHub: https://github.com/MaikvanderGaag/msft-vsts-extensions Marketplace: https://marketplace.visualstudio.com/items?itemName=maikvandergaag.maikvandergaag-publish-power-bi-file Issues: https://github.com/MaikvanderGaag/msft-vsts-extensions/issues  ","categories": ["DevOps"],
        "tags": ["ALM","PowerBI","Release","VSTS"],
        "url": "/2017/12/publish-powerbi-files-with-visual-studio-team-services/",
        "teaser": null
      },{
        "title": "2017 review of MSFTPlayground",
        "excerpt":"One of the things I like to do is sharing knowledge with the Community. I started my blog in 2008 and changed the platform a couple of times from WordPress to SharePoint and BlogEngine.Net to SharePoint to WordPress again.  In 2017 my blog grew from 201 posts to 231 posts and the views on my pages grew enormous. With a top of 732 views on one day.  Beside my blog I continued to use other things in 2017 and started something new:   Twitter: Continued to use twitter more actively. Facebook: I continued to use a Facebook page for my blog. GitHub: I continued to use GitHub to share code and knowledge. Visual Studio Team Services Extensions: I started creating extensions for Visual Studio Team Services.  Users and Page views  In 2016 the blog had 66,976 (11,000 more than 2016) unique users and had 98,377 page views (23,000 more than 2016).    &nbsp;  &nbsp;  Top 10 Pages of 2017  The top 10 pages of 2017 where pages from previous years.    &nbsp;  The top 10 for pages created in 2017 look like this.    &nbsp;  Thank you for visiting my blog in 2017. I hope to see you back in 2018, I will keep writing blog posts and will try to boost the quality even further.  &nbsp;  Happy New Year!  ","categories": ["General"],
        "tags": ["MSFTPlayground","VSTS"],
        "url": "/2018/01/2017-review-msftplayground/",
        "teaser": null
      },{
        "title": "VSTS Extension for Azure Role Based Access Control",
        "excerpt":"Today I published an extension for Visual Studio Team Services (VSTS) that gives you the ability to add and remove role based access assignments in Azure.  VSTS Extension  The screenshot below shows the extension.  &nbsp;    &nbsp;  The extension can do two actions:   Remove an Azure Role Assignment on a Resource Group. Add an Azure Role Assignment on a Resource Group.  Other arguments on this extension are:     Argument Description   Azure Subscription The Azure Subscription   Resource Group The Azure Resource group to configure   Action The specific action to take: Add or Remove   Azure Role Assignment The role to assign or remove   Users or Group Perform the action for users or groups   Groups Visible when Groups action is picked. In this field you need to specify the group names. For multiple separate by (',')   Users Visible when Users action is picked. In this field you need to specify the user principal names of the users. For multiple separate by (',')   Fail on Error Boolean value whether the task should fail when an error occurs    &nbsp;  Links  GitHub: https://github.com/MaikvanderGaag/msft-vsts-extensions  Marketplace: https://marketplace.visualstudio.com/items?itemName=maikvandergaag.maikvandergaag-azurerbac  Issues: https://github.com/MaikvanderGaag/msft-vsts-extensions/issues  ","categories": ["DevOps"],
        "tags": ["ALM","Azure","RBAC","VSTS"],
        "url": "/2018/02/vsts-extension-for-azure-role-based-access-control/",
        "teaser": null
      },{
        "title": "Invoke Azure Function in your Visual Studio Team Services CI/CD pipeline",
        "excerpt":"A utility task is available for Visual Studio Team Services (VSTS) to invoke an http triggered Azure function. The ability to invoke a Function from your CI/CD pipeline offers a lot of new possibilities within those pipelines.  Arguments  The Arguments that you can configure on the utility task:     Argument Description   Azure function URL The URL of the Azure function to invoke.   Function key The value of the available function or the host key for the function to invoke.   Request body The request body for the Azure function call.   Execution mode Synchronous mode (the default), or Asynchronous call where the Azure function calls back to update the timeline record.   Response parse expression How to parse the response body for success.    &nbsp;  Phase  When adding this task to a CI/CD pipeline you have to add a “Agentless phase”. To invoke an Function an agent isn't required.  &nbsp;    In the agentless phase add the “Invoke Azure Function” task.    &nbsp;  And configure the action as needed.  ","categories": ["DevOps"],
        "tags": ["Azure","Azure Functions","CD","CI","VSTS"],
        "url": "/2018/02/invoke-azure-function-in-your-visual-studio-team-services-ci-cd-pipeline/",
        "teaser": null
      },{
        "title": "Point to Site VPN Client won&rsquo;t install",
        "excerpt":"To connect an Azure App Service to a on-premise database you can make use of different solutions. Two of those solutions are:   Hybrid Connection VPN  On my blog I wrote a couple of articles on Hybrid Connections and this time I was working with the VPN connection.  For a project we were setting up the connection using the following documentation from Microsoft:   Configure a Point-to-Site connection to a VNet using native Azure certificate authentication: PowerShell  &nbsp;  VPN Client Installation  When we tried to install the VPN Client nothing happened. After clicking, rebooting and trying for a while it still didn’t install and I opened the event viewer and found the following message:  Activation of app Microsoft.Windows.Apprep.ChxApp_cw5n1h2txyewy:App.AppXc99k5qnnsvxj5szemm7fp3g7y08we5vm.mca failed with error: This app can't be activated by the Built-in Administrator. See the Microsoft-Windows-TWinUI/Operational log for additional information.  &nbsp;  This message remembered me off something I experienced a couple of years ago with an installer, I created myself. Because the installer is created from another machine the file is blocked by default and needs to be unblocked.  &nbsp;    &nbsp;  Beside that image I also created an animated version:    After unblocking the installer the VPN installation worked perfectly.  ","categories": ["Administration"],
        "tags": ["App Services","Azure","VPN"],
        "url": "/2018/04/point-to-site-vpn-client-install/",
        "teaser": null
      },{
        "title": "The securitydata Azure resource group",
        "excerpt":"Most of the times companies have rules in place for managing their Azure environment. The main rules that should be in place are “Azure Policies” and naming conventions. Naming convention should be used to easily identify Azure resources and making consistency within the Azure Portal. These naming convention could then also be written up into policies to make sure everyone is being compliant.  To get started with the naming conventions you can take a look at the following article of Microsoft:   Naming conventions  Resource Group naming convention  Most of the time (depending on the subscription setup) I use the following naming convention for the resource groups:  &lt;environment&gt;-rg-&lt;shortname&gt;   With this naming convention I can easily identify the resource group and also know for which environment it is. In a project we used the Azure Security Advisor and saw a resource group named “securitydata” appearing with a storage account in it. To comply with the rules we deleted the resource group and saw it appearing a few hours later.  &nbsp;  Removing the securitydata resource group  Because the group kept appearing we looked into another option. The security advisor uses the storage account for saving data it found during the data collection. If you look at the data collection blade of the security advisor you see that there are two options:   Use workspaces created by Security Center (default). Use another workspace    The first option makes use of the storage account and the second one is bound to a “OMS” workspace. We already had a OMS workspace within our subscription so we bound the data collection to that workspace. If you do not have a OMS workspace you can easily created (it is also a free service depending on the abilities you use).  To get everything in place follow the below steps:   Create a Log Analytics service (OMS) if you don’t have one.  Make sure that you apply the naming convention rule . Navigate to the Security Advisor.     Within the Security Advisor blade click on “Security Policy”.     Click on the subscription for which you want to collect data.     On the data collection blade, bind the security advisor data collection to your OMS workspace.     Save the changes In the “Pricing tier” blade check if the correct tier is selected. When done delete the “securitydata” resource group.  ","categories": ["Azure"],
        "tags": ["Azure","Governance","Resource Group"],
        "url": "/2018/04/the-securitydata-azure-resource-group/",
        "teaser": null
      },{
        "title": "Updates to my VSTS extensions",
        "excerpt":"Since a couple of months I have been developing extensions for Visual Studio Team Service (VSTS). I created extensions for task I had to do manual a couple of times. Last week I updated all of my extensions.  File Content to Variable  Small extension to save the content of a file within a variable. Release 2.2.0 contains some small fixes and updated tagging information.  https://marketplace.visualstudio.com/items?itemName=maikvandergaag.maikvandergaag-file-to-variable  Publish Stream Analytics Transformation  Small extension to publish a “asql” file to a Stream analytics Job. Release 1.2.0 contains some small fixes and updated tagging information.  https://marketplace.visualstudio.com/items?itemName=maikvandergaag.maikvandergaag-publish-stream-analytics  Publish Power BI File  This extension allows you to publish your PowerBI files to PowerBI.com. Because I had many update request I have updated the extension with new functionality and also renamed it more appropriate. The new function is called “PowerBI Actions”.  Because there is a new extension this extension is discontinued.  https://marketplace.visualstudio.com/items?itemName=maikvandergaag.maikvandergaag-publish-power-bi-file  PowerBI Actions  This extension is the evolved extension of “Publish PowerBI File” this extension contains a couple of new functionalities. The complete functionality list:   Publish a PowerBI file to PowerBI.com in the “me” workspace or create a workspace when needed. Update the connection string of a DirectQuery dataset.  https://marketplace.visualstudio.com/items?itemName=maikvandergaag.maikvandergaag-power-bi-actions  Azure Role Based Access Control  An extension to perform Azure Role Based Access Control actions within Azure. Release 1.2.0 contains some small fixes.  https://marketplace.visualstudio.com/items?itemName=maikvandergaag.maikvandergaag-azurerbac  &nbsp;  I hope you guys like my extensions. If you have any ideas, comments or features request please let me know. More information about my extensions can be found on GitHub:   https://github.com/maikvandergaag/msft-vsts-extensions  ","categories": ["Azure DevOps"],
        "tags": ["Extension","VSTS"],
        "url": "/2018/05/updates-to-vsts-extensions/",
        "teaser": null
      },{
        "title": "Azure Managed Service Identity and Local Development",
        "excerpt":"Instead of storing user credentials of an external system in a configuration file, you should store them in the Azure Key Vault. Before MSI (Managed Service Identity) you would have to store the credentials to use the key vault in the configuration file so this wasn’t really helpful. With MSI (Managed Service Identity) you do not have that problem anymore.  MSI  Managed Service Identity is basically an Identity that is Managed by Azure. Managed Identities are there in two forms:   A system assigned identity: When the identity is enabled, Azure creates an identity for the instance in the Azure AD tenant that's trusted by the subscription of the instance. After the identity is created, the credentials are provisioned onto the instance. The lifecycle of a system assigned identity is directly tied to the Azure service instance that it's enabled on. If the instance is deleted, Azure automatically cleans up the credentials and the identity in Azure AD. A user assigned identity: is created as a standalone Azure resource. Through a create process, Azure creates an identity in the Azure AD tenant that's trusted by the subscription in use. After the identity is created, the identity can be assigned to one or more Azure service instances. The lifecycle of a user assigned identity is managed separately from the lifecycle of the Azure service instances to which it's assigned.  The main difference between the two forms is that this system assigned identity will exist as long as your application exist. The system assigned identity will also not be visible within the Azure Active Directory blade under the applications.  Assigning a MSI to an Azure Function  To enable the Managed Service Identity for an Azure Function you have to apply the following steps:   Open the Azure Function in the Azure Portal Click on Platform Features and select \"Managed service identity\"     Click \"On\" and click \"Save\". In the background an Azure Application is created.      Give the application the proper rights on the service you would like to use.  Using MSI in Code  To use the Managed Service Identity in code only two lines of code are needed in combination with the Azure Key Vault. Before using it you will have to add the following NuGet package: \" Microsoft.Azure.Services.AppAuthentication\"  &nbsp;  [FunctionName(\"Sample Function\")] public static async Task&lt;HttpResponseMessage&gt; Run([HttpTrigger(AuthorizationLevel.Function, \"post\", Route = null)]HttpRequestMessage req, TraceWriter log) {     log.Info(\"Function triggered.\");     string secretInfo= ConfigurationManager.AppSettings[\"kv:resource\"];      //get secret for the application     var azureServiceTokenProvider = new AzureServiceTokenProvider();     var kv = new KeyVaultClient(new KeyVaultClient.AuthenticationCallback(azureServiceTokenProvider.KeyVaultTokenCallback));     string secret = kv.GetSecretAsync(secretInfo).Result.Value;      log.Info(\"Function finished.\");      return req.CreateResponse(HttpStatusCode.OK); } MSI and local debugging  When developing an Azure Function and start on your local machine, you also want to use the Managed Service Identity. But how do you do that? You do not have a Managed Service Identity on your local machine.  ","categories": ["Azure","Development"],
        "tags": ["Azure","Development","MSI"],
        "url": "/2018/08/azure-managed-service-identity-and-local-development/",
        "teaser": null
      },{
        "title": "Resource Group deployment via ARM templates",
        "excerpt":"When deploying an Azure Resource Manager (ARM) template you have to create a resource group within Azure. To deploy a template via script your script would look like the one below.  Select-AzureRmSubscription -SubscriptionId [subscriptionid] New-AzureRmResourceGroup -Name [name] -Location [location] -Force  New-AzureRmResourceGroupDeployment -ResourceGroupName [name] -TemplateFile [templatefile] In this script you explicitly create a resource group and then deploy the template to that resource group. Depending on the content of the template because there is also cross resource group deployment.  Resource group in an ARM Template  Since a couple of weeks it isn't necessary anymore to create a resource group before you deploy a template. You are now able to add the resource group creation in the template.  {   \"type\": \"Microsoft.Resources/resourceGroups\",   \"apiVersion\": \"2018-05-01\",   \"location\": \"westeurope\",   \"name\": \"rp-ARM-Test\",   \"properties\": {} } The resource group is a resource that you can incorporate in a template, making it a subscription wide deployment via ARM.  At the moment it is only possible to deploy subscription wide ARM templates via the Azure CLI, PowerShell compatibility will be added shortly.(the article will be updated when it is available).  Azure CLI  Make sure you use a new version of the Azure CLI. I have run the scripts with version 2.0.44.   Install Azure CLI 2.0  The script used for the deployment is shown below. I'm not certain why the \"-l\" is required. Normally it stands for the location but in the template you also specify a location. That location is used as the location of the resource group.  az deployment create -l [location] --template-file [template-file] --parameters @[parameters file] Deploying resources in the Resource group  Incorporating the resource group in the template also allows you to deploy resources in this group. The example below deploys a hosting plan and a website in the resource group.  {   \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",   \"contentVersion\": \"1.0.0.1\",   \"parameters\": {     \"resourceGroupName\": {       \"type\": \"string\"     },     \"location\": {       \"type\": \"string\"     },     \"hostingPlanName\": {       \"type\": \"string\"     },     \"webApp\": {       \"type\": \"string\"     },     \"skuName\": {       \"type\": \"string\",       \"defaultValue\": \"F1\",       \"allowedValues\": [ \"F1\", \"D1\", \"B1\", \"B2\", \"B3\", \"S1\", \"S2\", \"S3\", \"P1\", \"P2\", \"P3\", \"P4\" ]     },     \"skuCapacity\": {       \"type\": \"int\",       \"defaultValue\": 1,       \"minValue\": 1     }   },   \"variables\": {     },   \"resources\": [     {       \"type\": \"Microsoft.Resources/resourceGroups\",       \"apiVersion\": \"2018-05-01\",       \"location\": \"[parameters('location')]\",       \"name\": \"[parameters('resourceGroupName')]\",       \"properties\": {}     },     {       \"type\": \"Microsoft.Resources/deployments\",       \"apiVersion\": \"2017-05-10\",       \"name\": \"resourcegroupdeployment\",       \"resourceGroup\": \"[parameters('resourceGroupName')]\",       \"dependsOn\": [         \"[resourceId('Microsoft.Resources/resourceGroups/', parameters('resourceGroupName'))]\"       ],       \"properties\": {         \"mode\": \"Incremental\",         \"template\": {           \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",           \"contentVersion\": \"1.0.0.0\",           \"parameters\": {},           \"variables\": {},           \"resources\": [             {               \"apiVersion\": \"2016-09-01\",               \"name\": \"[parameters('hostingPlanName')]\",               \"type\": \"Microsoft.Web/serverfarms\",               \"location\": \"[parameters('location')]\",               \"tags\": {                 \"displayName\": \"HostingPlan\"               },               \"sku\": {                 \"name\": \"[parameters('skuName')]\",                 \"capacity\": \"[parameters('skuCapacity')]\"               },               \"properties\": {                 \"name\": \"[parameters('hostingPlanName')]\"               }             },             {               \"apiVersion\": \"2016-08-01\",               \"name\": \"[parameters('webApp')]\",               \"kind\": \"app\",               \"type\": \"Microsoft.Web/sites\",               \"location\": \"[parameters('location')]\",               \"tags\": {                 \"displayName\": \"Website\"               },               \"properties\": {                 \"name\": \"[parameters('webApp')]\",                 \"serverFarmId\": \"[parameters('hostingPlanName')]\"               },               \"dependsOn\": [                 \"[resourceId('Microsoft.Web/serverfarms', parameters('hostingPlanName'))]\"               ]             }           ],           \"outputs\": {}         }       }     }   ],   \"outputs\": {} } The deployment uses the cross deployment group deployment method.  For the complete sample I have also included a parameters file.  {   \"$schema\": \"http://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\",   \"contentVersion\": \"1.0.0.0\",   \"parameters\": {     \"hostingPlanName\": { \"value\": \"gaag-web-plan-05\" },     \"webApp\": { \"value\": \"gaag-web-05\" },     \"resourceGroupName\": { \"value\": \"gaag-rg-armtest\" },     \"location\": { \"value\": \"westeurope\" }   } } Deploying the files is done like this.  az deployment create -l westeurope --template-file 12-resourcegroup.json --parameters @12-resourcegroup.parameters.json Subscription wide deployment  Subscription wide deployment are now a possibility with the addition of resource groups in the ARM templates. Microsoft also added other subscription wide resources:   Policies Role-based access control Azure Security Center  More details about these resources and the deployment can be found here:   Deploy resources to an Azure subscription  &nbsp;  ","categories": ["Azure DevOps"],
        "tags": ["ARM","Azure","Resource Group"],
        "url": "/2018/08/resource-group-deployment-via-arm-templates/",
        "teaser": null
      },{
        "title": "Local Kubernetes cluster with Dashboard",
        "excerpt":"When developing containers or applications that need to be hosted on Kubernetes it is handy to have a local Kubernetes cluster.  With the use of Docker you can easily create a local single node cluster.  Docker  Start by installing Docker for your operating system. In this guide I used Docker for Windows but the operating system does not really matter.   Install Docker for Windows Install Docker for Mac  Follow the setup guide and Docker will be started after the install.  Adding Kubernetes  When Docker is running you will see a Docker icon in the taskbar. Add Kubernetes can be via the settings page of Docker. To open the settings right click the icon and choose settings.    In the settings there is a sub menu called \"Kubernetes\" on this page enable Kubernetes. If you want you can make the system containers visible.    Applying these settings will add Kubernetes within Docker.  Kubernetes Dashboard  Adding Kubernetes to Docker will not add the Kubernetes Dashboard. For adding the Kubernetes dashboard you have to do some additional steps.   Install kubectl: Install and Set Up kubectl. Open a Command Prompt.  If you have more Kubernetes cluster make sure you are running within the right Kubernetes context. You can get your current context via: kubectl config get-contexts and set the right context via: kubectl config use-context [context-name]  &nbsp;    Run the following kubectl command to add the dashboard to the cluster.  kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/alternative/kubernetes- dashboard.yaml As this is a development environment I used the alternative setup. The alternative setup is not secured and should not be used in production scenario's.  Kubernetes dashboard installation     To open the dashboard use the proxy option of kubectl.  kubectl proxy This will open a proxy to the dashboard. You can open the dashboard in the browser via the link.   http://localhost:8001/api/v1/namespaces/kube-system/services/http:kubernetes-dashboard:/proxy/  When you want to use the dashboard the proxy needs to be open by running the same  kubectl proxy command.  Permanently exposing the dashboard  You can also expose the dashboard permanently by adding a service to the cluster. Applying the below \"yaml\" file will add the service to the cluster.  apiVersion: v1 kind: Service metadata:   labels:     k8s-app: kubernetes-dashboard   name: kubernetes-dashboard-nodeport   namespace: kube-system spec:   ports:   - port: 80     protocol: TCP     targetPort: 9090     nodePort: 30555   selector:     k8s-app: kubernetes-dashboard   sessionAffinity: None   type: NodePort The service will expose the dashboard on port 30555. After applying you can open dashboard via this url: http://localhost:30555.  Reset Kubernetes  When testing and developing applications you sometimes want to reset the cluster in order to begin with a clean environment.  Docker has a handy option for resetting the cluster. Within the settings open the \"Reset\" option and click on \"Reset Kubernetes cluster\".    If you choose to reset the Kubernetes cluster remember that you will remove the dashboard aswell.  &nbsp;  ","categories": ["Administration"],
        "tags": ["AKS","Docker","Kubernetes","Windows"],
        "url": "/2018/08/local-kubernetes-cluster-with-dashboard/",
        "teaser": null
      },{
        "title": "Kubernetes (AKS) attached to Azure Storage (Files)",
        "excerpt":"Kubernetes (AKS) can be used for many situations. For a client we needed to make files available trough a Kubernetes Pod. The files needed to be shared between containers, nodes and pods.  To make these files available we used a file share that gave us a couple of advantages:   Files can be made read-only for a Pod. Files can be added via a Windows Network drive.  To prove the scenario worked we used a simple \"html\" file when we opened the external endpoint.  &lt;html&gt; &lt;head&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;File Test&lt;/h1&gt; &lt;/body&gt; &lt;/html&gt; The container type we used within Kubernetes is \"httpd\" which is a container with a simple apache web server. (https://hub.docker.com/_/httpd/).  Azure Storage Account  For the file share we choose \"Azure Files\". With Azure Files we can share a File Share with the Kubernetes Pods and Windows devices. For this you need to create an Azure Storage account.   Navigate to the Azure Portal (https://portal.azure.com). Click \"Create a resource\" and search for \"storage account\". Choose \"Storage account - blob, file, table, queue\". Fill in the correct properties and click on \"Create\". When the storage account is created open the resource and click on \"Files\".      In the Azure Files blade create a new \"File share\".    With the Azure File Share created you can connect it to your windows machine.   Click on the file share in the Azure File blade.     Click on \"connect\". In the connect blade it will present the PowerShell script that you can run on your environment to attach the file share.  $acctKey = ConvertTo-SecureString -String \"[key]\" -AsPlainText -Force $credential = New-Object System.Management.Automation.PSCredential -ArgumentList \"Azure\\kubernetesstorage01\", $acctKey New-PSDrive -Name Z -PSProvider FileSystem -Root \"[storage]\" -Credential $credential -Persist \"Note: Make sure you do not run this script as administrator. Running this script as administrator will add the drive but it will not show up in windows explorer for your account.\"    Kubernetes Secret  For the authentication to the file share a secret entry needs to be made within Kubernetes. The secret will contain a couple of properties:   Storage account name Storage access key  Adding the secrets Kubernetes can be done via a yaml file or the kubectl command line.  When using the yaml option encode the properties in base64. You can google for an online solution. I used: https://www.base64encode.net/ for example.   Command line  kubectl create secret generic storage-secret --from-literal=azurestorageaccountname=[storage name] --from-literal=azurestorageaccountkey=[account key] Yaml  apiVersion: v1 kind: Secret metadata:   name: storage-secret type: Opaque data:   azurestorageaccountname: [base64 account name]   azurestorageaccountkey: [base64 account key] The yaml will needs to be applied to Kubernetes by using the command:  kubectl apply -f [filename] Kubernetes deployment  For getting the container up and running we create a Kubernetes deployment that runs the container.  apiVersion: apps/v1beta1 kind: Deployment metadata:   name: webfile spec:   replicas: 1   template:     metadata:       labels:         app: webfile     spec:       containers:       - name: webfile         image: httpd         imagePullPolicy: Always         volumeMounts:         - name: azurefileshare           mountPath: /usr/local/apache2/htdocs/           readOnly: true         ports:         - containerPort: 80       volumes:       - name: azurefileshare         azureFile:           secretName: storage-secret           shareName: storage           readOnly: false This deployment is a simple one that creates a single pod and attaches a volume to the mountPath: /usr/local/apache2/htdocs/   The mountPath is the starting point for the apache web server meaning that everything that is placed on the file share will be exposed by the pod.  The deployment needs to be applied by using the following command:  kubectl apply -f [kubernetes deployment file] Kubernetes service  To get it externally available we have to take one more step. That step is configuring a Kubernetes service of the type \"LoadBalancer\".  apiVersion: v1 kind: Service metadata:   name: webfile spec:   type: LoadBalancer   ports:   - port: 80   selector:     app: webfile The service needs to be applied by using the command:  kubectl apply -f [service deployment file] Applying the service should generate an external IP address. Checking if it is provisioned correctly you can look on the Kubernetes Dashboard:    Or by running the following kubectl command:  kubectl get svc [service name]   Result  Opening the IP in the browser will result in an empty page. Uploading the sample html file named \"index.html\" will have the expected result.    &nbsp;  &nbsp;  ","categories": ["Azure"],
        "tags": ["AKS","Azure","Kubernetes","Storage"],
        "url": "/2018/09/files-from-kubernetes-aks/",
        "teaser": null
      },{
        "title": "AKS (Kubernetes) and no connection could be made because the target machine actively refused it",
        "excerpt":"A client of my had an error while connecting to different resources within their Kubernetes cluster in Azure (AKS).  Kubectl error  On the kubectl commands kubectl proxy and kubectl get nodes they got the error:  connectex: No connection could be made because the target machine actively refused it.  The complete message after visiting the dashboard (that wasn't loaded) on the command kubectl proxy was:  http: proxy error: dial tcp 127.0.0.1:8080: connectex: No connection could be made because the target machine actively refused it.    The problem occurs because the command line isn't able to load the correct config file from the file system. The config file is saved within an environment variable that you can change or alter.  Solution  The solution to the problem was changing or setting the environment variable to the kubernetes config file name \"KUBECONFIG\". You can do this by performing the following PowerShell command.  [Environment]::SetEnvironmentVariable(\"KUBECONFIG\", $HOME + \"\\.kube\\config\", [EnvironmentVariableTarget]::Machine) &nbsp;  ","categories": ["Azure"],
        "tags": ["Azure","kubectl","Kubernetes"],
        "url": "/2018/09/aks-kubernetes-and-no-connection-could-be-made-because-the-target-machine-actively-refused-it/",
        "teaser": null
      },{
        "title": "Function parameters while debugging PowerShell in Visual Studio Code",
        "excerpt":"Visual Studio Code is becoming one of the tools that you need to use when developing PowerShell scripts. It allows you to debug your scripts. But how do you add parameters for debugging. The upcoming paragraphs will explain how.  PowerShell  Parameters are common when developing scripts. Most probably they are configured with the Param object. Just like this example:  Param(     [String]$ParameterOne,     [String]$ParameterTwo )  Write-Output \"ParameterOne      :$ParameterOne\" Write-Output \"ParameterTwo      :$ParameterTwo\" Running this script called \"msftplayground.ps1\" within a PowerShell terminal .\\msftplayground.ps1will result with no parameters set because they need to be specified.  If you specify the parameters like this .\\msftplayground.ps1 -ParameterOne \"Test value 1\" -ParameterTwo \"Test Value 2\" those values can then be used during the executing of file.    Visual Studio Code  With the \"Debug\" and \"Integrated PowerShell Terminal\" Visual Studio Code became a real powerfull tool. Debugging sessions can be started by simply pressing \"F5\".  When you press \"F5\" within the PowerShell file Visual Studio Code executes the file. So if we for example start the script that we used in this article the following output will be shown    This is expected behavior because the debugger does not know the parameters that can be used.  So how do you specify the parameters?  Parameters need to be specified within the debugger configuration. Opening the configuration can be done via the debugger bar. Pressing the settings icon will open the configuration.    The configuration is done in a JSON file called \"launch.json\". In this file there are configurations for different situations. Choose the correct configuration for your situation and add your parameters in the \"args\" property.  A single configuration could look like this:  {    \"type\": \"PowerShell\",    \"request\": \"launch\",    \"name\": \"PowerShell Launch Current File\",    \"script\": \"${file}\",    \"args\": [\"-ParameterOne 'Test value 1'  -ParameterTwo 'Test Value 2'\"],    \"cwd\": \"${file}\" } Save the configuration and press \"F5\" to start the debugger. The PowerShell terminal will know show the expected result.    ","categories": ["Development"],
        "tags": ["Development","Powershell","Visual Studio Code"],
        "url": "/2018/09/function-parameters-while-debugging-powershell-in-visual-studio-code/",
        "teaser": null
      },{
        "title": "The added value of Azure DevOps for your organization",
        "excerpt":"Two weeks ago Microsoft announced Azure DevOps. Azure DevOps is a name change and rebranding of the product Visual Studio Team Services (VSTS). The Azure DevOps service consists of more than 15 years experience in software development services from TFS (Team Foundation Server) to Visual Studio Online to Visual Studio Team Services to Azure DevOps.  Visual Studio Team Services was linked to Visual Studio by name what was a bit confusing for a lot of organizations. With the name change Microsoft really confirms the fact&nbsp; that Azure DevOps is a service for every language, framework and platform.  Individual services  Azure DevOps consists of five services that can used simultaneously but also separately:    Pipelines: Used to streamline and automate your build and deployment activities (Continuous Integration / Continuous Deployment) Boards: Track tasks and work items using Kanban boards, team dashboards and custom reports. Artifacts: Integration with public and private package feeds such as NuGet, np men Maven Repos: Cloud service for storing and sharing public or private code repositories Test Plans: A service launched with Azure DevOps, that provides a browser-based test management solution for exploratory, planned manual, and user acceptance testing.    Besides the five separate services the service still has the default overview items like the dashboard and the Wiki.  Added value  The DevOps service can be used where it is really needed. You are able to choose the services that have an added value. The choice for services will not have an impact on the existing (software) development process.  For example, Github is used for code repositories. These repositories can be linked to Pipelines for the build and deployment process.  Another example, is the use of the Boards functionality to plan and follow up tasks, but save the code in another or maybe a existing repository.   Additional Resources   Projects Deep dive into Azure Artifacts Deep dive into Azure Test Plans Deep dive into Azure Repos Deep dive into Azure Boards   &nbsp;       Blog orginally published in dutch on the website of 3fifty. If your are based in the Netherlands take a look at the 3fifty kickstarter training for Azure DevOps.     ","categories": ["Azure DevOps"],
        "tags": ["Azure","Azure DevOps"],
        "url": "/2018/10/the-added-value-of-azure-devops-for-your-organization/",
        "teaser": null
      },{
        "title": "Azure DevOps Automation",
        "excerpt":"A couple of weeks ago the rename / rebranding of Visual Studio Team Services to Azure DevOps was announced. The rebranding is a great step forward into positioning the product even better and have more value for example for open source projects.  The service know exists out of five services that can be used independent&nbsp;of each other as long as you have an Azure DevOps Project.   Azure Pipelines:&nbsp;CI/CD that works with any language, platform, and cloud. Azure Boards: Work tracking with Kanban boards. Azure Artifacts: Public or private package repository Azure Repos: Private / public Git repos Azure Test Plans: Testing solution.  The above services can be specific for each project. All projects have the options to turn off a specific services. So if you for example are using Git you can start using Azure DevOps for the Builds and Release pipelines.    &nbsp;  When using Azure DevOps a lot and also having to set up and administrate multiple environments automation comes in handy. How can we automate things around Azure DevOps?  ARM  For almost every service in Azure ARM (Azure Resource Manager) templates can be used this is no different for Azure DevOps.  Azure DevOps Organization  {    \"type\": \"Microsoft.VisualStudio/account\",    \"name\": \"[parameters('accountName')]\",    \"apiVersion\": \"2014-02-26\",    \"location\": \"[parameters('location')]\",    \"tags\": {},    \"scale\": null,    \"properties\": {     \"operationType\": \"Create\",     \"accountName\": \"[parameters('accountName')]\"   },     \"dependsOn\": [] } Azure DevOps Project  {   \"name\": \"[concat(parameters('accountName'), '/', parameters('projectName'))]\",   \"type\": \"Microsoft.VisualStudio/account/project\",   \"location\": \"[parameters('location')]\",   \"apiVersion\": \"2014-02-26\",   \"properties\": {     \"ProcessTemplateId\": \"[parameters('processTemplateId')]\",     \"VersionControlOption\": \"[parameters('versionControlOption')]\"   } } Azure DevOps  Combining the two resources adds the ability to create an organization and project from one template.  Azure DevOps Organization and Project  {   \"$schema\": \"http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json#\",   \"contentVersion\": \"1.0.0.0\",   \"parameters\": {     \"accountName\": {       \"type\": \"string\",       \"metadata\": {         \"description\": \"The name of the Visual Studio Team Services account, if it doesn't exist it will be created.\"       }     },     \"projectName\": {       \"type\": \"string\",       \"metadata\": {         \"description\": \"The name of the Visual Studio Team Services project.\"       }     },     \"location\": {       \"type\": \"string\",       \"defaultValue\": \"[resourceGroup().location]\",       \"metadata\": {         \"description\": \"Location for all resources.\"       }     },     \"processTemplateId\": {       \"type\": \"string\",       \"defaultValue\": \"6B724908-EF14-45CF-84F8-768B5384DA45\",       \"allowedValues\": [         \"6B724908-EF14-45CF-84F8-768B5384DA45\",         \"ADCC42AB-9882-485E-A3ED-7678F01F66BC\",         \"27450541-8E31-4150-9947-DC59F998FC01\"       ],       \"metadata\": {         \"description\": \"Scrum: 6B724908-EF14-45CF-84F8-768B5384DA45 / Agile: ADCC42AB-9882-485E-A3ED-7678F01F66BC / CMMI: 27450541-8E31-4150-9947-DC59F998FC01\"       }     },     \"versionControlOption\": {       \"type\": \"string\",       \"defaultValue\": \"Git\",       \"allowedValues\": [         \"Git\",         \"Tfvc\"       ],       \"metadata\": {         \"description\": \"The version control of the Visual Studio Team Services project's source code: Git or Tfvc.\"       }     },     \"location\": {       \"type\": \"string\",       \"defaultValue\": \"[resourceGroup().location]\",       \"metadata\": {         \"description\": \"Location for all resources.\"       }     }   },   \"variables\": {},   \"resources\": [     {       \"type\": \"Microsoft.VisualStudio/account\",       \"name\": \"[parameters('accountName')]\",       \"apiVersion\": \"2014-02-26\",       \"location\": \"[parameters('location')]\",       \"tags\": {},       \"scale\": null,       \"properties\": {         \"operationType\": \"Create\",         \"accountName\": \"[parameters('accountName')]\"       },       \"dependsOn\": [],       \"resources\": [         {           \"name\": \"[concat(parameters('accountName'), '/', parameters('projectName'))]\",           \"type\": \"Microsoft.VisualStudio/account/project\",           \"location\": \"[parameters('location')]\",           \"apiVersion\": \"2014-02-26\",           \"properties\": {             \"ProcessTemplateId\": \"[parameters('processTemplateId')]\",             \"VersionControlOption\": \"[parameters('versionControlOption')]\"           }         }       ]      }   ] }  With the below PowerShell script its quick and easy to deploy these own resources.  New-AzureRmResourceGroupDeployment -ResourceGroupName [ResourceGroup Name] -TemplateFile [Template Path] -accountName [Organization name] -location [Location] -projectName [Project name] -versionControlOption Git -processTemplateId \"6B724908-EF14-45CF-84F8-768B5384DA45\" Together with the Rest API you can complete the whole process.   Azure DevOps Rest Api Reference  Leave a comments if you would like to see more articles on Azure DevOps automation.  ","categories": ["Azure DevOps"],
        "tags": ["ARM","Azure","Azure DevOps","VSTS"],
        "url": "/2018/10/azure-devops-automation/",
        "teaser": null
      },{
        "title": "Version number counter for Azure DevOps",
        "excerpt":"This week I released a new extension for Azure Pipelines called Version number counter. With this extension you add the ability to auto increment a version number for for example a build or release.  Features  Keys features of this extension are:   Auto increment patch version number. Ability to manually change the version number. Auto increment major version number when a maximum number for the minor version is specified. Specify a maximum value for the patch, minor or major version number. Auto increment minor version number when a maximum number for the patch version is specified.    Configuration  The extension can be found in the Azure DevOps marketplace (https://marketplace.visualstudio.com/items?itemName=maikvandergaag.maikvandergaag-versioncounter). When installed you can find it in a release or build pipeline under the name \"Version number counter\".    For the configuration of the extension there are two main parameters you need to set:   The saved version variable: This parameters is the name of the variable you use to save the version number. This variable will be incremented on each build. This variable can also be used as for example the build number. The saved version number needs to be in the format: *.*.* Azure DevOps Personal Access Token: A personal access token for an account that can update build of releases. When using the extension within a build or release pipeline you need to supply a personal access token with the appropriate amounts of rights:  For using it within a build: Build: Read &amp; Execute For using it within a release: Release: Read, write and execute      Adding a personal access token to your build or release pipeline is only secure when you use the Azure Keyvault. Somewhat less secure is using a secure variable    When you need to auto increment the minor or major version number you need to check the check box: \"Automatically update minor number\" or \"Automatically update major number\". When checked the options to insert the maximum patch / minor version number will appear.  For example fill in 9 as the maximum patch number 1.0.9 will be incremented to 1.1.0.  When you configure the variable with the version number as your build number format this number will be incremented with nicely with each build or release.      &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  ","categories": ["DevOps"],
        "tags": ["Azure DevOps","Extension","Pipeline","Version"],
        "url": "/2018/11/version-number-counter-for-azure-devops/",
        "teaser": null
      },{
        "title": "Building and Deploying Azure DevOps extensions with Azure Pipelines",
        "excerpt":"Azure DevOps extensions can be build and deployed by using Azure Pipelines. In the Azure DevOps marketplace there is a extension available that will help us to get everything up and running. In this blog post we will use the following extensions from the Marketplace:   Azure DevOps Extension Tasks: Task made by \"Microsoft DevLabs\" for constructing Azure DevOps extension packages and deploying them to the marketplace. Version number counter: Task made by myself that will help with controlling the version number of the extensions. More information about this task can be found on my blog.    Building the Azure Build Pipeline  With the extensions installed create a new build pipeline.  In the new build pipeline window choose the visual designer option.    In the screen that follows choose GitHub as we will use GitHub as the source location. If you want to use Azure Repos that isn't a problem. Authorize the connection and make sure you select the correct project and branch.    The build pipeline will open up. The first thing we will configure is the trigger of the build. Because I save multiple extensions within the same repository I have enabled the \"Continuous Integration\" option and also apply a branch filter and a path filter. The path filter is used to make sure the build is only started when something has changed within the specified path.    Next up are the \"Variables\". We add a variable called \"buildversion\" that the \"Version number counter\" extension will use to increment the version number.  Besides that the variable will be used as the build number format (under the options tab).      With all these configuration setup we can start adding tasks. For this build pipeline we will use three tasks.    Version number counter  This extension will be used to increment the version number of the extension. The option \"Automatically update minor number\" is disabled because I want to update this number manually.    Package Extension  Packaging a extension into a vsix file is done via this extension. In this extension we override some specific values like the \"publisher\" and the \"extension version\". The extension version is overwritten with the $(Build.BuildNumber) which is the same as $(buildversion) that we increment each time a build runs.    Publish Artifact  The last task will be used to save the package to the artifact location called \"azurerbac\". The path that will be published is a variable called: \"$(Extension.OutputPath)\". This variable is a output variable from the \"Package Extension\" tasks  and represents the location of the vsix file.    This completes the Azure build pipeline. You can test your pipeline by queuing the build.  Building the Azure Release Pipeline  With the build complete we will create a release pipeline that adds the packaged extension to the marketplace. Setting up the pipeline is done in two easy steps:  Adding the artifact  In a new release pipeline add a artifact. For simplicity set the \"Default version\" to \"Latest version\" and add a short alias name. Make sure the artifact will trigger the deployment of the stage that will be added during the next step.    Setup the Stage  In the release add an empty stage and add a single task named: \"Publish Extension\". To get this extension up and running a Visual Studio Marketplace service connections needs to be added. Do this by clicking the \"New\" button next to \"Visual Studio Marketplace connection\" when you just added the tasks to the stage.    To use this connection type you will need to generate a Personal Access Token for a user that has access to your Marketplace. The specific PAT requirements are shown at the bottom of the image. With this configured you can setup the task completely. Pay good attention to the visibility and pricing parameters of the task.    Save the release pipeline and run the complete process when complete you will notice that your extension has been added to the Visual Studio Marketplace.  Extending your Azure Release Pipeline  When this is up and running you can easily extend the pipeline to also deploy a private version of your extension the complete pipeline could look like the image below.    &nbsp;  &nbsp;  ","categories": ["Azure DevOps"],
        "tags": ["Azure DevOps","Azure Pipelines","Extensions"],
        "url": "/2018/11/building-and-deploying-azure-devops-extensions-with-azure-pipelines/",
        "teaser": null
      },{
        "title": "Administrating and publishing Power BI resources via Azure DevOps",
        "excerpt":"While Power BI started as a self-service tool, it has become a part of the enterprise reporting tooling. With enterprise reporting strong governance can quickly become necessary. This will mean that the reports are being created and managed centrally. When this happens there needs to be a method for managing requirement, changes and bugs.    For this process Azure DevOps can be used. Together with a Power BI extension for CI and CD you can manage the process from creating the report and publishing it to PowerBI.com.    Last year I published the extension called “Power BI Actions”. Today I released a new major version with new capabilities, features and enhancements. The new version of the extension contains the following features:     Publish a Power BI dashboard (pbix file). Create a Power BI workspace. Delete a Power BI workspace. Add a new admin user to a Power BI workspace. (other roles are not supported in the Power BI API) Update the connection of a Power BI report. (SQL, Azure Analysis Services, SharePoint, OData) Refresh a dataset.    Together with the new version the authentication towards Power BI has changed. In version 2 you needed a Native Azure Active Directory application and configure it within the task itself. In version 3 you still need the application but you also need to create a “Power BI Service Connection”. By using a Power BI Service connection, the authentication information does not have to be added to each task. The Power BI service connection is included in the extensions. All information regarding the authentication is saved within the service connection.    With the Power BI Service connection, it has become easier to maintain the authentication information. Besides that the connection can be used in any pipeline within the project.    Getting started    The following steps will get you started with automating the process for Power BI:     Go to the Azure DevOps market place: https://marketplace.visualstudio.com/azuredevops Search for the extension: “Power BI Actions” or use this direct link: https://marketplace.visualstudio.com/items?itemName=maikvandergaag.maikvandergaag-power-bi-actions Click on the “Get It Free” button and follow the installation instructions for the extension.       Azure Active Directory application    One of the other prerequisites is a Native Azure Active Directory application that has access to the Power BI API’s. The manual for setting up this type of application can be found on the Microsoft docs site:    https://docs.microsoft.com/en-us/power-bi/developer/register-app    Power BI Service Connection    With the extension installed a Power BI service connection can be added to your project in Azure DevOps.     Open the project were the service connection needs to be added. Click on “Project Settings” in the lower left corner. Find the option “Service Connections” under Pipelines. Click on “New Service Connection” and find “Power BI” Fill in all the ClientId of the Native application and the user that will perform the actions. Make sure that this user does not have multi factor authentication enabled.       Now that all the prerequisites are setup the task can be added to a pipeline. Search for a task called “Power BI Actions” and start using the extension.       Support    I maintain the extension in my free time and the sources of the extension can be found GitHub. If you encounter any issues or would like to see different features, please let me know by adding an issue to GitHub repository.     https://github.com/maikvandergaag/msft-extensions/issues    Extensions    Besides this extension I created several others. More information about these extension can be found here:      https://msftplayground.com/extensions/      ","categories": ["Azure DevOps","Power BI"],
        "tags": ["Azure","Azure DevOps","Power BI"],
        "url": "/2018/12/administrating-and-publishing-power-bi-resources-via-azure-devops/",
        "teaser": null
      },{
        "title": "2018 review of MsftPlayground",
        "excerpt":"One of the things I like to do is sharing knowledge with the Community. I started my website in 2008 and last year I noticed that I have been blogging for 10 years. My first post was from 27-03-2008 and I still love writing articles.    In 2018 I became a member of the  AIMS Performance Pro team because of me knowledge on Azure and other related topics.  The AIMS performance Pro team (#aimsperformancepro) is a program for ambitious and outspoken technical professionals who have interests in the intersection of IT integration, monitoring, analytics, performance tuning and troubleshooting.     In 2018 I investigated a lot of community options to be able to participate more in the community. Besides my Azure DevOps extensions (https://msftplayground.com/extensions/) I'm also investigating the possibility to start a meetup group to get the community together in 2019.     Users and Page Views    In 2017 my website had 71,174 unique users were in 2018 we had 83,864 unique users. Besides that the blog had 109,358 page views.         Thank you for visiting my website in 2018 and hope you will also be doing it in 2019.   ","categories": ["General"],
        "tags": ["2018","2019","Azure"],
        "url": "/2019/01/2018-review-of-msftplayground/",
        "teaser": null
      },{
        "title": "The new Azure certification paths from Microsoft",
        "excerpt":"Recently, Microsoft released new certifications and the corresponding learning paths. These certifications are more focused on job profiles. Microsoft also calls this \"rolebased learning\".     This is the biggest reason why Microsoft has created these paths. The old existing certifications for Microsoft Azure (70-532, 70-533, 70-535) are very broad in scope and cover a wide range of skills that most professionals can not master.    What do these new certification paths mean? In this article I will try to make this clear to you.    New certification paths    As described above, the new certification paths better reflect the day-to-day activities of professionals. This can also be seen in the names of the certifications:     Microsoft Certified Azure Fundamentals Microsoft Certified Azure Administrator Microsoft Certified Azure Developer Microsoft Certified Azure Solutions Architect Microsoft Certified Azure DevOps Engineer          Microsoft Certified Azure Fundamentals    You will become a Microsoft Certified Azure Fundamentals the moment you pass the exam:     AZ-900 Microsoft Azure Fundamentals    For who is it?    This certification is intended for candidates who want to demonstrate basic knowledge of cloud services and how these services work in Azure. This exam is therefore intended for people with non-technical backgrounds such as those involved in the sales or purchase of cloud-based solutions. By obtaining this certification, they demonstrate that they have basic knowledge of Azure and that they master the following concepts within the Azure cloud:     Understand cloud concepts Knowledge of Azure core services Understanding Security, Privacy, Compliance and Trust Azure costs and support    Microsoft Certified Azure Administrator    The following exams are required to obtain the Microsoft Certified Azure Administrator predicate:     AZ-100 Microsoft Azure Infrastructure and Deployment AZ-101 Microsoft Azure Integration and Security    If you have already passed the 70-533 exam you will be able to pass the following exam (until June 30):     AZ-102 Microsoft Azure Administrator Certification Transition          For who is it?    These exams are intended for system administrators and consultants who manage and maintain (Azure) cloud environments. By obtaining this certification, they demonstrate that they have the following knowledge.      Manage Azure subscriptions and resources Implement and manage storage Implement and manage virtual machines (VMs) Configure and manage virtual networks Manage identities Evaluate and perform server migration to Azure Implement and manage application services Implement advanced virtual networks Securing identities    Microsoft Certified Azure Developer    You will become Microsoft Certified Azure Developer at the moment you successfully pass the exam below:     AZ-203 Developing Solutions for Microsoft Azure       This certification path does not have a transition exam like the \"Microsoft Certified Azure Administrator\" path for the simple reason that only one exam is needed. During the development of this path, there were two exams for a long time (AZ-200 and AZ-201), but feedback from the people who took the exams led to the merging of the certifications into one overarching certificate.    For who is it?    his certification path is intended for developers who develop cloud solutions such as applications and services. With this certification they show that they have the following knowledge.     Development of Azure Infrastructure as a Service solutions Development of Azure Platform as a Service solutions Develop Azure storage solutions Implement Azure security Monitor, troubleshoot and optimize solutions  Connect and use Azure services and third-party services    Microsoft Certified Azure Solutions Architect    Architects can obtain the Microsoft Certified Azure Solution Architect certification by taking the following exams:     AZ-300 Microsoft Azure Architect Technologies AZ-301 Microsoft Azure Architect Design    As with the Microsoft Certified Azure Administrator certification, both exams must be passed for the Microsoft Certified Azure Solutions Architect predicate.    If you have already passed the 'old' 70-535 exam, an upgrade path is available:     AZ-302 Microsoft Azure Solutions Architect Certification Transition (transitie 70-535)       For who is it?    The certification path \"Azure Solutions Architect\" is for architects who advise stakeholders and translate business requirements into safe, scalable and reliable solutions.    By getting this certification they show that they have the following knowledge.     Implement and configure infrastructure Implement workloads and security Develop and implement applications Securing data Implementations for the cloud Determine workload requirements Designing identities and security Design data platform solutions Design Business Continuity strategy Design for implementation, migration and integration    Microsoft Certified Azure DevOps Engineer    A completely new path is that of \"Microsoft Certified Azure DevOps Engineer\". You obtain this certification when you have passed the following exam:     AZ-400 Microsoft Azure DevOps Solutions    When you want to obtain this certification, you are obliged to successfully complete the certification \"Azure Developer Associate\" or \"Azure Administrator Associate\".       For who is it?    The Microsoft Azure DevOps Engineer certificate is aimed at DevOps Professionals who combine people, processes and tools to continually deliver value to meet the needs of users and business objectives. By passing this exam they demonstrate that they have the following knowledge.     Designing a DevOps strategy Implement DevOps development processes Implementing Continuous Integration (CI) Implementing Continuous Delivery (CD) Implement dependence management Implement application infrastructure Implement continuous feedback    Blog originally published in dutch on the website of Microsoft Gold Learning Partner&nbsp;3fifty. If your are based in the Netherlands take a look at the website.    ","categories": ["Azure"],
        "tags": ["Azure","Certification","Learning","Microsoft"],
        "url": "/2019/01/the-new-azure-certification-paths-from-microsoft/",
        "teaser": null
      },{
        "title": "Getting Started with Azure Monitoring via AIMS",
        "excerpt":"Since a couple of weeks I'm part of the AIMS Performance Pro team because of my knowledge on Azure and other related topics.  The AIMS performance Pro team (#aimsperformancepro) is a program for ambitious and outspoken technical professionals who have interests in the intersection of IT integration, monitoring, analytics, performance tuning and troubleshooting.     It is a great honor for me to be part of the team, but I also want to add that you do not get sales talk about AIMS. In all my post I will give my own opinion both good or bad.     To give a clear opinion I started to try out the monitoring solution of AIMS for Azure. This blog post will get you up and running and in the future I will publish other post on what I discover with AIMS.    But what is so special about the monitoring solution of AIMS. Here is what they mention on their website.    AIMS it is  the only performance monitoring and governance solution that applies AI and machine learning to give you complete control over your Azure environment. With AIMS, you can&nbsp;automatically detect anomalies&nbsp;in your Azure Services,&nbsp;correlate issues&nbsp;across cloud and on-prem environments and&nbsp;actively control your Azure consumption costs.   AIMS    On the website you can also see which Azure Services they currently (01-2019) monitor:     Azure Web Apps API Management Cache/Redis Virtual Machines: Classic Compute Network/Network Interfaces Virtual Machines: Compute/Disks  Network/Public IP Addresses Cosmos DB Virtual Machines: Compute/VMs Event Hub Key Vault Logic Apps  Azure Billing Data  Service Bus Signal R Service Azure SQL DB Azure Storage App Service Plans Azure Functions    This requires a test-drive in my opinion.    Getting Started    To get started you will need a service principal in Azure Active Directory for AIMS to read all the information within your Azure Subscription. In this blog series we will give this service principal \"Reader\" rights on a management group that contains all my subscriptions. Because I want to monitor all my subscriptions.     Open the Azure Portal (https://portal.azure.com) Go to Azure Active Directory. Click on \"App Registrations\" and click on \"New Application Registration\" Fill in a name for the application. You could use \"AIMS Monitoring Application\" and set the type to \"Web App / API\" and fill in a random URL.          When the application is created the application blade will be shown. From this blade copy the \"Application ID\" to a notepad for later use. Click on \"Settings\" and then \"Keys\" and add a new key. Copy this key for later use.         Access within Azure    As mentioned before the application needs the \"Reader\" role within the subscription. As I have multiple subscriptions I have a management group that contains all of my subscriptions. To configure the \"Reader\" role on all subscriptions:      Open the \"Management Groups\" blade. (All services &gt; Management Groups) Select the correct Management Group and in the management group click on \"details\".          In the details blade click on \"Access Controls\" and then \"Role Assignments\". Click on \"add role assignment\" to add a specific role assignment. Within the role assignments blade select the appropriate role and application.         AIMS Environment    With the application configured we can add a subscription to an AIMS environment. To test this all out I requested a test environment from AIMS via the following link https://www.aims.ai/aims-free-edition. If you already have an account use this URL to login https://login.aimsinnovation.com/environments.    When you have the environment and also created a environment in there you are able to set it up for monitoring your subscription.      In your AIMS environment open the menu by clicking the gear icon.          In the menu click on \"Agents\"           On this page all of the already configured agents shown. Click on \"Connect Agent\" to add a new agent (subscription).          In the connect agent window make sure you select the \"Azure agent\" as your agent type and supply all the information of your subscription and of the newly created service principal and click on \"Create\" to start monitoring your subscription.       If you configured everything correctly information about your subscription will be retrieved and added to the portal. This can be checked by going to the \"Topology\" tab. In this tab you should see resource group names that are in the subscription. (Be patient it can sometimes take a while. The first time it took me about 5 minutes).       With the agent configured I'm really curious of what the system can do for me. In future post I will keep you updated of some of the features and my findings regarding the solution.   ","categories": ["Azure"],
        "tags": ["AIMS","Azure"],
        "url": "/2019/01/getting-started-with-azure-monitoring-via-aims/",
        "teaser": null
      },{
        "title": "Trigger a Pipeline from an Azure DevOps Pipeline",
        "excerpt":"In some situations,  it can be useful to trigger an Azure DevOps pipeline from a pipeline.  Triggering a pipeline can be done via the API and trough PowerShell. You can  write your own script file and use the PowerShell tasks but there is also a  custom task in the Azure DevOps marketplace:     Trigger Azure DevOps pipeline    With this task you can trigger a build or release pipeline from another pipeline within the same project or organization but also in another project or organization.    Personal Access Token    To get  started a Personal Access Token is needed with the appropriate rights to execute  pipelines. To generate a new Personal Access Token follow the below guide:     Use personal access tokens to authenticate     Give the personal  access token the following rights depending on your scenario:     Triggering a Release:  Release – Read, write &amp; execute Build - Read &amp; Execute (Needed for reading the artifacts)   Triggering a Build:  Build - Read &amp; execute      Copy the token for later use.    Configure extension    When you  have installed the extension, you can start by altering a pipeline from where  you want to trigger a different pipeline.     Add  a new task to the pipeline by clicking in “+” icon. In  the task window search for “Trigger” and select the task “Trigger Azure DevOps  pipeline”. In  the task click on “New” next to Azure DevOps Service connection to create a new  connection.        In the new service connection window fill in all the correct properties.  Connection name: The name for the connection   Organization URL: The URL of the organization for Azure DevOps this would be https://dev.azure.com/[organization name] Release URL: The URLfor the release API for Azure DevOps this would be  https://vsrm.dev.azure.com/[organization name]  Personal Access Token: The personal access token that needs to be used.            In  the task choose the right options. The field for the project, build and release  definitions should be filled in for you.    When you want  to trigger a release for a specific version make sure to fill in the build  number. When left empty the extension will use the latest version by default.    Besides that,  it is also possible to trigger a build for a specific branch. When you want to  make use of that option make sure you fill in the “Branch” property.   ","categories": ["Azure DevOps"],
        "tags": ["Azure DevOps","Pipeline","Trigger"],
        "url": "/2019/02/trigger-a-pipeline-from-an-azure-devops-pipeline/",
        "teaser": null
      },{
        "title": "Securing applications with the Azure Key Vault and Azure DevOps",
        "excerpt":"When developing applications for Azure security it always one of the items you need to cross of your list. Many security requirements can be solved by embedding the Azure Key Vault within your application. The Azure Key Vault can help you solve the following problems:     Secrets Management Key Management Certificate Management Store secrets backed by Hardware Security Modules    If you centralize the secrets in a Key Vault it allows you to control the distribution of those secrets and also manage who is allowed to get, list (many more actions) these secrets. In the most perfect situation a team member of the DevOps team does not even know those secrets. He only deploys the Azure resources via an Azure Pipeline and the rest is managed by another team. The only thing that needs access to the Key Vault in this situation is the application used by the Azure Service Connection.    To make the actions against the Azure Key Vault more easy I developed a extension called \"Azure Key Vault actions\" with this extension added to your build or release pipeline these actions can be performed:     Get Azure Key Vault secret value Add / Update Azure Key Vault secret Remove Azure Key Vault secret Add access policy Remove access policy Import Azure Key Vault certificate Get Azure Key Vault certificate Url    This extension solves a problem for maintaining and deploying large environments with Azure DevOps. For example an environment that you are deploying uses Azure App Services with a managed identity. This Identity may need automatic access to the Key Vault to retrieve secrets.    In Azure DevOps there is a default integration with the Azure Key Vault. I found that this integration does not offer all the actions that you may want to carry out.    Getting started    To get started with the extension follow the below steps.     Go to the Azure DevOps market place:&nbsp;https://marketplace.visualstudio.com/azuredevops Search for the extension: “Azure Key Vault actions” or use this direct link:&nbsp;https://marketplace.visualstudio.com/items?itemName=maikvandergaag.maikvandergaag-azurekeyvault Click on the “Get It Free” button and follow the installation instructions for the extension.         Azure Key Vault actions task    With the task installed it can be added to a Build or Release pipeline. Depending on the actions you choose you need to fill in the following properties:     Azure RM Subscription: The subscription / service connection to connect to. Action: The specific action to perform on the specified Key Vault. Key Vault name: The name of the key vault. Secret name: The name for the secret. Secret: The secret value, for this value you should use a secured variable within Azure DevOps Pipelines. Certificate name: Name of the certificate to retrieve or import. Certificate file: The certificate file to import. Certificate password: The password of the certificate. ObjectId: The objectId of the object in Azure Active Directory to give access to or remove from the access policies. Permissions to keys: Permissions to the keys separated by ',' Permissions to secrets: Permissions to the secrets separated by ',' Permissions to certificates: Permissions to the certificates separated by ',' Permissions to storage: Permissions to the storage separated by ',' Variable name: The name of the variable to save the results to. Overwrite: Overwrite the secret or certificate if it already present is in the Key Vault.    The task needs a Azure RM Subscription endpoint. The endpoint uses a Azure Active Directory application in the background. When using the task make sure this application has the appropriate role assignments on the Azure Key Vault (role assignment: owner).         Support    I maintain the extension in my free time and the sources of the extension can be found GitHub. If you encounter any issues or would like to see different features, please let me know by adding an issue to GitHub repository.     https://github.com/maikvandergaag/msft-extensions/issues    Extensions    Besides this extension I created several others. More information about these extension can be found here:        ","categories": ["Azure DevOps"],
        "tags": ["Azure","Azure DevOps","Extensions","Key Vault"],
        "url": "/2019/02/securing-applications-with-the-azure-key-vault-and-azure-devops/",
        "teaser": null
      },{
        "title": "Combining SonarQube and Azure DevOps",
        "excerpt":"Code analysis is a best practice in a operating continuous integration pipeline. SonarQube can be used in combination with Azure DevOps. If you do not know SonarQube, it is tool that centralizes static code analysis and unit test coverage. It can be used across multiple languages and for a single project up to enterprise scale.     SonarQube  can be used as a SaaS product or hosted on your own instance. SonarQube describes  the product as followed:    SonarQube provides the capability to not only show health of an application but also to highlight issues newly introduced. With a Quality Gate in place, you can fix the leak and therefore improve code quality systematically.     This sound interesting and when useful for your situation it should be placed within a continuous integration pipeline, but how do we get started.    SonarQube Deployment    To get  started with SonarQube I used the installation on an Azure App Service created  by a premium field engineer from Microsoft. The blog post below contains an ARM  template that really makes it a single click install.     SonarQube  Hosted On Azure App Service - https://blogs.msdn.microsoft.com/premier_developer/2018/12/23/sonarqube-hosted-on-azure-app-service/    What you  might wonder is what the reason was why I used the Azure App Service  deployment:     No  license needed You  do not have to create and manage a Virtual Machine. Setting  up SSL is easier. You  can easily integrate other Azure services like the Azure Key Vault. Other  features of the Azure App Service could be used like deployment slots.    Note: When using the “Deploy to Azure” button and you  are using a resource group naming policy you temporary disable it. The “Deploy  to Azure” method uses a test that creates a resource group with a GUID to  validate the template.    Deploying  the resource will not take long. Starting SonarQube is totally different  depending on the hosting platform it can take up to 15 minutes.         When the tooling  is started login with the admin credentials (admin/admin). Make sure you  change them after your first login.    SonarQube Configuration    For sending information to SonarQube a token is needed. To generate a token, click on your name in the top right corner and select “My Account”.         On the “My  Account” screen select “Security”.         Fill in a  name for the token and click on generate. Copy the token for later use. The  next step is to create a new project within SonarQube. The project will be the  centralized storage for your analytics information of the code. To create a new  project, click on the “+” sign next to your name.    Fill in the  appropriate information and select the main language of your project.         The project  is now ready, and we can start to configure Azure DevOps to send the analysis information  to your SonarQube environment.    Azure DevOps Extension    To make use  of SonarQube within Azure DevOps an extension needs to be installed. Here you  can find the links to the extensions for SonarQube:     SonarCloud  (SaaS version SonarQube)- https://marketplace.visualstudio.com/items?itemName=SonarSource.sonarcloud SonarQube  (Hosted version. The version we will use in this blog post)- https://marketplace.visualstudio.com/items?itemName=SonarSource.sonarqube    Build Pipeline    In your Azure  DevOps project create a new pipeline or open a pipeline that you want to extend.  Press the button to add a new task and search for Sonar you will see the  following available tasks. The three-task selected are for the hosted version  of SonarQube.         We will  start with the “Prepare Analysis Configuration” then the “Run Code Analysis”  and then the “Publish Quality Gate Result”.    Prepare Analysis  Configuration    The  “Prepare Analysis Configuration” is the most important task. With this task you  configure the agent job to work correctly with SonarQube.         To  establish the connection the tasks needs a SonarQube service endpoint. To  create a new service endpoint for SonarQube you can click on the “New” button. In  the configuration window for this endpoint fill in the correct information.         In the task  itself fill in the key of the project and the name. The task is now configured this  means the other tasks can be added to the pipeline. The other task itself do  not have any additional configuration.         If you want  to include test result you need to make sure to add the “Run Code Analysis”  task after the “Test Assemblies” task like the screenshot above.         In the  upcoming days and weeks, I will check more functionality and will share more information  on this blog.   ","categories": ["Azure DevOps"],
        "tags": ["Azure DevOps","CI","SonarQube"],
        "url": "/2019/02/combining-sonarqube-and-azure-devops/",
        "teaser": null
      },{
        "title": "Meet the specified naming conventions in Azure DevOps Git",
        "excerpt":"Collaborating in a Azure DevOps Git repository with a lot of people could become a chaos regarding branch folders. At the start of a project you can specify naming conventions for the branch names. In these names you can incorporate a '/' and the tooling (Azure Devops, TFS and Visual Studio) will see this as a folder. Example for the naming conventions:     One root branch in the repository for example \"master\". Create new branches under \"/features/[feature name]\"    With naming conventions specified you should be able to enforce these rules as well. In Azure DevOps repositories you have this options by setting permissions on branch levels. Using this option also allows you to configure specific names for specific groups of users.    Prerequisites     Team Foundation version control command (tf.exe). This tooling is normally installed when using the Visual Studio Developer Command Prompt. An existing Azure DevOps account.         Steps    The most important thing to do first is to remove the creation of branches on the root level of the repository.     [php]tf git permission /deny:CreateBranch /group:Contributors /collection:[Azure DevOps Instance] /teamproject:[ProjectName] /repository:[Repository Name][/php]     The script above removes the permissions for the \"Contributors\" group within the Azure DevOps project. When the default permissions are removed specific permissions for folders can be added to the repository.     [php]tf git permission /allow:CreateBranch /group:Contributors /collection:[Azure DevOps Instance] /teamproject:[Project Name] /repository:[Repository name] /branch:features[/php]     The \"branch\" arguments specifies the folder name you allow within the repository. In the example permissions are added to create branches in the features folder for users in the \"Contributors\" group.        ","categories": ["Azure DevOps"],
        "tags": ["Azure DevOps","Azure Repos","branch"],
        "url": "/2019/03/meet-the-specified-naming-conventions-in-azure-devops-git/",
        "teaser": null
      },{
        "title": "How I try to keep up with technology",
        "excerpt":"Technology  is changing in a very fast pace. If we look at Microsoft technologies like  Azure, you see changes every week. Because of this I get a lot of questions  from clients and colleagues how I keep up with all the changes and new  functionalities.    Podcasts    As a  consultant I travel a lot in my car, while traveling I’m almost always  listening to a Podcast. For this I use a tool called “Pocket Cast” that I  control via Android Auto in my car.    The  Podcasts that I listen to now are:     Azure Friday Podcast The Azure Podcast Azure Flash News DevOps Radio Arrested DevOps .Net Rocks The Cloud Cast MS Dev Show Coding Blocks DevOps.fm The Cloud Architects Podcast Radio TFS Azure DevOps Podcast    Reading    Besides  listening to podcasts I read a lot of books and articles. For this I use my  e-reader that is stacked with a lot of e-books. Besides that, I sync blog  articles to the devices by using Pocket.    To get a  good overview of the blog posts that I read I use a tool called “Feedly” (https://feedly.com/i/welcome) with feedly you get a good overview of the  articles that are new and also mark them as read. On my mobile device (Google  Pixel 2) I use the feedly native application for when I’m not working on my laptop.  On my Windows 10 device I use the application Nextgen Reader.    The blogs  that I follow are:     Andrew Lock | .NET Escapades Coding Blocks Blog posts – Codit eBook Deal of the Week :: Microsoft  Press Store Scott Hanselman's Blog Networking Blog Microsoft – TechCrunch Born to Learn Jeremy Thake’s musings - Medium Microsoft Edge Blog The AI Blog Microsoft Research Microsoft Azure Blog ZDNet Stories Channel 9 Colin's ALM Corner Azure DevOps Blog Van  Haren Publishing | BLOG –  (Dutch) Henry Been Visual Studio Geeks A Developer’s Life DevOps in action Oren Novotny MackBytes Willy[-Peter] Schaub Scrum Bug Darren's Blog Sound Code - Mark Heath's Blog Azure service updates Build Azure AzureBarry Cloud for the win!    Other sources    As a consultant  I have access to the Microsoft Advisors channel were we receive information and  notification about new functionalities in Azure or Azure DevOps. Next to these  source I have access to all the training material of Microsoft as a certified  trainer. My skillpipe account is full of books (MOC) that I read to stay up to  date and certified.    Recommendation    If you have  any recommendation on podcasts or blogs that I should follow please let me know.   ","categories": ["General"],
        "tags": ["Azure","Azure DevOps"],
        "url": "/2019/03/how-i-try-to-keep-up-with-technology/",
        "teaser": null
      },{
        "title": "Is it Continuous Deployment or Continuous Delivery",
        "excerpt":"The terms  Continuous Deployment and Continuous Delivery are most of the time mistaken  with each other. The abbreviation CD is sometime used for Continuous Deployment  and sometimes used for Continuous Delivery! Hack I even use the terms in wrong  situations were I forget to mention the right one, but there is a real  difference between the two.    Continuous Deployment    As we want  to keep things simple Continuous Deployment is nothing more then deploying  every code change to production.     Continuous Delivery    To explain  Continuous Delivery, more words are needed. &nbsp;Let’s start by looking at the definition of  DevOps from Donovan Brown:    “DevOps is the union of people, process, and products to enable Continuous Delivery of value to our end users”  Donovan Brown    When looking at the definition you will notice that Continuous Delivery is an enabler for the DevOps practices. Based on this we can say that Continuous Delivery is a set of tools, processes and techniques for rapid, reliable and continuous development and delivery of software a definition defined by \"Martin Fowler\".    This really  means that Continuous Delivery means more than just releasing software. This  can also be extracted from the eight principles of Continuous Delivery:     The process for releasing/deploying  software must be repeatable and reliable. Automate everything! If something is difficult or  painful, do it more often.  Keep everything in source control. Done means “released.”  Build quality in!  Everybody has responsibility for the  release process. Improve continuously    Conclusion    Combining the eight principles and the definition of Donovan Brown makes Continuous Delivery the real enabler for the DevOps practices were the term is not only a certain form of release management but much more.      ","categories": ["DevOps"],
        "tags": ["CD","DevOps"],
        "url": "/2019/04/is-it-continuous-deployment-or-continuous-delivery/",
        "teaser": null
      },{
        "title": "The past three months of a technology junkie",
        "excerpt":"The last couple of months I haven’t been blogging as much as I would like because it has been a real roller coaster for me and my family. For some reason we always do everything at the same time. My girlfriend was pregnant of our third child and we also decided to move to a new house. On 11 march my girlfriend gave birth to a beautiful son called Jurre Finn. I'm so proud of her and happy with our new son. During this period, we also had to arrange a lot regarding our new house where we will be moving in in July. This means that I will be busy the upcoming two months as well.       Community    During the upcoming two months I will try to be as active as possible but as you can imagine it will not be as much as always.  Besides the personal stuff of the last three months I also succeeded to finish some personal objectives.     Dutch Cloud Meetup    In February  I started a usergroup called “Dutch Cloud Meetup” in this usergroup we want to  discuss various cloud related topics.    The first meetup is scheduled for 12 June! I hope to see you there.    https://dutchcloudmeetup.online    SDN Event    I will be presenting on Azure DevOps extensions at the SDN event of 14 June in the Netherlands.    https://www.sdn.nl/EVENTS/14-juni-2019    ALM | DevOps Rangers    In March I  became one of the ALM and DevOps Rangers. The ALM | DevOps rangers is a group  of people that provide practical learning, guidance, experience, and  open-source solutions for the ALM and DevOps community.      https://www.almdevopsrangers.org/  https://www.almdevopsrangers.org/introducing-the-alm-devops-rangers-maik-van/    As mentioned above I will be busy until the end of July and try to do as much as possible but from August on I will be back in business and restart my blogging activity.    ","categories": ["Azure","General"],
        "tags": ["Azure","DevOps","Dutch Cloud Meetup"],
        "url": "/2019/05/the-past-three-months-of-a-technology-junkie/",
        "teaser": null
      },{
        "title": "On-demand Azure Policy Scan",
        "excerpt":"When moving to a DevOps way of working it is important to have a good set of rules on how to work with software and infrastructure.  When looking into Azure this rule set can be converted into Azure Policy and policies rules.     Azure policies enforce rules and effects over the resources, so those resources stay compliant with your standards and service level agreements. Azure Policy meets this need by evaluating your resources for non-compliance with assigned policies. For example, you can have a policy to allow only a certain location in your platform. Once this policy is implemented, new and existing resources are evaluated for compliance. For more information look at the documentation here.    As mentioned policies govern your Azure Subscription. The compliance of the platform is checked via the Portal or command line scripting. But when does the platform check the compliance of the resources?    Compliance check    The evaluation of the policies take place on the following events:     New policy assignment. ( ~30 minutes). Updated assignment of a existing policy. (~ 30 minutes) Deployment of a resources. (~15 minutes) Every 24 hours Update of the resource provider  On-demand (~3 minutes)    As described on the above list it can take a while for the policies are checked. If you want the fasted possible feedback you will need to do an on-demand scan.    On-demand    Via the rest API a on-demand scan of the policies can be triggered. When using this method information about the compliance will be available in around 3 minutes. Triggering can be done on resource group or on a Azure subscription.    https://management.azure.com/subscriptions/{subscriptionId}/providers/Microsoft.PolicyInsights/policyStates/latest/triggerEvaluation?api-version=2018-07-01-preview  Subscription (Post)    https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{ResourceGroup}/providers/Microsoft.PolicyInsights/policyStates/latest/triggerEvaluation?api-version=2018-07-01-preview  Resource Group (Post)    PowerShell    These API calls can be added in a simple PowerShell script to trigger the evaluation. The below example is a script to trigger the evaluation on every subscription you have access to.    $account = Get-AzContext if ($null -eq $account.Account) {     Write-Output(\"Account Context not found, please login\")     Connect-AzAccount }  $subscriptions = Get-AzSubscription  foreach($subscription in $subscriptions){     Set-AzContext -Subscription $subscription      $SubscriptionId = $subscription.Id      $azContext = Get-AzContext     $azProfile = [Microsoft.Azure.Commands.Common.Authentication.Abstractions.AzureRmProfileProvider]::Instance.Profile     $profileClient = New-Object -TypeName Microsoft.Azure.Commands.ResourceManager.Common.RMProfileClient -ArgumentList ($azProfile)     $token = $profileClient.AcquireAccessToken($azContext.Subscription.TenantId)      $authHeader = @{         'Content-Type'='application/json'         'Authorization'='Bearer ' + $token.AccessToken     }      $restUri = \"https://management.azure.com/subscriptions/$SubscriptionId/providers/Microsoft.PolicyInsights/policyStates/latest/triggerEvaluation?api-version=2018-07-01-preview\"  ","categories": ["Azure","DevOps"],
        "tags": ["Azure","Azure Policy","DevOps"],
        "url": "/2019/06/on-demand-azure-policy-scan/",
        "teaser": null
      },{
        "title": "Review of 2019",
        "excerpt":"For a couple of years, I have been doing a lot of community  work. The most of it was blogging on the various platforms I had during the  last decade. This year I wanted to take it even further and do a lot more  things in which I failed miserably.    Family    2019 has been on great and amazing on one side and a very hard one on the other side. Personally, I had great moments like the birth of my son Jurre but also a lot of worrier about my kids. But he, if I didn’t want to worry about my kids, I shouldn’t have taken them right ?. One thing is for certain they are the best thing that has ever happened to me.    One thing is also trough because of the worries and the restful  nights I have got into as much stuff as I have wanted that is a pity, but family  comes first and that leaves me to an even better 2020.    Dutch Cloud Meetup    In January I started one of my long-lasting ideas in  reaching out to the community on a regular base but also suppling the community  with regular events in and around Rotterdam. I made this choose because most of  the events in the Netherlands are held in Amsterdam which is a hassle to travel  to after a hard day of work.    That why I founded the Dutch Cloud Meetup and asked Henry  Been to help me organize event. This year we already had two great events. One  at my employer 3fifty and another at Ibis. For the year 2020 we have even more events  planned and are working for a large event in March with an international speaker.    Author    Another opportunity I got this year was the ability to be a technical  reviewer of a book that is being written by Henry Been. After reviewing a  couple of chapters I have even been asked as a Co-author for a chapter in the  book. I accepted and the book will be published somewhere in the beginning of  2020.    Speaking Engagements    Next to blogging I wanted to start with speaking as different  events. Because of personally situations I have not been able to speak at as  many events as I wanted but have been able to do a few:     Govern your Azure platform with Azure Policies @  Dutch Cloud Meetup Evolving PowerShell scripts into Azure Pipeline  extensions @ SDN Event    If you are ever looking for a speaker that a look at my  speaker profile on Sessionize.    Forthcoming    All together it has been an amazing year with a lot of opportunities  and other nice things. All together I can’t wait until 2020 will start and all  the nice things I will be able to do and start.    In 2020 you can also expect a lot more blog post from me.   ","categories": ["General"],
        "tags": ["2019","Review"],
        "url": "/2019/12/review-of-2019/",
        "teaser": null
      },{
        "title": "Testing ARM templates",
        "excerpt":"A few weeks ago I was looking at ways how I could test my ARM  templates. During my work I create a lot of these templates and it always a lot  of work to check them and comply with the best practices. Especially when we  work in teams and everybody has his own way of creating the templates.    After some research I found a blog post of: Olivier Miossec (a Microsoft Azure MVP). He was mentioning a tool called “ARM-TTK” which is being developed by Microsoft and is still in preview.    Reading this made me test the tool out.    ARM-TTK    ARM-TTK is written as a PowerShell Module that can be run in  Windows or Linux. When using Linux PowerShell Core needs to be installed.    The module is still in preview but Microsoft is actively  developing it on GitHub. Using this option everyone is allowed to test it and use  it in there on scenario’s.    To test it download the folder from the GitHub repository:     https://github.com/Azure/azure-quickstart-templates/tree/master/test/arm-ttk    As you may know with the downloaded folder you can Import the module. As there is no online location yet you have to reference the file name.    Import-Module '[Path to the Module]'   Executing ARM-TTK    With the module loaded in our PowerShell session it is time to execute some test against some of the ARM templates I have. To run a test run the following command:     Test-AzTemplate 'D:\\temp\\azuredeploy.json'   Running this command will give a result like the image below.       The tool uses a testing framework called Pester to test  certain scenario’s against the ARM template. Besides that the tool doesn’t use  the Azure context at this moment making it a very quick test framework with a  lot of great result. Some of the things it checks are:     JSON validation apiVersions  Should Be Recent: It will also show recent version you can use. artifacts  parameter: It checks the artifact parameters and whether they are used within  the template. Outputs Must Not Contain Secrets Parameters Property Must Exist Virtual Machines Should Not Be Preview    All together this looks like a really great tool that can be  extended very quickly and is really useful. Next step is creating this  extension within a Azure DevOps Pipeline task in order to use it CI and CD  scenario’s.   ","categories": ["Azure"],
        "tags": ["ARM","Azure"],
        "url": "/2020/01/testing-arm-templates/",
        "teaser": null
      },{
        "title": "Testing ARM Templates in Azure DevOps",
        "excerpt":"Two days Ago I published the post \"Testing ARM Templates\"  that mentioned the ARM-TTK PowerShell Module Microsoft is creating to test ARM templates. After writing the post I couldn’t resist creating a Azure DevOps Pipeline tasks that integrates the tooling.    ARM Template Tester    With the “ARM Template Tester” you have the option of testing your Azure Resource Manager templates within a build or release pipeline. The tasks itself does not have a lot of parameters that need to be set in order to use the extension:     Template folder: The folder for testing the ARM templates Stop on violation: Stop pipeline execution on a violation of the rules in the template       The extension itself does not have a lot of functionality at  this moment but I will try to extend it in the next couple of weeks/months. If  you have a great idea or want to help extending this task send me a email a  file an issue within GitHub.    Useful links     GitHub Task repository:  https://github.com/maikvandergaag/msft-extensions  Azure DevOps Task – ARM Template Tester:  https://marketplace.visualstudio.com/items?itemName=maikvandergaag.maikvandergaag-arm-ttk  ARM-TTK repository:  https://github.com/Azure/azure-quickstart-templates  GitHub Issues list:  https://github.com/maikvandergaag/msft-extensions/issues    ","categories": ["Azure DevOps"],
        "tags": ["ARM","Azure DevOps"],
        "url": "/2020/01/testing-arm-template-in-azure-devops/",
        "teaser": null
      },{
        "title": "Connecting to the Resource Graph Client via the Resource Graph SDK",
        "excerpt":"In order to retrieve information from Azure Subscriptions someone at a client wanted to use the Resource Graph API. For this they were building an ASP.Net Core application and where using the C# SDK. The problem that they were having was related to the authentication.     Azure Resource Graph    The resource graph is a service in Azure that is designed to extend Azure Resource Management by providing efficient and performant resource exploration with the ability to query at scale across a given set of subscriptions so that you can effectively govern your environment.    For this you make use of kusto queries that you send to the API.    More information about the Azure Resource Graph: https://docs.microsoft.com/en-us/azure/governance/resource-graph/overview    Implementation    To connect to the Resource Graph an authentication object needs to exist. We wanted to use a service principal. The process using this code is a background job and did not have any user interaction.    To use the Resource Graph within a C# application add the following NuGet packages:      Microsoft.Azure.Management.ResourceGraph: SDK for C# applications.   Microsoft.Azure.Services.AppAuthentication: Package for handling the authentication.    The resource graph NuGet package contains the “ResourceGraphClient” that is needed to perform queries against the resource graph API.     To be able to initiate a “ResourceGraphClient” it requires an object called “ServiceClientCredentials”. The \"ServiceClientCredentials\" object is a base class for the \"TokenCredentials\" object that we can construct for the service principal.    Note: The demo code is a simple setup. When using this in production you should save secure value in a vault and configuration values in a config file.     First off, we create a helper class with a method that will retrieve the \"ServiceClientCredentials\" object.    using Microsoft.IdentityModel.Clients.ActiveDirectory; using Microsoft.Rest; using System.Threading.Tasks;  namespace GraphClient {     public static class AuthenticationHelper     {         public static async Task&lt;ServiceClientCredentials> GetServiceClientCredentials(string resource, string clientId, string clientSecret, string authority)         {             AuthenticationContext authContext = new AuthenticationContext(authority);              AuthenticationResult authResult = await authContext.AcquireTokenAsync(resource, new ClientCredential(clientId, clientSecret));              string accessToken = authResult.AccessToken;              ServiceClientCredentials serviceClientCreds = new TokenCredentials(authResult.AccessToken);              return serviceClientCreds;         }     } }   The class contains a method to retrieve the required object. This is done by specifying the authentication context (the Azure AD tenant) and retrieving a token by specifying the resource \"https://management.core.windows.net\" (default for the resource graph API), the client id and client secret. With the token we can initiate the \"TokenCredentials\" object that derives from the \"ServiceClientCredentials\" class.    With this class and method in place we will develop the other peace needed to perform the queries.     using Microsoft.Azure.Management.ResourceGraph; using Microsoft.Azure.Management.ResourceGraph.Models; using System; using System.Collections.Generic;  namespace GraphClient {     class Program     {         static void Main(string[] args)         {             string clientId = \"[clientid]\";             string clientSecret = \"[clientSecret]\";             string tenantId = \"[tenantId]\";             string subscriptionId = \"[subscriptionId]\";             string query = \"Resources | project name, type | limit 5\";             string authority = $\"https://login.microsoftonline.com/{tenantId}\";              ResourceGraphClient client = new ResourceGraphClient(AuthenticationHelper.GetServiceClientCredentials(\"https://management.core.windows.net\", clientId, clientSecret, authority).Result);              var response = client.Resources(new QueryRequest(new List&lt;string>(){ subscriptionId }, query));              Console.WriteLine(response);         }     } }   This part of the code initiates the \"ResourceClient\" class that will execute a Kusto query via the \"Resources\" mehod.   ","categories": ["Azure","Development"],
        "tags": ["Azure","C#","Resources"],
        "url": "/2020/01/connecting-to-the-resource-graph-client-via-the-resource-graph-sdk/",
        "teaser": null
      },{
        "title": "Managing access control for Logic Apps",
        "excerpt":"In some situations access to the workflows needs to be controlled. This has to be done in some way to make sure only specific people can trigger or see the content of it. These situations are mostly applied to Logic Apps that can be triggered via a external endpoint.    One of the most known options is the Integration Service  Environment which also gives the option of connecting your Logic App to your  private virtual network., but this post will not go into any details about this  but will focus on the true PAAS options. If you want to know more about integration  service environment read this post:    https://azure.microsoft.com/nl-nl/blog/announcing-azure-integration-service-environment-for-logic-apps/    Access Control Control Configuration    Within the \"Workflow Settings\" the access control for a Logic App can be managed. Via this configuration you have three options of providing access to the workflow.       Any IP    By default \"Any IP\" is selected for a Logic App . As you might think this option allows traffic from any location.    ARM    This option does not require any specific configuration within a ARM template of a Logic App.    IP Ranges    The second option within the access control configuration is “Specific IP ranges”. This option will allow access and content restriction to the specified IP ranges.    ARM    Allowing access to specific IP ranges can be done for triggers, actions and content. Content will be mentioned in the last paragraph.    \"resources\": [     {         \"name\": \"[parameters('LogicAppName')]\",         \"type\": \"Microsoft.Logic/workflows\",         \"location\": \"[parameters('location')]\",         \"tags\": {},         \"apiVersion\": \"2016-06-01\",         \"properties\": {             \"definition\": {},             \"parameters\": {},             \"accessControl\": {                 \"triggers\": {                     \"allowedCallerIpAddresses\": [                         {                             \"addressRange\": \"10.0.0.0/24\"                         }                     ]                 },                 \"actions\": {                     \"allowedCallerIpAddresses\": [                         {                             \"addressRange\": \"10.0.0.0/24\"                         }                     ]                 }             }         }     } ]   Only other Logic Apps    Another option &nbsp;within Logic Apps is “Only other Logic Apps”. This option restricts trigger access to only other workflows within Azure. This scenario is mainly used when you are creating Logic Apps and have specific actions that are handled by a separate workflow.    ARM    Restricting a Logic App trigger to only other Logic Apps is done by not specifying any IP ranges within the ARM configuration.    \"resources\": [     {         \"name\": \"[parameters('LogicAppName')]\",         \"type\": \"Microsoft.Logic/workflows\",         \"location\": \"[parameters('location')]\",         \"tags\": {},         \"apiVersion\": \"2016-06-01\",         \"properties\": {             \"definition\": {},             \"parameters\": {},             \"accessControl\": {                 \"triggers\": {                     \"allowedCallerIpAddresses\": []                 },                 \"actions\": {                     \"allowedCallerIpAddresses\": []                 },                 \"contents\": {                     \"allowedCallerIpAddresses\": []                 }             }         }     } ]   IP ranges for contents    Within Access control configuration there is also an option restrict access and calls to get the input and output messages from run history to a specific range of IP addresses. This will disallow access to IPs that are not allowed to see the content within the run history which is a interesting option for Logic Apps with specific data.    ARM    Setting content restriction can also be done via ARM by specifying the the \"contents\" object within the access control of the resource template.    \"resources\": [    {       \"name\": \"[parameters('LogicAppName')]\",       \"type\": \"Microsoft.Logic/workflows\",       \"location\": \"[parameters('location')]\",       \"tags\": {},       \"apiVersion\": \"2016-06-01\",       \"properties\": {          \"definition\": {},          \"parameters\": {},          \"accessControl\": {             \"triggers\": {                \"allowedCallerIpAddresses\": [                   {                      \"addressRange\": \"10.0.0.0/24\"                   }                ]             },             \"actions\": {                \"allowedCallerIpAddresses\": [                   {                      \"addressRange\": \"10.0.0.0/24\"                   }                ]             },             \"contents\": {                \"allowedCallerIpAddresses\": [                   {                      \"addressRange\": \"10.0.0.0/24\"                   }                ]             }          }       }    } ]  ","categories": ["Azure"],
        "tags": ["ARM","Logic Apps","Security"],
        "url": "/2020/02/managing-access-control-for-logic-apps/",
        "teaser": null
      },{
        "title": "What can DevOps do for you",
        "excerpt":"Many organizations say they use DevOps, they see this as a methodology just like Scrum and Agile. However, DevOps is not a methodology but more a culture. The actual doing DevOps and living up to its principals will change the organization and his culture. In this blog I will give you more clarification regarding the DevOps way of working and what it can do for an organization.    What is Devops?    We are talking about DevOps here, but what is it? Many people refer to the collaboration between developers (dev) and administrators (operations). Over time, additional names such as “DevSecOps” and “BizDevOps” have been added. The collaboration between the various departments is the connecting factor here. Collaboration is a big part of the change you have to endure as an organization. This collaboration already says a lot, but it is not all that it is.    In the Microsoft world people know the definition of Cloud Advocate “Donovan Brown”. This definitions is from my opinion the real one.     “DevOps is the union of people, process, and products to enable continuous delivery of value to our end users.”  Donovan Brown    As you can read it is not only about the cooperation between developers and administrators, but about the cooperation between different people / departments in order to continuously deliver value of the end users.       The above image shows an infinite loop, this is something developers try to prevent originally, but it is important within DevOps that you as a team make a product and you also maintain it: \"You build it, you run it!\"    DevOps-team    A DevOps team consists of all the roles necessary to create and manage an application, for example. It is important that the team can continuously guarantee quality. This means that such a team can consist of many roles such as; a security officer, product owner, developer, administrators and functional specialists.    The biggest change is that these roles are normally spread within an organization and now effectively come together and start working together.    It is important for a DevOps team that the team is able to operate independently and effectively in order to deliver added value as quickly as possible. If DevOps principles are applied, such as: Continuous Delivery, Continuous Monitoring / Learning and Continuous Integration, a team can really accelerate, resulting in a shorter “time to market”. Besides this the quality of the products are being continuously guaranteed.    What are the advantages?    Working with DevOps teams offers many advantages; A team that is able to work efficiently and faster than, for example, traditional teams. By applying different principles such as Continuous Learning / Monitoring, the quality of products is also better.    This is mainly due to the fact that the team is continuous learning from the changes that are being made, in addition the interaction with the end customer is also more regular. This way they can ask themselves the question; is this functionality that we have developed really appreciated or is it better to remove it?    How do we apply it?    As mentioned, integrating DevOps into an organization requires a culture change or at least a change in mindset. It requires a different way of thinking and acting. To begin with teams, it is advisable to start small were in the value of DevOps can be proven and where it can be shown what the DevOps mindset can do for an organization.    To actually start with this, the tooling for such a team is also very important. By means of tooling such as Azure DevOps, a DevOps team can work successfully and efficient.    Blog originally published in dutch on the website of Microsoft Gold Partner 3fifty. If your are based in the Netherlands take a look at the website.      ","categories": ["Azure"],
        "tags": ["Azure DevOps","DevOps"],
        "url": "/2020/06/what-can-devops-do-for-you/",
        "teaser": null
      },{
        "title": "Book: Implementing Azure DevOps Solutions",
        "excerpt":"The last year I have worked with Henry Been to write the book: Implementing Azure DevOps Solutions. Henry has written the largest part of the book were I was responsible for chapter 12 and part of the reviewing.    Implementing Azure DevOps solutions helps DevOps engineers and administrators use Azure DevOps Services to master practices such as continuous integration and continuous delivery (CI / CD), containerization, and downtime deployments.    The book: Implementing Azure DevOps Solutions, by Henry Been and Maik van der Gaag starts with the foundation of continuous integration, continuous delivery and automated implementations. You will then learn how to apply configuration management and infrastructure as code (IaC), along with managing databases in DevOps scenarios. Next, you will discuss adjusting security and compliance with DevOps. As you progress, you will discover how to instrument applications and collect statistics to understand application usage and user behavior.     Container build strategy    The last part of this book helps you implement a container build strategy and manage Azure Kubernetes Services. Finally, understand how to create your own Azure DevOps organization, along with quick tips and tricks to apply effective DevOps practices with confidence.     By the end of this book, you will have gained the knowledge you need to ensure seamless application deployment and business continuity.     Characteristics     Discover a step-by-step approach to designing and creating a successful DevOps environment Understand how to deploy continuous integration and continuous deployment pipelines on Azure Integrate and implement security, compliance, containers and databases in your DevOps strategies    Interested in the book buy your copy at Packt or Amazon   ","categories": ["Azure"],
        "tags": ["Azure DevOps","Book"],
        "url": "/2020/10/book-implementing-azure-devops-solutions/",
        "teaser": null
      },{
        "title": "Getting started with Project Bicep for Azure ARM",
        "excerpt":"Bicep is at the time of writing an experimental language that you can be used to simplify the writing of ARM templates. Bicep is a so called DSL (Domain Specific Language) meaning that it is a specific language for a specific domains in this case ARM.    In short Bicep is a abstractions over the Azure Resource Manager and ARM templates which means that any thing that can be done in ARM templates should also be possible in Bicep.    Concrete this means that Bicep files need to be compiled to ARM json templates meaning that ARM effectively is an Intermediate Language.    Why do we want something like Bicep    Bicep is developed with the goal of being simpler and easier to understand than ARM templates.&nbsp; Besides that by hiding a lot of options it should be easier to read as well. But let’s check it out our selves to see if this is correct.    Really getting started    To get started with Bicep the compiler / tool needs to be installed on the machine.     Installation documentation: https://docs.microsoft.com/en-us/azure/azure-resource-manager/bicep/install    If you directly want to install Bicep for windows you can also use this link:     Windows: https://github.com/Azure/bicep/releases/latest/download/bicep-setup-win-x64.exe    The installation itself is very straight forward and will install the Bicep CLI.       Once installed, the installation can be validated by opening a Windows terminal and typing the following command:    bicep --help        For Bicep there is also a extension for you favorite IDE: Visual Studio Code. To make it easier to write Bicep files install the extension.     https://github.com/Azure/bicep/releases/latest/download/vscode-bicep.vsix    When trying to install the VSIX remember that you cannot double click the extension but that you need to install it from Visual Studio Code itself:       The first bicep file    With all pre-requisites installed we can create our first Bicep file in Visual Studio Code. In this article we will setup a Bicep file for a Azure App Service with Application Insight.    Take the first steps:     Create a new file and for example call it main.bicep Add parameters to you file with the param    In a mather of a few seconds you will have something like this:    param appName string  resource app 'Microsoft.Web/sites@2018-11-01' = {     name: appName }   Going from Bicep to ARM is very easy by using the following command:    bicep build [reference to your bicep file]   The above bicep file will result in a template like below.    {   \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",   \"contentVersion\": \"1.0.0.0\",   \"parameters\": {     \"appName\": {       \"type\": \"string\"     }   },   \"functions\": [],   \"variables\": {},   \"resources\": [     {       \"type\": \"Microsoft.Web/sites\",       \"apiVersion\": \"2018-11-01\",       \"name\": \"[parameters('appName')]\"     }   ],   \"outputs\": {} }   Import to notice in Bicep is the following: resource [reference] [type][version]     Reference: You can use this value throughout the complete file. This way you can reference resource specific variables. Type: the type of the resource Version: the specific version you would like to use    Extending the sample    The beauty of the Bicep language is the easy references and the dependencies the languages creates. This means you do not longer have to specify the dependencies but the language will do that for you.     If we continue with the sample after a few moment the file will look like this.    param appName string param location string = resourceGroup().location  resource insights 'Microsoft.Insights/components@2015-05-01' = {     name: 'insights-${appName}'     location: location     properties: {         Application_Type: 'web'     } }  resource hosting 'Microsoft.Web/serverfarms@2019-08-01' = {     name: 'hosting-${appName}'     location: location     sku: {         name: 'S1'     } }  resource app 'Microsoft.Web/sites@2018-11-01' = {     name: appName     location: location     identity: {         type: 'SystemAssigned'     }     properties: {         name: appName         siteConfig: {             appSettings: [                 {                     name: 'APPINSIGHTS_INSTRUMENTATIONKEY'                     value: insights.properties.InstrumentationKey                 }             ]         }         serverFarmId: hosting.id     } }  output appId string = app.id   The above Bicep file is a reference for a Azure Web Application with Application Insights attached. As mentioned before the dependencies and references defined look very easy.    Even notice the output option at the end that looks a lot cleaner and simpler than what you would normally do in ARM. To make the sample complete the above Bicep will result in.    {   \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",   \"contentVersion\": \"1.0.0.0\",   \"parameters\": {     \"appName\": {       \"type\": \"string\"     },     \"location\": {       \"type\": \"string\",       \"defaultValue\": \"[resourceGroup().location]\"     }   },   \"functions\": [],   \"variables\": {},   \"resources\": [     {       \"type\": \"Microsoft.Insights/components\",       \"apiVersion\": \"2015-05-01\",       \"name\": \"[format('insights-{0}', parameters('appName'))]\",       \"location\": \"[parameters('location')]\",       \"properties\": {         \"Application_Type\": \"web\"       }     },     {       \"type\": \"Microsoft.Web/serverfarms\",       \"apiVersion\": \"2019-08-01\",       \"name\": \"[format('hosting-{0}', parameters('appName'))]\",       \"location\": \"[parameters('location')]\",       \"sku\": {         \"name\": \"S1\"       }     },     {       \"type\": \"Microsoft.Web/sites\",       \"apiVersion\": \"2018-11-01\",       \"name\": \"[parameters('appName')]\",       \"location\": \"[parameters('location')]\",       \"identity\": {         \"type\": \"SystemAssigned\"       },       \"properties\": {         \"name\": \"[parameters('appName')]\",         \"siteConfig\": {           \"appSettings\": [             {               \"name\": \"APPINSIGHTS_INSTRUMENTATIONKEY\",               \"value\": \"[reference(resourceId('Microsoft.Insights/components', format('insights-{0}', parameters('appName')))).InstrumentationKey]\"             }           ]         },         \"serverFarmId\": \"[resourceId('Microsoft.Web/serverfarms', format('hosting-{0}', parameters('appName')))]\"       },       \"dependsOn\": [         \"[resourceId('Microsoft.Web/serverfarms', format('hosting-{0}', parameters('appName')))]\",         \"[resourceId('Microsoft.Insights/components', format('insights-{0}', parameters('appName')))]\"       ]     }   ],   \"outputs\": {     \"appId\": {       \"type\": \"string\",       \"value\": \"[resourceId('Microsoft.Web/sites', parameters('appName'))]\"     }   } }   In the next article I will describe the usage of Bicep modules that can be used for reusing components meaning that you do not have to copy the code around in multiple templates files.    If you want to learn more about Azure Bicep make sure to look at the tutorial on the github page as there are many more great features of this language:    ","categories": ["Azure"],
        "tags": ["ARM","Automation","Azure"],
        "url": "/2020/10/getting-started-with-project-bicep-for-azure-arm/",
        "teaser": null
      },{
        "title": "Control over a Azure Cloud environment? Create a governance plan!",
        "excerpt":"Control over your cloud environment is great. In order to properly monitor and manage a cloud environment, it is important to draw up a cloud governance plan based on the company policy.     In this plan, you name all aspects that are essential and then implement the policy. But what steps do you need to take to draw up a good cloud governance plan?    For every organization it is important to keep control on the cloud environment. This is not only related to; who has rights where, it is also about making the costs transparent. If these costs are transparent, it is easy to make a so-called IT charge or showback of these costs.     IT chargeback and IT showback are two policies used by IT departments to inform and / or bill the costs for the use of each department of division. Read more about IT charge and showback here.     To get control over the cloud platform one of the first steps is write a cloud governance plan.    Cloud Governance    From the company policy the Cloud governance is formalized For this document, the following aspects are mainly considered:      Business risks: Identifying and understanding business risks.  Policy and compliance: Convert risk into statements that support any compliance requirements. Processes: Ensuring compliance with the established policy.       You then write up five disciplines based on this policy, which are:     Cost management: Evaluate and monitor costs, keep IT expenses under control and gain insight into costs.  Basic security: Ensure compliance with IT security requirements by applying a basic security line.  Resource consistency: Provide consistency in resource configuration.  Basic Identity Configuration: Ensure that the basis for identity and access is maintained by applying consistent role definitions and assignments. Deployment acceleration: Accelerate deployment through centralization, consistency, and standardization across resource templates.    Governance Implementation    After setting up the cloud governance, it is time for implementation. With the implementation you ensure that the established policy can be complied with and checked.    By applying cloud governance you ensure a cloud platform that is in the best possible status and that the platform is compliant. Within Azure there are various services that support you in setting up cloud governance. Check out this website for more information on Azure Governance.    Management groups    Management groups are virtual containers in which subscriptions are placed. By means of these containers the subscriptions can be placed in a hierarchy. This makes it possible to manage subscriptions uniformly and manage policies and access rights across multiple subscriptions.          Some important points of Management groups are:      A structure for Management groups is set up per Azure Active Directory;  By default, a Tenant Root Management group is present per tenant.     Limits of Management groups are:     One Azure Active Directory tenant can contain 10,000 management groups;  The structure can be made up to 6 levels deep (excluding the Tenant Root Management group.); Groups or subscriptions can be placed in one group.    Azure Policies    As described, policies have been written down for the cloud platform. These policies can be enforced or registered through Azure Policies.     Azure Policies specifies the rules in a JSON format, making the rules easy to check by the platform. The platform itself has a large number of rules build-in, some examples for this are:      Require tag and its value on resource groups; Allowed locations; A maximum of 3 owners should be designated for your subscription; MFA should be enabled on accounts with owner permissions on your subscription.    The above rules are a small selection of the policies that can be used by default. Next to that you can also specify customer policies to be compliant with your governance design.    Other Azure Services     In addition to Azure Policies and management groups, there are a large number of other Azure Services and platform functionalities that support cloud governance:     Azure Automation: Automating various cloud infrastructure tasks.  Azure Monitor: Monitoring resources and supporting alerts for the cloud team.  Azure Blueprints: A blueprint for new subscriptions related to role assignment, resources and policies. Role Based Access Control: Grant rights based on roles of employees and administrators. Privilege Identity Management: Being able to temporarily assign additional rights for management tasks.    Blog originally published in Dutch on the website of Microsoft Gold Partner&nbsp;3fifty. If your are based in the Netherlands take a look at the website.      ","categories": ["Azure"],
        "tags": ["Azure","Governance"],
        "url": "/2020/10/control-over-a-azure-cloud-environment-create-a-governance-plan/",
        "teaser": null
      },{
        "title": "Managing the Microsoft Azure cloud is not easy, would a cloud team be useful?",
        "excerpt":"Most organizations start with the Microsoft Azure cloud by experimenting and before they know it, an environment is created that is no longer as transparent as they hoped. From experience we can tell that customers with a large number of subscriptions, fragmented resource groups and no insight into the costs are not unusual. (You're really not alone!).    How    Many organizations are faced with the question: \"How can we tackle the proliferation of subscriptions and lack of overview?\". When you ask this question to 3fifty, we have the following answer for you: The first step is to draw up an Azure Governance plan with regard to your cloud platform. This plan gives insight into the possibilities of the cloud platform and describes how the platform should look like.    But let's be honest, making a governance plan will not solve your problems! Step two is to put the governance plan into practice. You do this first of all by cleaning up the current environment and setting it up in such a way that it complies with the specified plan.    Step three is to implement a cloud team. If the platform is properly set up, you should consider a cloud team or a “Cloud Center of Excellence”. Since a cloud platform is not a traditional platform, you cannot expect that managing a cloud platform is the same as managing a data center and your current infrastructure. Much more expertise is required when managing a cloud platform and different activities are also significant.    What does a cloud team do    A cloud team within an organization is responsible for the cloud platform. In most cases, which of course differs per organization, they perform the following activities:     Think of common architecture models (reference architectures); Draw up best practices; Sharing knowledge about the cloud platform; Supporting questions related to the cloud; Evangelizing the cloud.    What are the drivers for working with a cloud team?    The cloud team is the knowledge group related to the cloud within the organization. They have the following motives:     Learning: Coaching and adopting cloud in the organization; Lead: Leading initiatives within the cloud platform; Security: Securing the cloud platform and keeping it solid as described.    The responsibilities and motives are also reflected in the final activities of a cloud team. The activities of the cloud team can be divided into the following categories:     Adoption Governance Knowledge Operation Strategic    When are you successful as an cloud team?    I often work with cloud teams at the customers I work for. Thanks to this experience and expertise, I can say that a cloud team is successful if they:     are multidisciplinary; are authorized to take action; have a visionary on the team; work agile; are technically; are involved with the organization and the cloud platform; are cloud minded and not thinking traditionally; have hands-on experience; are not too big (maximum 8 people).    Part of the team    As you have read, a cloud team within an organization has a great responsibility. Their activities are very different from a traditional IT environment. For this reason, we often see customers making the choice to also have an IT partner part of the cloud team. They also prevent the team from wearing the proverbial “blinders”.    Blog originally published in Dutch on the website of Microsoft Gold Partner&nbsp;3fifty. If your are based in the Netherlands take a look at the website      ","categories": ["Azure"],
        "tags": ["Agile","Azure","Governance"],
        "url": "/2020/11/managing-the-microsoft-azure-cloud-is-not-easy-would-a-cloud-team-be-useful/",
        "teaser": null
      },{
        "title": "Markdown generation for ARM and PowerShell",
        "excerpt":"After my session at Azure LowLands with the title \"You build It, You run It on the Microsoft Platform\"  a lot of people were interested in a script that I showed during the session.    The session itself is about developing a software package / product the DevOps way that also needs to be maintained after moving it to production. I tell a lot about how it should be done using 5 principles and my experience I had with multiple clients. During the demonstration I show Cloud Native resources in Azure that can be used in any kind of product or service.    Improvement of Daily work    One of the principles discussed is \"Improvement of Daily work\". This principle really makes our teams think about how they can improve the things we are doing and how that should be done. One of the thing that we did not like to do is write the documentation for our PowerShell scripts and ARM templates. For this we (Leon Boers and I) wrote some automation scripts to take care of this.    PowerShell    The script to generate the PowerShell documentation uses the \"Help\" that is specified within the script file.     &lt;# .SYNOPSIS     Script for generating Markdown documentation based on information in PowerShell script files.  .DESCRIPTION     All PowerShell script files have synopsis attached on the document. With this script markdown files are generated and saved within the target folder.  .PARAMETER ScriptFolder     The folder that contains the scripts  .PARAMETER OutputFolder     The folder were to safe the markdown files  .PARAMETER ExcludeFolders     Exclude folder for generation. This is a comma seperated list  .PARAMETER KeepStructure     Specified to keep the structure of the subfolders  .PARAMETER IncludeWikiTOC Include the TOC from the Azure DevOps wiki to the markdown files  .NOTES     Version:        1.0.0;     Author:         3fifty | Maik van der Gaag | Leon Boers;     Creation Date:  20-04-2020;     Purpose/Change: Initial script development;  .EXAMPLE     .\\New-MDPowerShellScripts.ps1 -ScriptFolder \"./\" -OutputFolder \"docs/arm\"  -ExcludeFolder \".local,test-templates\" -KeepStructure $true -IncludeWikiTOC $false .EXAMPLE     .\\New-MDPowerShellScripts.ps1 -ScriptFolder \"./\" -OutputFolder \"docs/arm\" #>   The idea of using this we got from someone who was using the same principle on GitHub. We adopted the idea and created the below script.    [CmdletBinding()]  Param (     [Parameter(Mandatory = $true, Position = 0)][string]$ScriptFolder,     [Parameter(Mandatory = $true, Position = 1)][string]$OutputFolder,     [Parameter(Mandatory = $false, Position = 2)][string]$ExcludeFolders,     [Parameter(Mandatory = $false, Position = 3)][bool]$KeepStructure = $false,     [Parameter(Mandatory = $false, Position = 4)][bool]$IncludeWikiTOC = $false )  BEGIN {     Write-Output (\"ScriptFolder         : $($ScriptFolder)\")     Write-Output (\"OutputFolder         : $($OutputFolder)\")     Write-Output (\"ExcludeFolders       : $($ExcludeFolders)\")     Write-Output (\"KeepStructure        : $($KeepStructure)\")     Write-Output (\"IncludeWikiTOC       : $($IncludeWikiTOC)\")      $arrParameterProperties = @(\"DefaultValue\", \"ParameterValue\", \"PipelineInput\", \"Position\", \"Required\")     $scriptNameSuffix = \".md\"     $option = [System.StringSplitOptions]::RemoveEmptyEntries      $exclude = $ExcludeFolders.Split(',', $option)  } PROCESS {     try {         Write-Information (\"Starting documentation generation for folder $($ScriptFolder)\")          if (!(Test-Path $OutputFolder)) {             Write-Information (\"Output path does not exists creating the folder: $($OutputFolder)\")             New-Item -ItemType Directory -Force -Path $OutputFolder         }          # Get the scripts from the folder         $scripts = Get-Childitem $ScriptFolder -Filter \"*.ps1\" -Recurse          foreach ($script in $scripts) {             if (!$exclude.Contains($script.Directory.Name)) {                 Write-Information (\"Documenting file: $($script.FullName)\")                   if ($KeepStructure) {                     if ($script.DirectoryName -ne $ScriptFolder) {                         $newfolder = $OutputFolder + \"/\" + $script.Directory.Name                         if (!(Test-Path $newfolder)) {                             Write-Information (\"Output folder for item does not exists creating the folder: $($newfolder)\")                             New-Item -Path $OutputFolder -Name $script.Directory.Name -ItemType \"directory\"                         }                     }                 } else {                     $newfolder = $OutputFolder                 }                  $help = Get-Help $script.FullName -ErrorAction \"SilentlyContinue\" -Detailed                  if ($help) {                     $outputFile = (\"$($newfolder)/$($script.BaseName)$($scriptNameSuffix)\")                     Out-File -FilePath $outputFile                      if ($IncludeWikiTOC) {                         (\"[[_TOC_]]`n\") | Out-File -FilePath $outputFile                         \"`n\" | Out-File -FilePath $outputFile -Append                     }                      #synopsis                     if ($help.Synopsis) {                         (\"## Synopsis\") | Out-File -FilePath $outputFile -Append                         (\"$($help.Synopsis)\") | Out-File -FilePath $outputFile -Append                         \"`n\" | Out-File -FilePath $outputFile -Append                     } else {                         Write-Warning -Message (\"Synopsis not defined in file $($script.fullname)\")                     }                      #syntax                     if ($help.Syntax) {                         (\"``````PowerShell`n $($help.Syntax.syntaxItem.name)`n``````\") | Out-File -FilePath $outputFile -Append                         \"`n\" | Out-File -FilePath $outputFile -Append                     } else {                         Write-Warning -Message (\"Syntax not defined in file $($script.fullname)\")                     }                      #notes (seperated by (name): and (value);)                     if ($help.alertSet) {                         (\"## Information\") | Out-File -FilePath $outputFile -Append                         $text = $help.alertSet.alert.Text.Split(';', $option)                         foreach ($line in $text) {                             $items = $line.Trim().Split(':', $option)                             (\"**$($items[0]):** $($items[1])`n\") | Out-File -FilePath $outputFile -Append                         }                         \"`n\" | Out-File -FilePath $outputFile -Append                     } else {                         Write-Warning -Message (\"Notes not defined in file $($script.fullname)\")                     }                      #description                     if ($help.Description) {                         \"## Description\" | Out-File -FilePath $outputFile -Append                         $help.Description.Text | Out-File -FilePath $outputFile -Append                         \"`n\" | Out-File -FilePath $outputFile -Append                     } else {                         Write-Warning -Message \"Description not defined in file $($script.fullname)\"                     }                      #examples                     if ($help.Examples) {                         (\"## Examples\") | Out-File -FilePath $outputFile -Append                         \"`n\" | Out-File -FilePath $outputFile -Append                         forEach ($item in $help.Examples.Example) {                             $title = $item.title.Replace(\"--------------------------\", \"\").Replace(\"EXAMPLE\", \"Example\")                             (\"### $($title)\") | Out-File -FilePath $outputFile -Append                             if ($item.Code) {                                 (\"``````PowerShell`r`n $($item.Code) `r`n``````\") | Out-File -FilePath $outputFile -Append                             }                         }                     } else {                         Write-Warning -Message \"Examples not defined in file $($script.fullname)\"                     }                      if ($help.Parameters) {                         (\"## Parameters\") | Out-File -FilePath $outputFile -Append                         forEach ($item in $help.Parameters.Parameter) {                             (\"### $($item.name)\") | Out-File -FilePath $outputFile -Append                             $item.description[0].text | Out-File -FilePath $outputFile -Append                             (\"| | |\") | Out-File -FilePath $outputFile -Append                             (\"|-|-|\") | Out-File -FilePath $outputFile -Append                             (\"| Type: | $($item.Type.Name) |\") | Out-File -FilePath $outputFile -Append                             foreach ($arrParameterProperty in $arrParameterProperties) {                                 if ($item.$arrParameterProperty) {                                     (\"| $arrParameterProperty : | $($item.$arrParameterProperty)|\") | Out-File -FilePath $outputFile -Append                                 }                             }                             \"`n\" | Out-File -FilePath $outputFile -Append                         }                     } else {                         Write-Warning -Message \"Parameters not defined in file $($script.fullname)\"                     }                  } else {                     Write-Error -Message (\"Synopsis could not be found for script $($script.FullName)\")                 }             }         }     } catch {         Write-Error \"Something went wrong while generating the output documentation: $_\"     } } END {}   Within this script you can see that we have added a different implementation for the \"Notes\" section. In the notes section we wanted to place more information and also be able to format in a specific way. That is why we created this format:     [Name]: [Value];    This why we are able to add all kind of information and keep is nicely formatted. The generated documentation looks like:       ARM Templates    When we finished the documentation generation for PowerShell script files we thought that it is also quite easy to use the same principle for ARM templates.    The only thing we had to think of was the way we wanted to add additional information for the documentation. For this we added the \"Metadata\" property. This property does not violate the schema validation and additional properties could be added like description, version and author.    \"metadata\": {    \"Description\": \"This template deploys a standard storage account.\",    \"Author\": \"3fifty | Maik van der Gaag | Leon Boers\",    \"Version\": \"1.0.0\" }   The script looks quite the same as the PowerShell version but adopted the specific ideas for ARM.    [CmdletBinding()]  Param (     [Parameter(Mandatory = $true, Position = 0)][string]$TemplateFolder,     [Parameter(Mandatory = $true, Position = 1)][string]$OutputFolder,     [Parameter(Mandatory = $false, Position = 2)][string]$ExcludeFolders,     [Parameter(Mandatory = $false, Position = 3)][bool]$KeepStructure = $false,     [Parameter(Mandatory = $false, Position = 4)][bool]$IncludeWikiTOC = $false )   BEGIN {     Write-Output (\"TemplateFolder       : $($TemplateFolder)\")     Write-Output (\"OutputFolder         : $($OutputFolder)\")     Write-Output (\"ExcludeFolders       : $($ExcludeFolders)\")     Write-Output (\"KeepStructure        : $($KeepStructure)\")     Write-Output (\"IncludeWikiTOC       : $($IncludeWikiTOC)\")      $templateNameSuffix = \".md\"     $option = [System.StringSplitOptions]::RemoveEmptyEntries     $exclude = $ExcludeFolders.Split(',', $option) } PROCESS {     try {         Write-Information (\"Starting documentation generation for folder $($TemplateFolder)\")          if (!(Test-Path $OutputFolder)) {             Write-Information (\"Output path does not exists creating the folder: $($OutputFolder)\")             New-Item -ItemType Directory -Force -Path $OutputFolder         }          # Get the scripts from the folder         $templates = Get-Childitem $TemplateFolder -Filter \"*.json\" -Recurse -Exclude \"*parameters.json\",\"*descriptions.json\",\"*parameters.local.json\"          foreach ($template in $templates) {             if (!$exclude.Contains($template.Directory.Name)) {                 Write-Information (\"Documenting file: $($template.FullName)\")                  if ($KeepStructure) {                     if ($template.DirectoryName -ne $TemplateFolder) {                         $newfolder = $OutputFolder + \"/\" + $template.Directory.Name                         if (!(Test-Path $newfolder)) {                             Write-Information (\"Output folder for item does not exists creating the folder: $($newfolder)\")                             New-Item -Path $OutputFolder -Name $template.Directory.Name -ItemType \"directory\"                         }                     }                 } else {                     $newfolder = $OutputFolder                 }                  $templateContent = Get-Content $template.FullName -Raw -ErrorAction Stop                 $templateObject = ConvertFrom-Json $templateContent -ErrorAction Stop                  if (!$templateObject) {                     Write-Error -Message (\"ARM Template file is not a valid json, please review the template\")                 } else {                     $outputFile = (\"$($newfolder)/$($template.BaseName)$($templateNameSuffix)\")                     Out-File -FilePath $outputFile                     if ($IncludeWikiTOC) {                         (\"[[_TOC_]]`n\") | Out-File -FilePath $outputFile                         \"`n\" | Out-File -FilePath $outputFile -Append                     }                      if ((($templateObject | get-member).name) -match \"metadata\") {                         if ((($templateObject.metadata | get-member).name) -match \"Description\") {                             Write-Verbose (\"Description found. Adding to parent page and top of the arm-template specific page\")                             (\"## Description\") | Out-File -FilePath $outputFile -Append                             $templateObject.metadata.Description | Out-File -FilePath $outputFile -Append                         }                          (\"## Information\") | Out-File -FilePath $outputFile -Append                         $metadataProperties = $templateObject.metadata | get-member | where-object MemberType -eq NoteProperty                         foreach ($metadata in $metadataProperties.Name) {                             switch ($metadata) {                                 \"Description\" {                                     Write-Verbose (\"already processed the description. skipping\")                                 }                                 Default {                                     (\"`n\") | Out-File -FilePath $outputFile -Append                                     (\"**$($metadata):** $($templateObject.metadata.$metadata)\") | Out-File -FilePath $outputFile -Append                                 }                             }                         }                     }                      (\"## Parameters\") | Out-File -FilePath $outputFile -Append                     # Create a Parameter List Table                     $parameterHeader = \"| Parameter Name | Parameter Type |Parameter Description | Parameter DefaultValue | Parameter AllowedValues |\"                     $parameterHeaderDivider = \"| --- | --- | --- | --- | --- | \"                     $parameterRow = \" | {0}| {1} | {2} | {3} | {4} |\"                      $StringBuilderParameter = @()                     $StringBuilderParameter += $parameterHeader                     $StringBuilderParameter += $parameterHeaderDivider                      $StringBuilderParameter += $templateObject.parameters | get-member -MemberType NoteProperty | ForEach-Object { $parameterRow -f $_.Name , $templateObject.parameters.($_.Name).type , $templateObject.parameters.($_.Name).metadata.description, $templateObject.parameters.($_.Name).defaultValue , (($templateObject.parameters.($_.Name).allowedValues) -join ',' )}                     $StringBuilderParameter | Out-File -FilePath $outputFile -Append                      (\"## Resources\") | Out-File -FilePath $outputFile -Append                     # Create a Resource List Table                     $resourceHeader = \"| Resource Name | Resource Type | Resource Comment |\"                     $resourceHeaderDivider = \"| --- | --- | --- | \"                     $resourceRow = \" | {0}| {1} | {2} | \"                      $StringBuilderResource = @()                     $StringBuilderResource += $resourceHeader                     $StringBuilderResource += $resourceHeaderDivider                      $StringBuilderResource += $templateObject.resources | ForEach-Object { $resourceRow -f $_.Name, $_.Type, $_.Comments }                     $StringBuilderResource | Out-File -FilePath $outputFile -Append                       if ((($templateObject | get-member).name) -match \"outputs\") {                         write-verbose (\"Output objects found.\")                         if (Get-Member -InputObject $templateObject.outputs -MemberType 'NoteProperty') {                             (\"## Outputs\") | Out-File -FilePath $outputFile -Append                             # Create an Output List Table                             $outputHeader = \"| Output Name | Output Type | Output Value |\"                             $outputHeaderDivider = \"| --- | --- | --- |  \"                             $outputRow = \" | {0}| {1} | {2} | \"  ","categories": ["Azure","Azure DevOps"],
        "tags": ["DevOps","Powershell"],
        "url": "/2021/02/markdown-generation-for-arm-and-powershell/",
        "teaser": null
      },{
        "title": "The new and shiny - Azure Template Specs",
        "excerpt":"Azure Template specs is a new resource within Azure that is used for saving Azure Resource Manager Templates (ARM). With these template specifics you can later create resources based on the template within the specification.    Why use Azure Template Specs    Azure Template Specs are really useful for situations where you or the organization would like to create resources based on a specific template. This can for example be a VM resources in a specific VNET configuration or a reference architecture created fully in ARM templates.    The specs can be shared within the Azure platform as they are kept within a Resource Group. The templates can then be deployed to any subscription you have access to as everything is based on RBAC.    Setting up your first template spec    During this guide we will make use of the following ARM template, it contains a small architecture that is often used together: Azure App Service, Hosting Plan and Application Insights.    {   \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",   \"contentVersion\": \"1.0.0.0\",   \"parameters\": {     \"apiName\": {       \"type\": \"string\"     },     \"apiHostingPlan\": {       \"type\": \"string\"     },     \"apiSkuName\": {       \"type\": \"string\",       \"defaultValue\": \"S1\",       \"allowedValues\": [         \"F1\",         \"D1\",         \"B1\",         \"B2\",         \"B3\",         \"S1\",         \"S2\",         \"S3\",         \"P1\",         \"P2\",         \"P3\",         \"P4\"       ]     },     \"insightsname\": {       \"type\": \"string\"     },     \"location\": {       \"type\": \"string\",       \"defaultValue\": \"[resourcegroup().location]\"     }   },   \"variables\": {   },   \"resources\": [     {       \"type\": \"Microsoft.Web/sites\",       \"apiVersion\": \"2018-11-01\",       \"name\": \"[parameters('apiName')]\",       \"location\": \"[parameters('location')]\",       \"tags\": {},        \"dependsOn\": [         \"[resourceId('microsoft.insights/components/', parameters('insightsname'))]\",         \"[resourceId('Microsoft.Web/serverfarms/', parameters('apiHostingPlan'))]\"       ],       \"identity\": {         \"type\": \"SystemAssigned\"       },       \"kind\": \"web\",       \"resources\": [],       \"properties\": {         \"name\": \"[parameters('apiName')]\",         \"siteConfig\": {           \"appSettings\": [             {               \"name\": \"APPINSIGHTS_INSTRUMENTATIONKEY\",               \"value\": \"[reference(resourceId('microsoft.insights/components/', parameters('insightsname')), '2015-05-01').InstrumentationKey]\"             }           ]         },         \"serverFarmId\": \"[resourceId('Microsoft.Web/serverfarms', parameters('apiHostingPlan'))]\",         \"clientAffinityEnabled\": true       }     },     {       \"type\": \"Microsoft.Web/serverfarms\",       \"apiVersion\": \"2018-11-01\",       \"name\": \"[parameters('apiHostingPlan')]\",       \"location\": \"[parameters('location')]\",       \"tags\": {       },       \"sku\": {         \"name\": \"[parameters('apiSkuName')]\"       },       \"properties\": {         \"name\": \"[parameters('apiHostingPlan')]\"       }     },     {       \"type\": \"microsoft.insights/components\",       \"apiVersion\": \"2020-02-02-preview\",       \"name\": \"[parameters('insightsname')]\",       \"location\": \"[parameters('location')]\",       \"tags\": {},        \"properties\": {         \"ApplicationId\": \"[parameters('apiname')]\",         \"Request_Source\": \"IbizaWebAppExtensionCreate\"       }     }   ],   \"outputs\": {   } }   Getting started    To get started with Azure Template specs a resource group needs to be added to Azure for saving the specifications. From this resource group you will be able to share the Azure Template Spec by using the default Role Based Access Model of Azure.    Let’s create a new resource group by using the following command    New-AzResourceGroup -Name rg-templatespec -Location WestEurope   With the command “New-AzTemplateSpec” a template specification is added to Azure. Now lets use it with the template from the example.     As the template specs are still in public preview we need to make sure we have the latest version of the “Az.Resources” module. Use the following command to install the latest prerelease version.    Install-Module -Name Az.Resources -AllowPrerelease -Force -AllowClobber -SkipPublisherCheck   Note: At the time of writing this article Azure Templates Specs are in preview. To make use of PowerShell make sure to install version 5.0.0 or later. When using Azure CLI use version 2.14.2 or later.    With the correct version of the PowerShell module installed the function “New-AzTemplateSpec” can be used.    New-AzTemplateSpec -Name \"aztempwebsitespec\" -Version 1.0 -ResourceGroupName rg-templatespec -Location WestEurope -TemplateFile ./azuredeploy.json      The function includes a “Version” parameter. This parameter is a string value that specifies the version of the file. By running the command a second time you are able to deploy a new version (when you for example have done a bug fix). You off course then have to specify a different value for the version or you would override the existing version.    The Azure Portal    After you have run the command the Azure Template Spec is created and visible via PowerShell, CLI and off course also the Azure Portal.    Get-AzTemplateSpec      Via the Azure portal you are able to deploy the template to any subscription you have access to. By selecting the deploy button.       Updating the Template Spec    As mentioned in the previous paragraph the template spec can be updated by running the exact same command as before and changing the version parameter to a different version.       In the portal you are then also able to see the version and for example track the changes made to the template spec.       Deploying via PowerShell    As you may also guess the template spec can also be used in PowerShell to deploy the resources. Microsoft has enhanced the “New-AzResourceGroupDeployment” function with the ability to deploy from a template specification. For this you will need to have a reference to the Azure Template Spec id as shown in the below script.    The parameters are asked during execution but the can also be added by using a parameters file and add that to the function parameter “TemplateParameterFile”.    $id = (Get-AzTemplateSpec -Name aztempwebsitespec -Version 1.1 -ResourceGroupName rg-templatespec).Versions.Id  New-AzResourceGroupDeployment -TemplateSpecId $id -ResourceGroupName template-test   Conclusion    The Azure Template Spec offers you a great way of sharing default templates with you users. This can be in company but also for example service providers.    By using the default Role Based Access model you are also able to restrict access to different kind of templates. So I would suggest that you check this new and shiny resource in Azure.    ","categories": ["Azure"],
        "tags": ["ARM","Azure"],
        "url": "/2021/02/the-new-and-shiny-azure-template-specs/",
        "teaser": null
      },{
        "title": "Private Azure DevOps agent pool based the Microsoft Hosted Agents",
        "excerpt":"In Azure DevOps it is possible to create a agent pool that references a Azure virtual machine scale set. This scale set can be based on one of the build in images but can also be based on a custom image.    The images that Microsoft uses for there build agents are made opensource. To be precise the code to make the images is opensource. Making very easy for us to build private agents with the same technology.     GitHub Actions virtual environments    With the information in this repository we can build up our own build agents and use those as the default image for a Azure virtual machine scale set.    Creating the image    To create the image we need a couple of tools installed:     Packer: HashiCorp Packer automates the creation of any type of machine image. It embraces modern configuration management by encouraging you to use automated scripts to install and configure the software within your Packer-made images. Packer brings machine images into the modern age, unlocking untapped potential and opening new opportunities. https://packer.io Azure CLI: The Azure command line interface: https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-windows?view=azure-cli-latest&amp;tabs=azure-cli    To install packer I use a tool called Chocolately.    choco install packer   Chocolately also makes sure that the PATH variable to packer is configured.    When the prerequisites are installed the first step is to clone the repository. The recommanded thing to do is to do this directly on the C:\\ drive.    git clone https://github.com/actions/virtual-environments.git   Then import the helper module that is included with the virtual environment repository.    Set-Location C:\\virtual-environments Import-Module .\\helpers\\GenerateResourcesAndImage.ps1   Then the PowerShell function 'GenerateResourcesandImage' can be executed. For this the following arguments need to be supplied:     SubscriptionId: The Id of the subscription were you want to create the image. ResourceGroupName: The resource group were the image will be created . ImageGenerationRepositoryRoot: The root path of the image generation repository source. ImageType: The type of the image. In the repository the following types are supported:  Windows2016 Windows2019 Ubuntu1604 Ubuntu1804 Ubuntu2004   AzureLocation: The location for the Azure Resources GithubFeedToken: Token to retrieve sources from GitHub. You can generate this token at the following location: User --> Settings --> Developer Settings --> Personal Access Token    GenerateResourcesAndImage -SubscriptionId [Subscription ID] -ResourceGroupName \"[Resourcegroup name]\" -ImageGenerationRepositoryRoot \"$pwd\" -ImageType Windows2019 -AzureLocation \"WestEurope\" -GithubFeedToken \"[GitHub Token]\"   When running the script locally you are requested to sign-in to Azure. Make sure the account you are logging in with has the appropriate rights to create the resources.    Note: Executing this command will start up the generation of the image that can take about 6 till 7 hours, depending on the image you are building. Building the Windows 2019 image on my machine took 6 hours and 51 minutes. The function will also create a resource group for a temporary VM so make sure that you do not have active policies that could interfere this process.       In the Azure subscription you will see resources appearing:     Azure Storage Account: In the specified resource group a storage account will be created to save the VHD of the image. Resource Group: A resource group with a generated name will appear. In this resource group the VM will be created to generate the image.       After a long time (in my situation 6 hours and 51 minutes later) the VHD is created and saved within the storage account.       Creating the Virtual Machine Scale Set    With the VHD saved within the storage account. We can create a image based on that disk. To do this I used the cloud shell. Open the cloud shell and perform this command:    az image create -g rg-devops-image --os-type Windows --source [URI to VHD in storage] --name privatehostedimage    -g: The name of the resource group --os-type: The type of the operating system of the image --source: The URI to the VHD in the storage account --name: The name for the image    With the image in place we can create a Azure Virtual Machine Scale set that we can use within Azure DevOps. For this use the following command and run it within the Cloud Shell.    az vmss create \\ --name [name for the scale set] \\ --resource-group [resource group name] \\ --image [name of the image] \\ --vm-sku Standard_D2_v3 \\ --storage-sku StandardSSD_LRS \\ --authentication-type password \\ --instance-count 2 \\ --disable-overprovision \\ --upgrade-policy-mode manual \\ --single-placement-group false \\ --platform-fault-domain-count 1 \\ --load-balancer \"\"   Make sure that when u create the set the following things are configured:     Overprovisioning should be disabled. The upgrade policy should be set to manual.    Configuring the Agent Pool in Azure DevOps    The last step is to configure the agent pool in Azure DevOps. The Agent Pool can be configured on two levels: Project or Organization. In this example we will create it on the organization level.     Go to the settings page of the organization. Click on Agent Pools under the category Pipelines. Click on the top right button called \"Add pool\" Select: \"Azure Virtual machine scale set\" as the agent pool type Select the project that has the correct \"Service Connection\" to the subscription that contains the scale set or create a new service connection by clicking on the Authorize button. Select the correct scale set and configure the other options.    Note: At the time of writing only Service Connections with Client ID, Client Secret credentials are supported.       Now you can use agents that are privately hosted and make sure you have the capacity for additional demands.     ","categories": ["Azure DevOps"],
        "tags": ["Azure","Azure DevOps"],
        "url": "/2021/02/private-azure-devops-agent-pool-based-the-microsoft-hosted-agents/",
        "teaser": null
      },{
        "title": "Running bicep within Azure DevOps Pipelines",
        "excerpt":"During Ignite Microsoft released bicep version 0.3.1. With this version the bicep language is not experimental any more. Some pointer about this version are:     Parity with ARM Templates Integration with Azure CLI (v2.20.0+) and Azure PowerShell (v5.6.0+) De-compiler Supported by Microsoft    If you want to get started with bicep you can read my post about bicep I wrote a few months ago. When building ARM templates you may also want to start building the templates in the bicep language and execute the build within a automated pipelines to for example perform testing against the template with the ARM-TTK tooling.     Support    At the time of writing this post there is no default support for bicep in Azure DevOps. As mentioned above it is integrated within Azure CLI and Azure PowerShell but the CLI task and Azure PowerShell task in Azure DevOps do not already contain the latest version of the tool to support bicep.    Running bicep in a pipeline    So how do we run bicep then within a pipeline? To get started the tooling needs to be installed on the agent. As I normally use the Microsoft hosted agent the bicep tooling needs to be installed on each run as we do not have access to the agents.    This brings us to the first task for the pipeline that will perform the install. To install bicep the following bash code is used.    curl -Lo bicep https://github.com/Azure/bicep/releases/latest/download/bicep-linux-x64 chmod +x ./bicep sudo mv ./bicep /usr/local/bin/bicep bicep --help   The script it self  needs to be performed on a Linux machine to install the bicep tooling and configure bicep into the PATH variable.    Doing this in a Azure DevOps Pipeline is very easy by using the bash task:    - bash: |    curl -Lo bicep https://github.com/Azure/bicep/releases/latest/download/bicep-linux-x64    chmod +x ./bicep    sudo mv ./bicep /usr/local/bin/bicep    bicep --help       displayName: 'Install bicep'   With bicep installed the bicep commands can be executed. In my Pipelines a regularly do this in a separate task but the bash code could be added to the previous task as well.    - bash: |    bicep build $(System.DefaultWorkingDirectory)/storage.bicep       displayName: 'Build bicep file'   In the above task the bicep file in my repository is build and saved in the same location.    Completing the pipeline    With the possibility to build the bicep files in the pipeline the pipeline can be completed to copy the generated ARM templates to for example the pipeline artifacts location. These last steps are included in the sample pipeline shown below.       trigger: - main  pool:   vmImage: ubuntu-latest  steps: - bash: |    curl -Lo bicep https://github.com/Azure/bicep/releases/latest/download/bicep-linux-x64    chmod +x ./bicep    sudo mv ./bicep /usr/local/bin/bicep    bicep --help       displayName: 'Install bicep'  - bash: |    bicep build $(System.DefaultWorkingDirectory)/storage.bicep       displayName: 'Build bicep file'  - task: CopyFiles@2   displayName: 'Copy Files to: $(build.artifactstagingdirectory)\\arm'   inputs:     SourceFolder: '$(System.DefaultWorkingDirectory)'     Contents: '*.json'     TargetFolder: '$(build.artifactstagingdirectory)/arm'  - task: PublishPipelineArtifact@1   inputs: ","categories": ["Azure","Azure DevOps"],
        "tags": ["ARM","Azure","Azure DevOps","bicep","IaC"],
        "url": "/2021/03/running-bicep-within-azure-devops-pipelines/",
        "teaser": null
      },{
        "title": "Running bicep in GitHub Actions",
        "excerpt":"Bicep is a language that is used to simplify the writing of ARM templates. Bicep is a so called DSL (Domain Specific Language) meaning that it is a specific language for a specific domains in this case ARM. The last couple of months I have written two more posts about bicep:     Getting started with Project Bicep for Azure ARM Running bicep within Azure DevOps Pipelines    As mention in the second post Microsoft released bicep version 0.3.1. With this version the bicep language is not experimental any more. Some pointer about this version are:     Parity with ARM Templates Integration with Azure CLI (v2.20.0+) and Azure PowerShell (v5.6.0+) De-compiler Supported by Microsoft    The second post covers how bicep can be used within Azure DevOps Pipelines. From this post I also got some question to write down the same for GitHub Actions.    Support    At the time of writing this post there GitHub Actions also supports the Azure CLI option of running bicep. But for having a faster process the outline of this post will download and install the latest version of bicep.     Running bicep in GitHub Actions    So how do we run bicep then within GitHub Actions ? To get started the tooling needs to be installed on the agent. This brings us to the first task for the pipeline that will perform the install. To install bicep the following bash code is used.    curl -Lo bicep https://github.com/Azure/bicep/releases/latest/download/bicep-linux-x64 chmod +x ./bicep sudo mv ./bicep /usr/local/bin/bicep bicep --help   The script it self  needs to be performed on a Linux machine to install the bicep tooling and configure bicep into the PATH variable. Doing this in GitHub Actions is very easy by using the run action:     - run: |       curl -Lo bicepinstall https://github.com/Azure/bicep/releases/latest/download/bicep-linux-x64      chmod +x ./bicepinstall      sudo mv ./bicepinstall /usr/local/bin/bicep      bicep --help   With bicep installed the bicep commands can be executed this is validated by running the \"bicep --help\" at the end of the installation run. To run bicep against one of the bicep files we add a separate action.     - run: bicep build ./bicep/storage.bicep   In the above action the bicep file in my repository is build and saved in the same location.    Completing the GitHub workflow    With the possibility to build the bicep files in the pipeline the GitHub workflow can be completed to copy the generated ARM templates to the workflow artifacts location. These last steps are included in the sample workflow file  below.     name: CI  # Controls when the action will run.  on:   # Triggers the workflow on push or pull request events but only for the main branch   push:     branches: [ main ]   pull_request:     branches: [ main ]    # Allows you to run this workflow manually from the Actions tab   workflow_dispatch:  # A workflow run is made up of one or more jobs that can run sequentially or in parallel jobs:   # This workflow contains a single job called \"build\"   build:     # The type of runner that the job will run on     runs-on: ubuntu-latest      # Steps represent a sequence of tasks that will be executed as part of the job     steps:       # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it       - uses: actions/checkout@v2        - run: |            curl -Lo bicepinstall https://github.com/Azure/bicep/releases/latest/download/bicep-linux-x64           chmod +x ./bicepinstall           sudo mv ./bicepinstall /usr/local/bin/bicep           bicep --help              - run:    bicep build ./bicep/storage.bicep              - name: Archive production artifacts         uses: actions/upload-artifact@v2         with:           name: dist-without-markdown           path: ./**/*.json   The complete example can be found on GitHub:    ","categories": ["GitHub Actions"],
        "tags": ["bicep","GitHub Actions"],
        "url": "/2021/03/running-bicep-in-github-actions/",
        "teaser": null
      },{
        "title": "Azure Template Specs managed via Azure DevOps with bicep",
        "excerpt":"Azure template spec is a resource type within Azure used for storing an Azure Resource Manager (ARM) template for later use. By using template specs you have the ability to share ARM templates with other users within the organization as they also make use of the role-based access control within the Azure platform.    Note: At the time of writing Azure Resource Manager template specs are in public preview.     Why start using template specs    Today there are not a lot of good ways of sharing ARM templates within a organization. Template specs are really useful in situations where you want to share the templates or if you would like to create resources based on a specific template. This can for example be a VM in a specific VNET configuration or a reference architecture created fully in ARM templates.    The specs can be shared within the Azure platform as they are kept within a Resource Group. The templates can then be deployed to any subscription you have access to as everything is based on RBAC.    In one of my previous posts I already mentioned some information on how to work with template specs and how to create them.    Template specs highlights    As mentioned before template specs are preview at the time I was writing this article so the highlights mentioned below can change over time:     Template specs can be managed via Azure PowerShell or Azure CLI.    Note: To use it with Azure PowerShell, you must install&nbsp;version 5.0.0 or later. To use it with Azure CLI, use&nbsp;version 2.14.2 or later.      Template specs are Azure resources that contains a ARM template. This is not a specific ARM template but can be a template you created before. Template specs can be created by using the command \"New-AzTemplateSpec\" for PowerShell or \"az ts create\" for CLI Template specs can be retrieved by using the command \"Get-AzTemplateSpec\" for PowerShell or \"az ts list\" for CLI    Lifecycle Management    As template specs will really benefit your organizations therefor the lifecycle of these resources need to be managed in correct way. This is were Azure DevOps comes in by helping deploying and versioning the template specs. During the rest of this article we will go over the process of building an bicep file into a ARM template that will be used within a template spec.    Bicep    As you may know bicep is the new way of creating IaC against the Azure Platform as bicep is a DSL (Domain Specific Language) for ARM.    This is why we build up our template spec from a bicep file. More information on how to get started or how to integrate bicep within Azure DevOps can you find in other articles of this blog.     Getting started with Project Bicep for Azure ARM Running bicep within Azure DevOps Pipelines Running bicep in GitHub Actions    During this guide we use a small sample that deploys an Azure App Service, Application Insights, Log analytics and Hosting plan that are all connected. The below snippet shows the bicep file that will be used.    param skuName string = 'S1' param skuCapacity int = 1 param location string = resourceGroup().location param appServicePlanName string param websiteName string param appInsightsName string param logAnalyticsName string  resource appServicePlan 'Microsoft.Web/serverfarms@2020-06-01' = {   name: appServicePlanName   location: location   sku: {     name: skuName     capacity: skuCapacity   } }  resource appService 'Microsoft.Web/sites@2020-06-01' = {   name: websiteName   location: location   identity: {     type: 'SystemAssigned'   }   properties: {     serverFarmId: appServicePlan.id     httpsOnly: true     siteConfig: {       minTlsVersion: '1.2'     }   } } resource appServiceLogging 'Microsoft.Web/sites/config@2020-06-01' = {   name: '${appService.name}/logs'   properties: {     applicationLogs: {       fileSystem: {         level: 'Warning'       }     }     httpLogs: {       fileSystem: {         retentionInMb: 40         enabled: true       }     }     failedRequestsTracing: {       enabled: true     }     detailedErrorMessages: {       enabled: true     }   } }  resource appServiceAppSettings 'Microsoft.Web/sites/config@2020-06-01' = {   name: '${appService.name}/appsettings'   properties: {     APPINSIGHTS_INSTRUMENTATIONKEY: appInsights.properties.InstrumentationKey   }   dependsOn: [     appInsights     appServiceSiteExtension   ] } resource appServiceSiteExtension 'Microsoft.Web/sites/siteextensions@2020-06-01' = {   name: '${appService.name}/Microsoft.ApplicationInsights.AzureWebsites'   dependsOn: [     appInsights   ] } resource appInsights 'microsoft.insights/components@2020-02-02-preview' = {   name: appInsightsName   location: location   kind: 'string'   properties: {     Application_Type: 'web'     WorkspaceResourceId: logAnalyticsWorkspace.id   } }  resource logAnalyticsWorkspace 'Microsoft.OperationalInsights/workspaces@2020-03-01-preview' = {   name: logAnalyticsName   location: location   properties: {     sku: {       name: 'PerGB2018'     }     retentionInDays: 120   } }   The bicep file is used to construct the ARM template in Azure DevOps in order to use the ARM file as a definition for the template spec.    Setting up the pipeline    To manage the lifecycle a Azure DevOps pipeline will be used that will perform severals tasks:     Build the ARM template from the bicep file. Check if the resource groups for the template specs exists or other wise create it.  Deploy or update the template spec Update the semantic version for the template spec by using the extension: Version number counter    Build the ARM template from the bicep file.    To build the ARM template from the bicep file the same principle is used that is available in the article \"Running bicep within Azure DevOps Pipelines\" with some small exceptions because the pipeline we will be using now will run on a windows agent.    steps: - task: PowerShell@2   displayName: \"Install bicep\"   inputs:     targetType: 'inline'     script: |       $installDir = New-Item -ItemType Directory -Path $(installPath) -Force       $installDir.Attributes += 'Hidden'       (New-Object Net.WebClient).DownloadFile(\"https://github.com/Azure/bicep/releases/latest/download/bicep-win-x64.exe\", \"$(installPath)\\bicep.exe\")     pwsh: true - task: PowerShell@2   displayName: \"Build bicep file\"   inputs:     targetType: 'inline'     script: |       $(bicep) build $(System.DefaultWorkingDirectory)/$(filepath)     pwsh: true   The above snippets shows the process of installing bicep on the agent and running the build process for a specified ARM template.  As you may notice the other settings of the pipeline are omitted for clarity. The result of the second task will be the arm template that can be used for the template spec.    Resource group    As template specs are regular Azure resources they are also created within a resource group. For this reason we check during the execution of the pipeline if the resource group we want to use is available or needs to be created. This action will be done by using the Azure PowerShell task and running that runs a small PowerShell script.    - task: AzurePowerShell@5   displayName: \"Check resource group\"   inputs:     azureSubscription: 'Subscription MPN Sandbox'     ScriptType: 'InlineScript'     Inline: |       if (!(Get-AzResourceGroup \"$(resourcegroupname)\" -ErrorAction SilentlyContinue)){            New-AzResourceGroup -Name \"$(resourcegroupname)\" -Location \"$(location)\"       }else{           Write-Output \"The resource group already exists\"       }     azurePowerShellVersion: 'LatestVersion'     pwsh: true   This inline script will retrieve the resource group. If it is not present it will create it otherwise it prints out a message that the resource group already exists.    Deploying and updating the template spec    Now that the prerequisites for template spec is ready the deployment of it can be added to the pipeline. You may have read in the article \"The new and shiny – Azure Template Specs\" that this is done via the command \"New-AzTemplateSpec\". This command needs the following arguments:     Name: The name of the template spec. This will also be the name of the resource in azure. Version: The version of the template spec. ResourceGroupName: The name of the resource group to deploy the template spec to. Location: The location for the template spec resource. TemplateFile: The ARM template for the template spec.    Filling this in results in an additional PowerShell task for the pipeline that creates the template spec.    - task: AzurePowerShell@5   displayName: \"Deploy template spec\"   inputs:     azureSubscription: 'Subscription MPN Sandbox(b27ac091-3de5-46d1-8f61-add3171f6e52)'     ScriptType: 'InlineScript'     Inline: |       New-AzTemplateSpec -Name \"$(templateSpecName)\" -Version 1.0 -ResourceGroupName \"$(resourcegroupname)\" -Location \"$(location)\" -TemplateFile \"$(System.DefaultWorkingDirectory)/$(filepathArm)\"     azurePowerShellVersion: 'LatestVersion'     pwsh: true   Executing this task will result into a template spec resource in Azure within the specified resource group.       Updating the template spec can be done by just rerunning the pipeline. This will override the current version with the new ARM template.    Template versioning    This is were the task \"Version number counter\" comes in. This task can be used to automatically update the semantic version number on each run based on the configuration. For this it needs a specified variable with the existing version number and a Personal Access token to update the version number. More information about this task can be found here:     Version number counter    - task: versioncounter@1   displayName: \"Update version number\"   inputs:     VersionVariable: 'versionnumber'     UpdateMinorVersion: true     MaxValuePatchVersion: '9'     DevOpsPat: '$(pipeline-pat)'   With this configuration the version number is saved in a variable called \"versionnumber\". This means that the task that deploys the template spec needs to be updated to also leverage the version number. Running the pipeline multiple time will make sure that the specified versions are saved within the template spec.       Conclusion    Combining tooling and services like bicep, ARM, template specs and Azure DevOps is very useful for maintaining your resources and keeping them up-to-date.    The complete files used to write this article can be found on my personal GitHub:     maikvandergaag/bicep: Project for bicep files (github.com)  ","categories": ["Azure","Azure DevOps"],
        "tags": ["ARM","Azure","Azure DevOps","Azure Spring Clean","Azure Template Specs","bicep","biceplang","IaC"],
        "url": "/2021/03/azure-template-specs-managed-via-azure-devops/",
        "teaser": null
      },{
        "title": "Using multiple repositories within Azure DevOps pipelines",
        "excerpt":"In numerous situations I have seen seen people sharing code from a centralized Azure Repos sending the scripts via email. On every new version they would send out a new version. Working with Azure DevOps this isn't a preferred solution, as you do not know how the script is being used and when there is a bug in the script there is no real fast option to update all distributed scripts.    This is where the option comes in to checkout multiple repositories within you Azure DevOps Pipeline. The rest of the article guides you trough the process of setting this up.    Different Azure DevOps Organization    When a repository from a different organization is one of the requirements a Service Connection is a prerequisite. This service connection will be used to setup the connection between the organizations by using a Personal Access Token. Depending on the use case make sure you generate a PAT with the correct permissions. Usually I generate a PAT with read permissions.       Within the project were the pipeline will be created create a new service connection of the type \"Azure Repos / Team Foundation Server\". In the creation dialog fill in the PAT en the organization and click verify to check the connection.       Checking out multiple Azure Repos    Loading repositories is completely handled from within the pipeline and currently the below repository types are supported:     Azure Repos Git Github Github Enterprise Bitbucket cloud    For this article we use multiple Azure DevOps repositories. Within the pipeline the other repositories are specified by using the \"resources\" option were in a \"repository\" is specified.    Check out code from different organization     As mentioned above to check out code from a different organization the service connection needs to be created and specified within the repository reference.    resources:   repositories:   - repository: [Name for reference in your Pipeline]      endpoint: [The created Service Connection]     type: git     name: [Project]/[Github Repository]     ref: [Branch]   The above code snippet shows which information needs to be supplied for making the connection.    Check out code from the same organization    Checking out code from the same organization requires not that much steps to get up and running and can be done directly from the \"checkout\" step from within the pipeline.    - checkout: git://[Project name]/[Repository]@[Branch]   I personally like to specify the repository completely as I'm then also able to supply the name that I would like to use within the check out step.    resources:   repositories:     - repository: [Name for reference in your Pipeline]        name: [Github Repository]       type: git       ref: [Branch]   Putting all Azure Repos together     When this is all in place you can checkout the different repositories within the pipeline within the stages were you need the sources. The current repository of the pipeline is referenced by the keyword \"self\" and the others are referenced by there name specified within the resources.    resources:   repositories:   - repository: DifferentOrganization      endpoint: Repos - MSFTPlayground     type: git     name: msft-ccoe/msft-ccoe     ref: master   - repository: SameOrganization      name: Azure DevOps Discovery/Governance     type: git     ref: master  trigger: - none  pool:   vmImage: 'windows-latest'  steps: - checkout: self - checkout: DifferentOrganization - checkout: SameOrganization - script: dir $(Build.SourcesDirectory)   Where are the files checked out to?    Normally when using just the current repo the files are saved directly within the \"$(Build.SourcesDirectory)\". With multiple repositories configured for each reference a folder is created based on the repository name. This is shown by the output of the above reference yml file.       Conclusion    Sharing code in some situations can be handy, but when you want to keep control over the scripts and how they can be executed you really need to check this option. You can take this far as setting up generic pipeline that is used within other projects.    For more information about checking out repositories:     Check out multiple repositories in your pipeline - Azure Pipelines | Microsoft Docs ","categories": ["Azure DevOps"],
        "tags": ["Azure DevOps","Pipelines","Repositories"],
        "url": "/2021/05/using-multiple-repositories-within-azure-devops-pipelines/",
        "teaser": null
      },{
        "title": "Run cloud native application anywhere with Azure Arc enabled Kubernetes",
        "excerpt":"At the beginning of this year Microsoft released Azure Arc enabled Kubernetes, this enabled you to attach conformant&nbsp;Kubernetes&nbsp;clusters to Azure for management.&nbsp;With a new preview release Microsoft introduced the capabilities of running Azure application services on Kubernetes anywhere you want. This could be on a Kubernetes cluster on the google cloud, AWS, on-premises or even on Azure it self.     The services that you are able to deploy in this preview are:      Azure App Services Azure Logic Apps Azure Functions Azure Event Grid Azure API Management    This functions opens up a lot of new possibilities to developers and also opens up option for using other cloud platforms next to Azure.    How to get started    To get started you are required to have a running Kubernetes cluster, this can be on any cloud platform, on-premises or even on your laptop. For this example I have used a local Kubernetes cluster running on my laptop with the use of Docker for Windows and a Kubernetes cluster within one of my Azure subscriptions.    To be able to deploy app services within Kubernetes we have to perform two main tasks: Connect the Kubernetes cluster to Azure Arc and create a custom location within the Azure platform.    To perform these steps we need the following prerequisites:     az cli version 2.16.0 or later (you can upgrade the cli via the below command)    az upgrade    Latest version of Helm    choco install kubernetes-helm       Kubernetes context connected to the Kubernetes cluster you want to connect to az cli extensions: connectedk8s, k8s-extension, customlocation    az extension add --name connectedk8s az extension add --name k8s-extension az extension add --name customlocation   Custom locations for Azure services are only supported in the below regions at the time of writing this article: - East US - West Europe     Connect Kubernetes cluster to Azure Arc    To connect the Kubernetes cluster to Azure Arc specific resource providers need to be registered on the subscriptions were you want to deploy Azure Arc to. The required resource providers are: Microsoft.Kubernetes, Microsoft.KubernetesConfiguration, Microsoft.ExtendedLocation.    These registration can be done via the Azure Portal but in this article we will use the Azure CLI to perform all the steps.    az provider register --namespace [provider]   Perform the command for all three providers and wait for the registration to finish. In total this could take up to 30 minutes to complete. The status can be monitored by using this command.    az provider show -n [provider] -o table      Azure Arc it self is a Azure resource that needs to be deployed to an Azure Resource group. That is why we will also create a resource group.    az group create -n [group name]-l [location] -o table   Next step is to connect the Kubernetes cluster to Azure Arc. In this step all required resources will be deployed to Kubernetes and Azure were the Azure Arc resource will be created.    az connectedk8s connect -n [Azure Arc Instance name] ` -g [Resource group name]      This command will initiate a new deployment to the Kubernetes cluster via Helm and as you see in the screenshot the deployment done there was to my AKS cluster. Were it is specified that you only have to connect to Azure Arc for enabled services because Azure Monitor and Defender are natively support within AKS.    When this deployment is succeeded the cluster is connected to Azure Arc. In the below screenshots you can see the differences in deployments on the cluster before connecting to Azure Arc and after connecting it to Azure Arc.    Before       After       Adding custom location to Azure    With the Kubernetes cluster connected to Azure Arc a custom location can be added. Adding this location is done in two easy steps by using the extensions on the Azure CLI.    Enable custom locations    To use custom locations the functionality has to be activated on the cluster it self. So make sure u are connected to Azure and still have the active context to the Kubernetes cluster that you would like to connect.    az connectedk8s enable-features -n [cluster name] ` -g [Resource group name] ` --features cluster-connect custom-locations   After enabling the feature the custom location can be activated for three different service instances:     Azure Arc enabled Data Services Azure App Service on Azure Arc Event Grid on Kubernetes    Each of these have there own specific command for the activation:    Azure Arc enabled Data Services    az k8s-extension create --name [name for the extension] ` --extension-type microsoft.arcdataservices ` --cluster-type connectedClusters ` -c [name of the Kubernetes cluster] ` -g [resource group name ARC] ` -scope cluster --release-namespace arc ` --config Microsoft.CustomLocation.ServiceAccount=sa-bootstrapper   Azure App Service on Azure Arc    az k8s-extension create --name [name for the extension] ` --extension-type microsoft.arcdataservices ` --cluster-type connectedClusters ` -c [name of the Kubernetes cluster] ` -g [Resource group name ARC] ` --scope cluster ` --release-namespace arc ` --config Microsoft.CustomLocation.ServiceAccount=sa-bootstrapper   Event Grid on Kubernetes    az k8s-extension create --name [name for the extension] ` --extension-type Microsoft.EventGrid ` --cluster-type connectedClusters ` -c [name of the Kubernetes cluster] ` -g [Resource group name ARC] ` --scope cluster ` --release-namespace eventgrid-ext ` --configuration-protected-settings-file protected-settings-extension.json ` --configuration-settings-file settings-extension.json      The scripts shown above are all standard scripts to get started with the extension. If you for example want to deploy Azure App Services additional configuration is required. To start using App Service try the following command to configure the extension.    az k8s-extension create --resource-group [resource group name] ` --name [name for the extension] ` --cluster-type connectedClusters ` --cluster-name [cluster name] ` --extension-type 'Microsoft.Web.Appservice' ` --release-train stable ` --auto-upgrade-minor-version true ` --scope cluster ` --release-namespace [namespace] ` --configuration-settings \"Microsoft.CustomLocation.ServiceAccount=default\" ` --configuration-settings \"appsNamespace=[namespace]\" ` --configuration-settings \"clusterName=[cluster name]\" ` --configuration-settings \"loadBalancerIp=[static ip of cluster]\" ` --configuration-settings \"keda.enabled=true\" ` --configuration-settings \"buildService.storageClassName=default\" ` --configuration-settings \"buildService.storageAccessMode=ReadWriteOnce\" ` --configuration-settings \"customConfigMap=[namespace]/kube-environment-config\" ` --configuration-settings \"envoy.annotations.service.beta.kubernetes.io/azure-load-balancer-resource-group=[kubernetes infra resource group name]\"   With the extension created some information needs to be retrieved to be able to do the final step. For the final step the resourceId of the Azure Arc enabled Kubernetes cluster is required and the same for the extension that has been activated on the cluster    Resource Id connected cluster    az connectedk8s show -n [Azure Arc enabled cluster] ` -g [Resource group name]  ` --query id -o tsv   Resource Id extension for connected cluster    az k8s-extension show --name [name of the extension] ` --cluster-type connectedClusters ` -c [Azure Arc enabled cluster] ` -g [Resource group name]  ` --query id -o tsv   With this information retrieved the cluster can be defined as a custom location:    az customlocation create -n [location name] ` -g [Resource group name] ` --namespace [namespace in Kubernetes] ` --host-resource-id [cluster id] ` --cluster-extension-ids [extension id]   Things to be aware of when you process the above command:     Per connection a different namespace needs to be defined.    Next Step    After following the above steps your cluster is connected to Azure Arc and should be available as a custom location. The existing custom location can be checked with the below command.    az customlocation list -o table      These location can also be found by opening the Azure Arc blade within the Azure portal and clicking on custom locations.       As shown in the screenshots four different locations are added to Azure. Two to a local cluster installed on my own laptop and two for the Kubernetes cluster hosted in Azure.    Enable Azure App Services in Kubernetes    Creating services in the specified location requires you to have Azure App Services Kubernetes environment. By creating this environment the cluster is enabled to receive new request for app services.    az appservice kube create --resource-group [resource group] ` --name [name for the environment] ` --custom-location [Id of the custom location] ` --static-ip [static ip used in the extension installation]   The custom location id used in the above command can be retrieved with the following script.    $customLocationId=$(az customlocation show --resource-group [resource group name] ` --name [location name] ` --query id ` --output tsv)   Note: If the \"az appservice kube\" command does not work make sure you install a defined version with the following script.    az extension remove --name appservice-kube az extension add --yes ` --source \"https://aka.ms/appsvc/appservice_kube-latest-py2.py3-none-any.whl\"   When done all kind of new services will have appeared in the resource group used during the connection.       Creating resources on the Kubernetes environment    The Kubernetes environment is now prepared for new Azure App Services. These services can be created through the portal or via the Azure CLI.     Azure CLI    When using the Azure CLI the custom location id needs to be supplied when creating the hosting plan and the web application for example.    az appservice plan create -g [resource group name] ` -n [name hosting plan] ` --custom-location [custom location id] ` --per-site-scaling ` --is-linux ` --sku K1  az webapp create --plan [name hosting plan] ` --resource-group [resource group name] ` --name [web app name] ` --custom-location [custom location id] ` --runtime [runtime]   Azure Portal    On every location were you are able to use custom location the location will appear on the locations drop down.       More to learn    On docs.microsoft.com a lot more articles and information can be found about this subject. The below list points you to some interesting articles:     Azure Arc enabled Kubernetes Create and manage custom locations on Azure Arc enabled Kubernetes Create a web app on Azure Arc - Azure App Service App Service on Azure Arc - Azure App Service     ","categories": ["Azure"],
        "tags": ["Arc","Azure","Azure App Services","Kubernetes","mvpbuzz"],
        "url": "/2021/06/run-cloud-native-application-anywhere-with-azure-arc-enabled-kubernetes/",
        "teaser": null
      },{
        "title": "Execute scripts against Azure from GitHub Actions",
        "excerpt":"Within Azure DevOps service connections are used for the authentication against the Azure platform but in GitHub this works a little bit different. Let me explain how to setup a connection to your Azure Subscription to execute scripts.    GitHub Secret    The information for the authentication is saved within so called secrets that are encrypted within GitHub that are saved on the organization, repository or repository environment level. The credential information for the authentication against Azure is saved in a json object.    {     \"clientId\": \"[clientId]\",     \"clientSecret\": \"[clientSecret]\",     \"subscriptionId\": \"[subscription id]\",     \"tenantId\": \"[Azure Active Directory Tenant Id]\" }   As you can see in the above snippet a Service Principal is used to authenticate against Azure. So to get started make sure you have a service principal or that you create one. Documentation about that can be found on docs.microsoft.com:     Create an Azure AD app &amp; service principal in the portal    Follow the below steps to create a repository secret:     Within the GitHub repository go to settings and then secrets.        Click on \"new repository secret\" Fill in a name for the secret and use the json object for the value of the secret.       Use the Azure steps within GitHub Actions    With the secret in the repository the actions to communicate with Azure can be added to the workflow. First up is adding the Azure Login action.       In this action the secret needs to be referenced by \"secrets.[Secret Name]\". The action will make sure that you are loggedin.    - name: Azure Login\t   uses: Azure/login@v1   with: \tcreds: $    Note: If you want to make use of Azure PowerShell make sure to add the following property: \"enable-AzPSSession: true\"     Now that you are authenticated the script can be executed against Azure using the context that was initiated via the Azure Login action. There is no need for additional configuration of the context.    The following actions creates a Azure Template Spec for example.    - name: Azure CLI Action\t \tuses: Azure/cli@1.0.4 \twith: \tinlineScript: az ts create --name az-tempspec-bicepmodulestorage-github --version \"1.0\"      --resource-group sponsor-rg-templatespecs --location \"westeurope\" --template-file      \"./04-bicepmoduletemplatespec/04-bicepmoduletemplatespec.json\"     ","categories": ["Azure"],
        "tags": ["Azure","DevOps","GitHub Actions"],
        "url": "/2021/06/execute-scripts-against-azure-from-github-actions/",
        "teaser": null
      },{
        "title": "Using private repositories for Bicep modules",
        "excerpt":"As of version 0.4.1008 bicep now supports repositories for saving bicep modules. Repositories that are supported at the time of writing this article are:     Azure Container Registry Azure Template Specs    Why should you use repositories    You should use repositories for reusability and modularization. This comes in handy when working with large and complex environment and maybe as well within enterprises with a lot of DevOps team were everyone can use the same modules. By using the repositories, you can easily reuse modules for different deployments and it supports role based access control so you are in control of who has access to the modules.    Personally I'm a big fan of Azure Template Specs and have been using them since the start so I immediately jumped in and tested it out. Modules specified in your Bicep file get downloaded on the bicep build command and included in your final ARM template.    Before you begin make sure you have the correct version of bicep installed on your local machine!    Lets use a Template Spec     Make sure you have a template spec available within Azure. If you are new to Template Specs check a previous blog post of mine:    The new and shiny - Azure Template Specs - Microsoft Playground (msftplayground.com)    With a template spec available we start by creating a bicep configuration file at the root path to make it easy to leverage our repositories. In the configuration file when can specify short aliases to point to the specific repository.     Start by creating a file called 'bicepconfig.json'  and use the intellisense of Visual Studio Code to create a 'moduleAliases' section.    {   \"moduleAliases\": {     \"ts\": {},     \"br\": {}   } }   The 'ts' abbreviation stands for template spec and 'br' for bicep registry. Add a new alias for 'ts' by creating a sub object with the name of your alias that has the following properties:     subscription: The subscription were the template specs are saved. resourceGroup: The name of the resource group.    \"ts\": {           \"TemplateSpecs\": {             \"subscription\": \"5b389701-0e47-4738-b4fe-ddb67ac3a036\",             \"resourceGroup\": \"gaag-rg-templates\"           } }   In your bicep file specify a module like you have always done but make sure to do a specific implementation for the path to the module:    ts/[template spec alias name]:[template spec name]:[template spec version]   An example of a module with a template spec looks like below    module automation 'ts/TemplateSpecs:az-tempspec-automationaccount:0.2' ={   name: 'automationSpecTempSpec'   params:{     automationaccountName: 'azautomationSpec'     logAnalyticsWorkspaceName: 'azlaworkspaceName'     location: location   } }   When opening a bicep file with a module from a repository the bicep extension tries to download the module to the local cache by using the command bicep restore in the background.    The linter within Visual Studio Code will show if the downloading does not succeed. To be able to download the file you must have access to the template spec.    Restoring / downloading the module from the private repository can also be done manually, by running the following command:    build restore [bicep file]   When the file is downloaded to the local cache you will have intellisense within Visual Studio Code helping you configure the module.       Azure Container Registry    Like mentioned above the second option for saving and sharing modules is a Azure Container Registry. Once you have a Azure Container Registry it is very easy to start saving your modules.    First we need to get a module within the Azure Container registry and this is were the new command 'publish' comes in. For this command you need to specify the source file and the target path within the container registry.    bicep publish .\\.modules\\004-vnet.bicep --target br:azcrbicepregistry.azurecr.io/bicep/modules/vnet:1.0   Executing the command uploads the module to the registry.       For the bicep repository we will also make an alias as this makes it very ease to reference the container registry.    Add a new alias within 'br' by creating a new object with the name of your alias that has the following property:     registry: The URL of the container registry.      \"br\": {       \"SponsorRegistry\": {         \"registry\": \"azcrbicepregistry.azurecr.io\"       }     }   In your bicep file register a module like you have always done but make sure and now use the specific implementation for a bicep repository:    br/[repositry alias name] or [registry url]:[path to the module]:[version]   An example of a module with a bicep repository looks more or less the same as a regular one.    module storageExternal 'br/SponsorRegistry:bicep/modules/storage-account:v1.0' ={   name: 'storageAccountExternal'   params:{     accountName:'azstrbiceptestingexternal'     location:location   } }   Downloading the module works the same as with the template specs repository.      Good to know    Like always there are a few things to be aware of:     Role based access applies to both repositories. This means when publishing modules to a registry or template spec you must have permissions. For the container registry you must have permissions to push an image. To be able to deploy a module from a Template spec you must have read permissions and for a container registry you must have permissions to pull an image. The bicep command uses credentials on the background from the Azure CLI or Azure PowerShell. There are situations were it is needed to make sure which credentials are used. This is configured in the bicepconfig.json. Sample for this is show in the following snippet.      \"cloud\": {     \"currentProfile\": \"AzureCloud\",     \"credentialPrecedence\": [       \"AzureCLI\",       \"AzurePowerShell\"     ]   }    ","categories": ["Azure"],
        "tags": ["Azure","Azure Container Registry","Azure Template Specs","bicep","biceplang","IaC"],
        "url": "/2021/11/using-private-repositories-for-bicep-modules/",
        "teaser": null
      },{
        "title": "Export existing Azure resources to Bicep",
        "excerpt":"With the new release of Bicep (v0.4.1124) it is now possible to export existing Azure resources to Bicep code. This means you do not have to export your resources to ARM and then do a decompile.    Exporting your existing resources is very easy and can be done within Visual Studio Code.    Get Resource ID    To be able to export the resource to Bicep you need to have the resource ID of the resource. You can find this resource very easily by opening the resource within Azure and opening the \"properties\" blade. One of the properties in this blade is the Resource ID       Export Bicep    To export the template follow these steps:     Open Visual Studio Code Open an existing \"bicep\" file or create a new one. Press ctrl+shift+p or open the command pallet. Search for \"Bicep\" and select the option \"Insert resource\". This will ask for the resource ID. Paste the resource ID and press enter. The resource is now imported within your Bicep file.      ","categories": ["Azure"],
        "tags": ["Azure","bicep","biceplang"],
        "url": "/2021/12/export-existing-azure-resources-to-bicep/",
        "teaser": null
      },{
        "title": "User-friendly compliance messages for Azure Policy",
        "excerpt":"When resource creation fails because of Azure Policies the error is not always that descriptive and you will have to search for the failure. If you look at the below example you see an error that arises when creating a resource group. The error only tells us that it is because of policy configuration but what could be the cause:      To find the cause the raw error investigated to find the policy that is denying the creation. As shown below the creation is denied by the policy assignment called \"Resource Group - Naming MVP\".       User-friendly none compliant messages    Within Azure, it is possible to attach user-friendly non-compliant messages that the user will see when something is denied by policies when creating resources.    These messages can be added to the platform when assigning policies in Azure. The message can be added in the \"Non-compliancy message\" tab of the policy assignment blade.       By adding this non-compliance message it becomes clearer when something fails because of policies. The only downside at this moment is that you still have to look at the raw error. The screenshot below shows that the non-compliance message is now included in the error giving much more insight on why the creation failed.       Assigning non-compliancy message via bicep    The non-compliance message can also be added when assigning policies through bicep or ARM. In the below snippet you can find the property called \"nonComplianceMessages\" that contains the possible non-compliance messages.    resource policyAssignment'Microsoft.Authorization/policyAssignments@2021-06-01' = {   name: 'string'   location: 'string'   properties: {     description: 'string'     displayName: 'string'     enforcementMode: 'string'     metadata: any()     nonComplianceMessages: [       {         message: 'string'         policyDefinitionReferenceId: 'string'       }     ]     notScopes: [       'string'     ]     parameters: {}     policyDefinitionId: 'string'   } }   Conclusion    With the use of the non-compliance messages, it becomes easier to search for the policies that are denying the creation of resources.     It would be great if these non-compliance messages can be added to the regular error summary blade.   ","categories": ["Azure"],
        "tags": ["Azure","Compliance","Governance","Policy"],
        "url": "/2022/01/user-friendly-compliancy-messages-for-azure-policy/",
        "teaser": null
      },{
        "title": "Custom security recommendation within Microsoft Defender for Cloud",
        "excerpt":"Microsoft Defender for Cloud offers the option to include custom security recommendations. For custom security recommendations you can think of corporate security guidelines that need to be configured on cloud resources.    Microsoft Defender for Cloud    Microsoft defender for cloud is a combination of already existing tools within the Azure platform. It combines Azure Security Center and Azure Defender. When you would like to read more regarding this rename I would suggest you read this post from Microsoft.    Defender for Cloud is the tool for security posture management and threat protection. It helps you to strengthen the security posture of cloud resources. Defender for Cloud offers the tooling to harden your resources with security recommendations.    Microsoft Defender for Cloud helps streamline the process for meeting regulatory compliance requirements, using the&nbsp;regulatory compliance dashboard. Within this dashboard, there are by default a couple of Industry &amp; regulatory standards already present to help you keep compliance.    On that dashboard, the Azure Security Benchmark is activated by default.       The screenshot above shows a couple of regulatory standards that are activated within my environment but you also have the option to add additional standards by choosing the \"add more standards\" button.    Custom security policy    Within organizations, there are security regulations that need to be followed. In a perfect situation, you want to be able to add these standards and security policies to the Azure platform. This can be done by defining Azure Policies and Azure Policy Initiatives.    Azure Policies    Azure Policies can be defined within bicep and via this way also deployed to the Azure Platform. The snippet below shows an Azure Policy that audits Azure resource groups for specific tags. When a resource group for example has the tag \"environment\" and the value is set to \"production\" the resource group should have a resource lock configured.    targetScope = 'managementGroup'  param policyCategory string = 'Custom' param policySource string = 'Guardrails'  resource bicepAuditResourceLock 'Microsoft.Authorization/policyDefinitions@2021-06-01' = {   name: 'Audit resource locks based on tags'   properties: {     displayName: 'Audit Resource Locks on Resource Groups based on Tags'     description: 'Audit the use of resource locks on resource groups based on tags used'     policyType: 'Custom'     mode: 'All'     metadata: {       category: policyCategory       source: policySource       version: '0.1.0'       securityCenter: {         RemediationDescription: 'The resource group should have a resource locks assigned. Go to the resource group and assign a resource lock or adjust the tag value'         Severity: 'High'       }     }     parameters: {       tagName: {         type: 'String'         metadata: {           displayName: 'Tag Name'           description: 'The Tag name to audit against (i.e. Environment, CostCenter, etc.)'          }       }       tagValue: {         type: 'String'         metadata: {          displayName: 'Tag Value'          description: 'Value of the tag to audit against (i.e. Prod/UAT/TEST, 12345, etc.)'         }        }     }     policyRule: {       if: {         allOf: [          {           field: 'type'           equals: 'Microsoft.Resources/subscriptions/resourceGroups'          }          {           field: '[concat(\\'tags[\\', parameters(\\'tagName\\'), \\']\\')]'           equals: '[parameters(\\'tagValue\\')]'          }         ]        }        then: {         effect: 'auditIfNotExists'         details: {          type: 'Microsoft.Authorization/locks'          existenceCondition: {           field: 'Microsoft.Authorization/locks/level'           equals: 'CanNotDelete'          }         }        }     }   } }  output policyDefId string = bicepAuditResourceLock.id   Policy initiatives    If there are a lot of policies that need to be configured, these policies can be grouped into policy initiatives. This helps you manage the policies in groups and also check the compliance groups on for example a specific category.    The below snippet shows a bicep file for a policy initiative that contains several policies but also a policy initiative where the policy is linked.    targetScope = 'managementGroup'  param policyCategory string = 'Custom' param policySource string = 'Guardrails'  //tag parameters param lockTag string = 'environment' param lockValue string = 'prd'  module auditPolicy '../audit_hybrid_use_benefit/policy.bicep' = {   name: 'auditPolicy'   params: {     policyCategory: policyCategory     policySource: policySource   } }  module auditResourceLock '../audit_resource_locks_on_resource_groups_based_on_tags/policy.bicep' = {   name: 'auditResourceLock'   params: {     policyCategory: policyCategory     policySource: policySource   } }  module deployResourceLock '../deploy_resource_locks_on_resource_groups_based_on_tags/policy.bicep' = {   name: 'deployResourceLock'   params: {     policyCategory: policyCategory     policySource: policySource   } }  resource corporateInitiative 'Microsoft.Authorization/policySetDefinitions@2021-06-01' = {   name: 'corporate_initiative'   properties: {     policyType: 'Custom'     displayName: 'Corporate Initiative'     description: 'Corporate Initiative containing all corporate guardrails'     metadata: {       category: policyCategory       source: policySource       version: '0.1.0'     }     parameters: {}     policyDefinitions: [       {         policyDefinitionId: auditPolicy.outputs.policyDefId         parameters: {}       }       {         policyDefinitionId: auditResourceLock.outputs.policyDefId         parameters: {           tagName: {             value: lockTag           }           tagValue: {             value: lockValue           }         }       }       {         policyDefinitionId: deployResourceLock.outputs.policyDefId         parameters: {           tagName: {             value: lockTag           }           tagValue: {             value: lockValue           }         }       }     ]   } }   Deploy initiative    The initiative shown above can be deployed in a couple of ways. The best approach is to configure an Azure DevOps / GitHub project to manage and deploy the configuration but for a short reference, we will use PowerShell.    Login-AzAccount  bicep build \".\\policies\\corporate_initiative\\initiative.bicep\" $tempFile = \".\\policies\\corporate_initiative\\initiative.json\" $mgId = \"mgid\"  New-AzManagementGroupDeployment -Name \"DeployInitiative\" -ManagementGroupId $mgId -TemplateFile $tempFile -Location \"WestEurope\"   The script deploys the initiative to an Azure Management Group high in the hierarchy. This gives the ability to use the initiative and policies on a lower level as well.    Security Settings    If the platform contains a policy initiative with the security guardrails of the organization, this initiative can be used as a custom security policy within Microsoft Defender for Cloud.    This custom initiative can be configured with the following steps:     Go to \"Microsoft Defender for Cloud\" within the Azure portal. Click on \"Environment Settings\" on the left side of the \"Microsoft Defender for Cloud\" blade Click on the correct environment that needs to be configured.        Within the environment settings blade make sure the \"Security policy\" is selected. On the \"Security policy\" blade scroll down to \"Your custom initiatives\" and click on \"Add a custom initiative\" On the next blade select your custom initiative and click add. The policy initiative will be configured and assigned to the environment.    Now that the initiative is configured it takes some time to show up correctly within the portal but when it does you can, for example, find it under \"Regulatory compliance\".       Custom remediation steps    In the OOTB security policies remediation steps can be found to remediate the issues. Within custom policies, there is also an option. Within the policy code, additional data needs to be supplied in the \"metadata\" property.    resource bicepAuditResourceLock 'Microsoft.Authorization/policyDefinitions@2021-06-01' = {   name: 'Audit resource locks based on tags'   properties: {     displayName: 'Audit Resource Locks on Resource Groups based on Tags'     description: 'Audit the use of resource locks on resource groups based on tags used'     policyType: 'Custom'     mode: 'All'     metadata: {       category: policyCategory       source: policySource       version: '0.1.0'       securityCenter: {         RemediationDescription: 'The resource group should have a resource locks assigned. Go to the resource group and assign a resource lock or adjust the tag value'         Severity: 'High'       }     }   The above snippet is a small section of the policy that contains the \"metadata\" property. Within the metadata property, there is the option to specify Security Center specific information like the remediation description. Additional options can be found in the security center API reference.  ","categories": ["Azure"],
        "tags": ["Azure","Azure Policy","Defender","Governance"],
        "url": "/2022/02/custom-security-recommendation-within-microsoft-defender-for-cloud/",
        "teaser": null
      },{
        "title": "Version number counter now supports the build identity",
        "excerpt":"I released a new version of the \"Version number counter\" extension for Azure DevOps this week. The \"Version number counter\" extension for Azure Pipelines is an extension that automatically increments a version number on each run.&nbsp;     Version number counter - Visual Studio Marketplace    With this new version, I added another great feature. This addition includes the support of the build identity. By using the build identity, you do not have to specify a Personal Access Token for the authentication, but the authentication goes via OAuth of the build identity.    How to use the extenstion    After installing the extension, the extension can be made available within your pipeline via the UI or your favorite IDE.    Permissions    Specific permissions are required for both options (OAuth or PAT) to increment the version variable.     OAuth    When using the OAuth system token. The build identity: Project Collection Build Service ({OrgName}) should have the \"Edit build pipeline\" permissions on the pipeline.    Personal Access Token    The minimal permissions required for a PAT are:     Build: Read &amp; Execute    Classic    When using the classic pipelines, drag the extension to the pipeline and configure it correctly.      When using the OAuth token, ensure that the \"Allow scripts to access the OAuth token\" is configured on the agent job.       Yaml    The below snippet represents how to use the extension in YAML and make sure that it can use the OAuth token of the build identity.    - task: versioncounter@2   inputs:     VersionVariable: 'versionnumber'     UseSystemAccessToken: true   env:       SYSTEM_ACCESSTOKEN: $(System.AccessToken)   When you would like more information about job authorizations and the use of the build identity, take a look at the below article:     Understand job access tokens - Azure Pipelines | Microsoft Docs   ","categories": ["Azure DevOps"],
        "tags": [],
        "url": "/2022/02/version-number-counter-now-supports-the-build-identity/",
        "teaser": null
      },{
        "title": "Azure Costs Analysis preview with Anomaly detection",
        "excerpt":"Microsoft has released new functionality in public preview to manage the costs of Azure subscriptions. This new functionality gives excellent insights into the costs and lets you detect any anomaly of the expenses on those subscriptions.    New Cost Analysis Preview    The new cost analysis preview blade shows the Azure resources in a different overview than before. This new overview gives you more insight into your data and your costs.    Azure Cost Analysis Preview    To use the cost analysis preview, go to the following link. On this blade, select the appropriate scope.       The overview offers the option to report on and analyze your cloud costs and review critical insights to understand better and control spending patterns. This is accomplished on four levels:     Resource Groups Subscriptions Services Reservations    All four overviews offer great insights. I love the services overview where you see the costs per service and can view the expenses underneath it. For example, bandwidth costs and VPN gateway costs.       Anomaly detection    The costs analysis preview now also includes anomaly detection. To better understand the functionality, we should also know what an anomaly is. If we look into a dictionary, we will find something like:     Something different, abnormal, peculiar, or not easily classified:&nbsp;something&nbsp;anomalous. Deviation from the common rule:&nbsp;irregularity    It means that a cost increase of 5000 euros could be an anomaly regarding cost management. But that would not be the case if the billing was raised monthly, for example.      After selecting a specific view for the first time, the preview is enabled, and you will get more insights regarding the Azure costs. When using the preview for the first time, you will see the below screenshot.      Opening up the blade after a day will show any anomaly, if there are any.   ","categories": ["Azure"],
        "tags": ["Azure","Costs","Preview"],
        "url": "/2022/03/azure-costs-analysis-preview-with-anomaly-detection/",
        "teaser": null
      },{
        "title": "Building your Azure Policies - Part 1",
        "excerpt":"Azure Policy is a way of enforcing standards and guardrails and assessing resource compliance. In Azure, the compliance dashboard offers an aggregated view to validate the state of the platform with additional options to see the resources that are not compliant.       Policy    An Azure Policy itself evaluates resources within Azure based on the rules you specify. The rules are defined in a JSON format and called policy definitions, and policies can also be grouped into so-called policy initiatives. The policies themself can be deployed on several scopes that, of course, keep the standard Azure inheritance in place. The scopes it can be deployed to are:     Management groups Subscriptions Resource Groups Resources    If there are resources at a scope that can not comply with the guardrails/standards, these resources can be excluded by defining an exclusion on the policy assignment.    The Azure Portal offers a lot of policies OOTB, but in many situations, you want to create your own. This is when the policy rules come in.    Policy Rule Effects    As mentioned above, Azure Policies have rules that have specific effects. At the time of writing this article, the following effects are supported:     Append: Append additonal information / fields on the resource. Like appending additional rows to an firewall array. Audit: Log specific information as warning in the activity log. AuditIfNotExists: Audit resource when specific configuration is missing and have gone through the if condition of the rule. Deny: Deny creation when some configuration is missing or the configuration is not allowed by the standards. DeployIfNotExists: Deploy resources when not configured. This will trigger an template deployment. Disabled: Disabled effect means that the policy itself is disabled. Modify: Modify an existing deployment to fit the rules specified. This is mainly used to do CRUD operations on properties and tags for subscriptions, resource groups and resources.    When working with policies and custom policies, it is good to know the order of the evaluation process. The below-ordered list shows this:     Disabled Append and Modify Deny Audit AuditIfNotExists and DeployIfNotExists.    When are policies evaluated    Policies are evaluated on multiple occasions and, depending on the way used, will show results in around 30 minutes:     Manual / On-demand At policy assignment When changing a policy assignment When exemptions are created or updated. Default evaluation cycle (once every 24 hours) Resource creation or update    For more information about policies, read the Azure Policy documentation.    Creating your own Policy    Creating your policy is done using Bicep or writing the definition in JSON. With the use of Bicep, you can use the standard way of deploying the policies.     We will work on a policy that solves the following problem for this series of posts.    As a administrator I want to make sure that resources that are tagged as production resources are not able to be changed. Unless specific actions are taken.     The above problem can be solved by combining Azure Policies with Azure Resource Locks, especially the read-only lock deployed with the \"DeployIfNotExists\" effect.    resource bicepDeployResourceLock 'Microsoft.Authorization/policyDefinitions@2021-06-01' = {   name: 'Deploy read-only resource locks based on tags'   properties: {} }   The first part of the policy in Bicep contains the default information, and the policy itself is specified in the properties property. Let's discuss that in parts.    Policy Metadata    The first section is the metadata. The policy metadata contains all the information about the policy that defines the name, description, category, and everything else. With the help of the 'securityCenter' part, you can specify additional feedback for Azure Security Center.    For our use case, these properties are filled with information to identify the policy correctly.    properties: {     displayName: 'Deploy read-only resource lock on resource groups based on tag'     description: 'Deploy read-only resource lock on resource groups based on tag'     policyType: 'Custom'     mode: 'All'     metadata: {       category: 'Custom'       source: 'Guardrails'       version: '0.1.0'       securityCenter: {         RemediationDescription: 'The resource group should have a resource locks assigned. Go to the resource group and assign a resource lock or adjust the tag value'         Severity: 'High'       }     }     parameters: {...     }     policyRule: {...     } }   Policy Parameters    The second part we will discuss is the parameters. Within the policy parameter section, parameters for the policy itself can be defined. Using this adds the ability to supply information when assigning the policies.    For this use case, we will define two parameters. The tag name so that we can specify against what tag we want to check within the rule and tag value used to check for the rule to be successful.    properties: {     metadata: {...     }     parameters: {       tagName: {         type: 'String'         metadata: {           displayName: 'Tag Name'           description: 'The Tag name to check against (i.e. Environment CostCenter etc.)'         }       }       tagValue: {         type: 'String'         metadata: {           displayName: 'Tag Value'           description: 'Value of the tag to audit against (i.e. Prod/UAT/TEST 12345 etc.)'         }       }     }     policyRule: {...     } }   Policy Rule    The last part is the most important. This section contains the rule, which is the real heart of the policy. This section defines where to check against and what action (effect) is handled.    properties: {     metadata: {...     }     parameters: {...     }     policyRule: {       if: {         allOf: [           {             field: 'type'             equals: 'Microsoft.Resources/subscriptions/resourceGroups'           }           {             field: '[concat(\\'tags[\\', parameters(\\'tagName\\'), \\']\\')]'             equals: '[parameters(\\'tagValue\\')]'           }         ]       }       then: {         effect: 'deployIfNotExists'         details: {           type: 'Microsoft.Authorization/locks'           roleDefinitionIds: [             '/providers/microsoft.authorization/roleDefinitions/8e3af657-a8ff-443c-a75c-2fe8c4bcb635'           ]           existenceCondition: {             field: 'Microsoft.Authorization/locks/level'             equals: 'CanNotDelete'           }           deployment: {             properties: {               mode: 'incremental'               template: {                 '$schema': 'http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#'                 contentVersion: '1.0.0.0'                 parameters: {}                 variables: {}                 resources: [                   {                     name: 'ReadOnly'                     type: 'Microsoft.Authorization/locks'                     apiVersion: '2017-04-01'                     properties: {                       level: 'ReadOnly'                       notes: 'Prevent changes to the resource group and resources'                     }                   }                 ]                 outputs: {                   policy: {                     type: 'string'                     value: '[concat(\\'Added resource lock\\')]'                   }                 }               }             }           }         }       }     }   In the above rule, we do the following: If the field type is of the type resource group and field tags contains a tag with the specified tag name (the parameter) with the value (the parameter) that is configured, then deploy the specified deployment template, which includes the read-only lock.    The deployment part of the rule contains an ARM template for the deployment.    Remediation    In Azure, you can also remediate existing resources. In our example, remediating a resource group means that the remediation is done when instructing Azure Policy to remediate the resource.     Azure Policy in Bicep    The complete policy in Bicep looks like the snapshot below.    targetScope = 'managementGroup'  param policyCategory string = 'Custom' param policySource string = 'Guardrails'  resource bicepDeployResourceLock 'Microsoft.Authorization/policyDefinitions@2021-06-01' = {   name: 'Deploy read-only resource locks based on tags'   properties: {     displayName: 'Deploy read-only resource lock on resource groups based on tag'     description: 'Deploy read-only resource lock on resource groups based on tag'     policyType: 'Custom'     mode: 'All'     metadata: {       category: policyCategory       source: policySource       version: '0.1.0'       securityCenter: {         RemediationDescription: 'The resource group should have a read-only resource locks assigned. Go to the resource group and assign a resource lock or adjust the tag value'         Severity: 'High'       }     }     parameters: {       tagName: {         type: 'String'         metadata: {           displayName: 'Tag Name'           description: 'The Tag name to audit against (i.e. Environment CostCenter etc.)'         }       }       tagValue: {         type: 'String'         metadata: {           displayName: 'Tag Value'           description: 'Value of the tag to audit against (i.e. Prod/UAT/TEST 12345 etc.)'         }       }     }     policyRule: {       if: {         allOf: [           {             field: 'type'             equals: 'Microsoft.Resources/subscriptions/resourceGroups'           }           {             field: '[concat(\\'tags[\\', parameters(\\'tagName\\'), \\']\\')]'             equals: '[parameters(\\'tagValue\\')]'           }         ]       }       then: {         effect: 'deployIfNotExists'         details: {           type: 'Microsoft.Authorization/locks'           roleDefinitionIds: [             '/providers/microsoft.authorization/roleDefinitions/8e3af657-a8ff-443c-a75c-2fe8c4bcb635'           ]           existenceCondition: {             field: 'Microsoft.Authorization/locks/level'             equals: 'ReadOnly'           }           deployment: {             properties: {               mode: 'incremental'               template: {                 '$schema': 'http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#'                 contentVersion: '1.0.0.0'                 parameters: {}                 variables: {}                 resources: [                   {                     name: 'ReadOnly'                     type: 'Microsoft.Authorization/locks'                     apiVersion: '2020-05-01'                     properties: {                       level: 'ReadOnly'                       notes: 'Prevent changed of the resource group or its resources'                     }                   }                 ]                 outputs: {                   policy: {                     type: 'string'                     value: '[concat(\\'Added resource lock\\')]'                   }                 }               }             }           }         }       }     }   } }  output policyDefId string = bicepDeployResourceLock.id   Deploy the Azure Policy    When the policy is defined in Bicep, it can quickly be deployed with the standard deployment commands on the scope you want. The below snippet shows how to deploy the file on the management group scope.    $tempFile = \"..\\policy.bicep\" $mgId = \"mgid\"  New-AzManagementGroupDeployment -Name \"DeployPolicy\" -ManagementGroupId $mgId -TemplateFile $tempFile -Location \"WestEurope\"   The definition can also be defined in JSON. When expressed in JSON, the structure is the same. The snipped below shows it defined in JSON.    {  \"properties\": {   \"displayName\": \"Deploy read-only resource lock on resource groups based on tag\",   \"policyType\": \"Custom\",    \"mode\": \"All\",     \"metadata\": {       \"category\": \"policyCategory\",       \"source\": \"policySource\",       \"version\": \"0.1.0\",       \"securityCenter\": {         \"RemediationDescription\": \"The resource group should have a read-only resource locks assigned. Go to the resource group and assign a resource lock or adjust the tag value\",         \"Severity\": \"High\"       }     },   \"parameters\": {    \"tagName\": {     \"type\": \"String\",     \"metadata\": {      \"displayName\": \"Tag Name\",      \"description\": \"The name of the tag to check against.\"     }    },    \"tagValue\": {     \"type\": \"String\",     \"metadata\": {      \"displayName\": \"Tag Value\",      \"description\": \"Value of the tag to check against\"     }    }   },   \"policyRule\": {    \"if\": {     \"allOf\": [      {       \"field\": \"type\",       \"equals\": \"Microsoft.Resources/subscriptions/resourceGroups\"      },      {       \"field\": \"[concat('tags[', parameters('tagName'), ']')]\",       \"equals\": \"[parameters('tagValue')]\"      }     ]    },    \"then\": {     \"effect\": \"deployIfNotExists\",     \"details\": {      \"type\": \"Microsoft.Authorization/locks\",      \"roleDefinitionIds\": [       \"/providers/microsoft.authorization/roleDefinitions/8e3af657-a8ff-443c-a75c-2fe8c4bcb635\"      ],      \"existenceCondition\": {       \"field\": \"Microsoft.Authorization/locks/level\",       \"equals\": \"ReadOnly\"      },      \"deployment\": {       \"properties\": {        \"mode\": \"incremental\",        \"template\": {         \"$schema\": \"http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",         \"contentVersion\": \"1.0.0.0\",         \"parameters\": {},         \"variables\": {},         \"resources\": [          {           \"name\": \"ReadOnly\",           \"type\": \"Microsoft.Authorization/locks\",           \"apiVersion\": \"2017-04-01\",           \"properties\": {            \"level\": \"ReadOnly\",            \"notes\": \"Prevent changes of the resourceGroup and its resources\"           }          }         ],         \"outputs\": {          \"policy\": {           \"type\": \"string\",           \"value\": \"[concat('Added resource lock')]\"          }         }        }       }      }     }    }   }  } }   With the help of the below PowerShell function, the policy definition can be deployed to the Azure Platform.    [CmdletBinding()] Param(   [Parameter(Mandatory = $true)]   [ValidateSet('Subscription', 'Managementgroup')]   [string]$Scope,   [Parameter(Mandatory = $true)]   [string]$ScopeName,   [Parameter(Mandatory = $true)]   [string]$PolicyFile,   [Parameter(Mandatory = $false)]   [string]$RoleIds )  $policyDefinitionFileContent = Get-Content -Raw -Path $PolicyFile $policyDefinitionFile = ConvertFrom-Json $policyDefinitionFileContent $policyDefinitionName = $policyDefinitionFile.properties.displayName  $parameters = @{} $parameters.Add(\"Name\", $policyDefinitionName) switch ($Scope) {     \"ManagementGroup\" {       $parameters.Add(\"ManagementGroupName\", $ScopeName)     }     \"Subscription\" {         $sub = Get-AzSubscription -SubscriptionName $ScopeName         $parameters.Add(\"SubscriptionId\", $sub.Id)     } }  $definition = Get-AzPolicyDefinition @parameters -ErrorAction SilentlyContinue  $parameters.Add(\"Policy\", $policyDefinitionFileContent) if($definition){     Write-Output \"Policy definition already exists, policy will be updated\" }else{     Write-Output \"Policy does not exist\" }  New-AzPolicyDefinition @parameters   To be continued   ","categories": ["Azure"],
        "tags": ["Azure","Compliance","Policy"],
        "url": "/2022/03/building-your-azure-policies-part-1/",
        "teaser": null
      },{
        "title": "Building your Azure Policies (Azure DevOps) - Part 2",
        "excerpt":"As written in Part 1, policies are used to maintain the state. The Azure portal itself contains a lot of OOTB policies, but you also want to create and deploy your own. This part will go over how to deploy custom policy definitions using Azure DevOps.    Script    The script shared in part 1 is adjusted to read out the policy definition files within a directory. This is done because you do not want to deploy just one definition per pipeline. Below the updated PowerShell script is added.    [CmdletBinding()] Param(     [Parameter(Mandatory = $true)]     [ValidateSet('Subscription', 'Managementgroup')]     [string]$Scope,     [Parameter(Mandatory = $true)]     [string]$ScopeName,     [Parameter(Mandatory = $true)]     [string]$PolicyFolder,     [Parameter(Mandatory = $false)]     [string]$RoleIds )  $policyFiles = Get-ChildItem -Path $PolicyFolder -Recurse -Filter \"*.json\" foreach ($policyFile in $policyFiles) {      Write-Output \"Working on Policy: $($policyFile.Name)\"      $policyDefinitionFileContent = Get-Content -Raw -Path $PolicyFile     $policyDefinitionFile = ConvertFrom-Json $policyDefinitionFileContent     $policyDefinitionName = $policyDefinitionFile.properties.displayName      $parameters = @{}     $parameters.Add(\"Name\", $policyDefinitionName)     switch ($Scope) {         \"ManagementGroup\" {             $parameters.Add(\"ManagementGroupName\", $ScopeName)         }         \"Subscription\" {             $sub = Get-AzSubscription -SubscriptionName $ScopeName             $parameters.Add(\"SubscriptionId\", $sub.Id)         }     }      $definition = Get-AzPolicyDefinition @parameters -ErrorAction SilentlyContinue      $parameters.Add(\"Policy\", $policyDefinitionFileContent)     if ($definition) {         Write-Output \"Policy definition already exists, policy will be updated\"     }     else {         Write-Output \"Policy does not exist\"     }      New-AzPolicyDefinition @parameters }   Policy definitions can be saved in Azure DevOps as code. The definitions in source control can be deployed via Azure DevOps Pipelines.    Prerequisites    To be able to start deploying the definitions, we need to have the following in place:     Service principal in Azure Active Directory Service Connection in Azure DevOps Access rights in Azure Pipeline    First couple of prerequisites    We will need a service principal to deploy policy definitions via Azure DevOps. Creating this can be done via the portal. When the principal is made, give that principal the correct permissions on the scope where you want to deploy the definitions. I will provide the principal access to the management group where the policies need to be deployed for this article.    Of course, we can give the principal owner permissions on the scope, but we want to stick to the least privileges. Therefore we provide the principal the 'Resource Policy Contributor' role, which is enough for deploying Azure Policies.       With the rights in place, the service connection in Azure DevOps can be configured. In Azure DevOps, create an \"Azure Resource Manager\" service connection and fill in the correct information regarding your platform.        Pipeline    Next up is the pipeline. For the pipeline, we will start with an empty one that we save in a GitHub repository, where we also save the policy definition files. From the empty pipeline, remove the default tasks and add an Azure PowerShell task that connects to the Service Connection we created.       Point this task to the correct script file in the repository and ensure the arguments are supplied using variables. The Yaml of the task will look like the snippet below.    - task: AzurePowerShell@5   inputs:     azureSubscription: 'Root Management Group Connection'     ScriptType: 'FilePath'     ScriptPath: './scripts/azpolicy.ps1'     ScriptArguments: '-Scope \"$(scope)\" -ScopeName \"$(scopeName)\" -PolicyFolder \"$(folder)\"'     azurePowerShellVersion: 'LatestVersion'     pwsh: true   Bringing this all together will result in a simple pipeline like below.    trigger: - main  pool:   vmImage: ubuntu-latest  variables:   - name: scope     value: \"ManagementGroup\"   - name: scopeName     value: \"mgName\"   - name: folder     value: \"./policies/deploy\"  steps: - task: AzurePowerShell@5   inputs:     displayName: 'Deploy Azure Policy Definitions'     azureSubscription: 'Root Management Group Connection'     ScriptType: 'FilePath'     ScriptPath: './scripts/azpolicy.ps1'     ScriptArguments: '-Scope \"$(scope)\" -ScopeName \"$(scopeName)\" -PolicyFolder \"$(folder)\"'     azurePowerShellVersion: 'LatestVersion'     pwsh: true    Of course, this is not a production-grade solution, but it highlights how to manage your policy definitions in code and how to deploy them. In the following article, we will deploy definitions via GitHub Actions.    To be continued  ","categories": ["Azure","Azure DevOps","DevOps"],
        "tags": ["Azure","Azure DevOps","Compliance","Policy"],
        "url": "/2022/03/building-your-azure-policies-azure-devops-part-2/",
        "teaser": null
      },{
        "title": "Building your Azure Policies (GitHub Actions) - Part 3",
        "excerpt":"As written in part 1, policies are used to maintain the state. The Azure portal itself contains a lot of OOTB policies, but you also want to create and deploy your own. In this article, we will go over how to deploy the policy definitions using GitHub Actions.    In part 2, we deployed the definitions via Azure DevOps pipelines, and for the Github Actions, we will also use the script file. But in GitHub Actions, there is also another option for managing policy definitions.    Prerequisites     Service principal in Azure Active Directory Credentials saved wihtin GitHub Access rights in Azure GitHub workflow    First couple of prerequisites    For this test, we will reuse the principal that we created in part 2 of the series. The credentials for this will be saved within a Github Secret.    GitHub Secret    The information for the authentication is saved within so-called&nbsp;secrets&nbsp;that are encrypted within GitHub that are saved on the organization, repository, or repository environment level. The credential information for the authentication against Azure is saved in JSON format.    {     \"clientId\": \"[clientId]\",     \"clientSecret\": \"[clientSecret]\",     \"subscriptionId\": \"[subscription id]\",     \"tenantId\": \"[Azure Active Directory Tenant Id]\" }   To save the credential information, you can follow the below steps:     Within the GitHub repository go to settings and then secrets.        Click on “new repository secret” Fill in a name for the secret and use the json object for the value of the secret.       Use the Azure steps within GitHub Actions    With the credentials saved, we can get started with the workflow. Let's create a new workflow in the UI and give it a name and file name and remove the information that is not required. To work with Azure, we will use the so-called Azure steps and start with the login step.    Add the 'azure/login' step and connect it to the correct secret. The YAML snippet is below.    - uses: azure/login@v1       with:         creds: $.         enable-AzPSSession: true    \"Make sure you add 'enable-AzPSSession: true' if you want to make use of Azure PowerShell in the workflow.\"    In this task, you see the reference to the secret we saved in the previous paragraph.    GitHub Actions and Azure PowerShell    If you want to start from GitHub and deploy your definitions that haven't been deployed to Azure yet, you can reuse the script that we used in part 2 and execute it within GitHub actions via the Azure PowerShell Action.        - name: Run Azure PowerShell script       uses: azure/powershell@v1       with:         inlineScript: |           ./scripts/azpolicy.ps1 -Scope \"$\" -ScopeName \"$\" -PolicyFolder \"$\"         azPSVersion: \"latest\"   We reference the same script file and supply it with the correct arguments in the task. For your reference, the complete Github Actions file would look like below.    name: Policy - All Policies on:   workflow_dispatch:     inputs:       remarks:         description: 'Reason for triggering the workflow run'         required: false         default: 'Updating Azure Policies'  env:   Folder: './deploy'   Scope: 'ManagementGroup'   ScopeName: '324f7296-1869-4489-b11e-912351f38ead'  jobs:   build:     runs-on: ubuntu-latest     steps:     - name: Checkout       uses: actions/checkout@v2     - uses: azure/login@v1       with:         creds: $         enable-AzPSSession: true      - name: Run Azure PowerShell script       uses: azure/powershell@v1       with:         inlineScript: |           ./scripts/azpolicy.ps1 -Scope \"$\" -ScopeName \"$\" -PolicyFolder \"$\"         azPSVersion: \"latest\"    GitHub Actions and Manage Policy Task    GitHub actions contain a task (Manage Azure Policy) to manage policy definitions, deploy them to the correct scopes, and manage the assignments. The downside of this task is that it requires the definitions to be deployed on Azure because it references the ids.    To use this, it is best to export the definitions from Azure and work in the below folder hierarchy.       The name of the folders refers to the policy names, and the policy.json files contain the information of the policies. These files are the same as shown in part 1, except they have the id, type, and name properties. The task only needs to reference the policy folder, and you are good to go and manage your policy definitions.    name: Policy - All Policies on:   workflow_dispatch:     inputs:       remarks:         description: 'Reason for triggering the workflow run'         required: false         default: 'Updating Azure Policies'  jobs:   build:     runs-on: ubuntu-latest     steps:     - name: Checkout       uses: actions/checkout@v2     - uses: azure/login@v1       with:         creds: $     - name: Create or Update Azure Policies       uses: azure/manage-azure-policy@v0       with:               paths:  |                             policies/**   As you can see in the pipeline file, you may have noticed that this pipeline does not reference the management group. This is because the JSON file contains a reference to the id of the existing policy definition, so it knows where it is deployed.    With this task is, you can also manage your assignments by specifying that in the same folder structure when interested, take a look at the page of the extension.    Of course, this is not a production-grade solution, but it gives you the highlights on how to manage your policy definitions in code and how to deploy them.    To be continued     Part 1 - Building your Azure Policies – Part 1 Part 2 - Building your Azure Policies (Azure DevOps) – Part 2   ","categories": ["Azure","GitHub Actions"],
        "tags": ["Azure","GitHub Actions","Policy","Powershell"],
        "url": "/2022/03/building-your-azure-policies-github-actions-part-3/",
        "teaser": null
      },{
        "title": "Default Role Based Access Control on newly created resource groups",
        "excerpt":"Within our company, we have a sponsorship subscription we use for developing / testing purposes. As we heavily use this subscription, we often come into problems with access rights depending on the type of resource group.    In a larger environment, we would solve this by creating multiple subscriptions, but as we do not have multiple sponsorship subscriptions, we came up with another idea.    We categorize resource groups by using tags and came up with the idea to set up access rights on the resource groups based on the tags that are supplied on newly created resource groups. To get this operational, a colleague and I thought out a new custom policy that he created that I am sharing with the community.    For this policy, we use the policy effect 'deployIfnotexists'. By using this effect, we have the option to execute a deployment when a new resource group is created by using the below policy rule.                \"if\": {                 \"allOf\": [                     {                         \"field\": \"type\",                         \"equals\": \"Microsoft.Resources/subscriptions/resourceGroups\"                     },                     {                         \"field\": \"[concat('tags[', parameters('tagName'), ']')]\",                         \"equals\": \"[parameters('tagValue')]\"                     }                 ]             }   This rule checks if the type of the resource is a resource group and if it contains a tag with a specific value. The tag and its value it needs to check up on are specified when assigning the policy to a specific scope.    The 'then' of the rule will then execute a deployment, which is just a general RBAC deployment via ARM.    \"then\": {                 \"effect\": \"deployIfNotExists\",                 \"details\": {                     \"EvaluationDelay\": \"AfterProvisioningSuccess\",                     \"roleDefinitionIds\": [                         \"/providers/microsoft.authorization/roleDefinitions/8e3af657-a8ff-443c-a75c-2fe8c4bcb635\"                     ],                     \"type\": \"Microsoft.Authorization/roleAssignments\",                     \"existenceCondition\": {                         \"allOf\": [                             {                                 \"field\": \"Microsoft.Authorization/roleAssignments/roleDefinitionId\",                                 \"equals\": \"[concat('/subscriptions/',subscription().subscriptionId ,parameters('roleId'))]\"                             },                             {                                 \"field\": \"Microsoft.Authorization/roleAssignments/principalId\",                                 \"equals\": \"[parameters('principalId')]\"                             }                         ]                     },                     \"deployment\": {                         \"properties\": {                             \"mode\": \"incremental\",                             \"parameters\": {                                 \"principalType\": {                                     \"value\": \"[parameters('principalType')]\"                                 },                                 \"principalId\": {                                     \"value\": \"[parameters('principalId')]\"                                 },                                 \"roleId\": {                                     \"value\": \"[parameters('roleId')]\"                                 }                             },                             \"template\": {                                 \"$schema\": \"http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",                                 \"contentVersion\": \"1.0.0.0\",                                 \"parameters\": {                                     \"principalType\": {                                         \"type\": \"string\"                                     },                                     \"principalId\": {                                         \"type\": \"string\"                                     },                                     \"roleId\": {                                         \"type\": \"string\"                                     }                                 },                                 \"variables\": {},                                 \"resources\": [                                     {                                         \"name\": \"[guid(resourceGroup().id, deployment().name)]\",                                         \"type\": \"Microsoft.Authorization/roleAssignments\",                                         \"apiVersion\": \"2020-10-01-preview\",                                         \"properties\": {                                             \"principalId\": \"[parameters('principalId')]\",                                             \"roleDefinitionId\": \"[parameters('roleId')]\",                                             \"principalType\": \"[parameters('principalType')]\"                                         }                                     }                                 ],                                 \"outputs\": {                                     \"policy\": {                                         \"type\": \"string\",                                         \"value\": \"[concat('Added RBAC Rights')]\"                                     }                                 }                             }                         }                     }                 }             }   On the 'deployifnotexists', there is also an 'EvaluationDelay' specified. This specifies when the existence of the related resources should be evaluated. The delay is only used for evaluations that are a result of a create or update resource request. So the evaluation is done after the provisioning is succeeded.    The complete policy definitions then look like this.    {     \"properties\": {         \"displayName\": \"Add access rights based on tags\",         \"policyType\": \"Custom\",         \"mode\": \"All\",         \"description\": \"Policy to add access rights based on tags added to a resource group\",         \"metadata\": {             \"version\": \"1.0.0\",             \"category\": \"Custom\"         },         \"parameters\": {             \"tagName\": {                 \"type\": \"String\",                 \"metadata\": {                     \"displayName\": \"Tag Name\",                     \"description\": \"The Tag name to audit against (i.e. Environment CostCenter etc.)\"                 },                 \"defaultValue\": \"Environment\"             },             \"tagValue\": {                 \"type\": \"String\",                 \"metadata\": {                     \"displayName\": \"Tag Value\",                     \"description\": \"Value of the tag to audit against (i.e. Prod/UAT/TEST 12345 etc.)\"                 }             },             \"roleId\": {                 \"type\": \"string\",                 \"metadata\": {                     \"displayName\": \"roleId\",                     \"description\": \"roleId\",                     \"strongType\": \"Microsoft.Authorization/roleDefinitions\"                 }             },             \"principalId\": {                 \"type\": \"string\",                 \"metadata\": {                     \"displayName\": \"principalId\",                     \"description\": \"principalId\"                 }             },             \"principalType\": {                 \"type\": \"string\",                 \"metadata\": {                     \"displayName\": \"principalType\",                     \"description\": \"principalType\"                 },                 \"allowedValues\": [                     \"Device\",                     \"ForeignGroup\",                     \"Group\",                     \"ServicePrincipal\",                     \"User\"                 ]             }         },         \"policyRule\": {             \"if\": {                 \"allOf\": [                     {                         \"field\": \"type\",                         \"equals\": \"Microsoft.Resources/subscriptions/resourceGroups\"                     },                     {                         \"field\": \"[concat('tags[', parameters('tagName'), ']')]\",                         \"equals\": \"[parameters('tagValue')]\"                     }                 ]             },             \"then\": {                 \"effect\": \"deployIfNotExists\",                 \"details\": {                     \"EvaluationDelay\": \"AfterProvisioningSuccess\",                     \"roleDefinitionIds\": [                         \"/providers/microsoft.authorization/roleDefinitions/8e3af657-a8ff-443c-a75c-2fe8c4bcb635\"                     ],                     \"type\": \"Microsoft.Authorization/roleAssignments\",                     \"existenceCondition\": {                         \"allOf\": [                             {                                 \"field\": \"Microsoft.Authorization/roleAssignments/roleDefinitionId\",                                 \"equals\": \"[concat('/subscriptions/',subscription().subscriptionId ,parameters('roleId'))]\"                             },                             {                                 \"field\": \"Microsoft.Authorization/roleAssignments/principalId\",                                 \"equals\": \"[parameters('principalId')]\"                             }                         ]                     },                     \"deployment\": {                         \"properties\": {                             \"mode\": \"incremental\",                             \"parameters\": {                                 \"principalType\": {                                     \"value\": \"[parameters('principalType')]\"                                 },                                 \"principalId\": {                                     \"value\": \"[parameters('principalId')]\"                                 },                                 \"roleId\": {                                     \"value\": \"[parameters('roleId')]\"                                 }                             },                             \"template\": {                                 \"$schema\": \"http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",                                 \"contentVersion\": \"1.0.0.0\",                                 \"parameters\": {                                     \"principalType\": {                                         \"type\": \"string\"                                     },                                     \"principalId\": {                                         \"type\": \"string\"                                     },                                     \"roleId\": {                                         \"type\": \"string\"                                     }                                 },                                 \"variables\": {},                                 \"resources\": [                                     {                                         \"name\": \"[guid(resourceGroup().id, deployment().name)]\",                                         \"type\": \"Microsoft.Authorization/roleAssignments\",                                         \"apiVersion\": \"2020-10-01-preview\",                                         \"properties\": {                                             \"principalId\": \"[parameters('principalId')]\",                                             \"roleDefinitionId\": \"[parameters('roleId')]\",                                             \"principalType\": \"[parameters('principalType')]\"                                         }                                     }                                 ],                                 \"outputs\": {                                     \"policy\": {                                         \"type\": \"string\",                                         \"value\": \"[concat('Added RBAC Rights')]\"                                     }                                 }                             }                         }                     }                 }             }         }     } }     GitHub    Check out GitHub for the script files and the policy also supplied in Bicep:    https://github.com/maikvandergaag/msft-azureplatform/tree/main/policies/add_access   ","categories": ["Azure"],
        "tags": ["Azure","Policy","RBAC"],
        "url": "/2022/08/default-role-based-access-control-on-newly-created-resource-groups/",
        "teaser": null
      },{
        "title": "Microsoft Defender for DevOps",
        "excerpt":"During Ignite 2022, Microsoft released Microsoft Defender for DevOps. With Microsoft Defender for DevOps, you can discover, monitor, and detect threats in your source code management systems and source code.     A connection to the source management system is required to get these insights. With this connection, you allow Defender for Cloud to discover the resources in, for example, your Azure DevOps organization or your GitHub Repositories.    Connecting Azure DevOps    In the following steps, we will go over the procedure to connect Defender for DevOps to your Azure DevOps organization:     Log in to the Azure Portal and go to \"Defender for Cloud.\" In the left menu, select \"Environment Settings.\"        In the Environment Settings blade, click on \"Add environment\" and \"Azure DevOps.\"        Defender for Cloud needs a resource created in Azure for the connection to work. That is why the connection steps ask you for a resource group and a name. Fill in a name and select or create a resource group. In the next step, select the right plan. For now, there is just one option called \"DevOps.\"        When the plan is selected, the authorization to Azure DevOps can be configured. Log in to your account for Azure DevOps and select the organization and projects for which you need to configure Defender for DevOps.        The last step is to create the connection. Click on create and wait until the link is deployed and configured.       After a while will start to pop up in Defender for DevOps. With this, you will be able to mitigate vulnerabilities in Azure DevOps.    Connecting GitHub    Connecting GitHub is almost the same as connecting Azure DevOps except for the connection and authorization. Let's discover this by following the below steps:     Start from \"Defender for Cloud.\" In the left menu, select \"Environment Settings.\" On the \"Add Environment\" step, select \"GitHub.\" The connection details and plans blade are identical to the Azure DevOps connection. Fill in the correct info and proceed to the Authorization blade. In the \"Authorize connection\" blade, authorize the connection in two steps. The first step is to supply the credentials and the second step is to install the Defender for DevOps app on the repositories of your choice.       Just as with the DevOps connection for Azure DevOps, it will take a while before the information is displayed in the Defender for DevOps tab.       Azure Resources    You can check the resource group for the created resources/connections if interested. To see the resources check the box \"Show hidden types\" to make the connections visible.       Information    If you want to learn more about Microsoft Defender for Cloud or especially Defender for DevOps, check out the following resources:     Look Inside: Microsoft Defender for Cloud | Microsoft Press Store Microsoft Defender for Cloud documentation | Microsoft Learn Microsoft Defender for DevOps - the benefits and features | Microsoft Learn   ","categories": ["Azure"],
        "tags": ["Azure","Azure DevOps","Defender","DevOps"],
        "url": "/2022/10/microsoft-defender-for-devops/",
        "teaser": null
      },{
        "title": "Shift Left Security with Microsoft Security DevOps (MSDO)",
        "excerpt":"With the new capabilities released add Ignite, you are now even more capable of shifting security checks further to the left. In this article, I explain how to get started by adding DevOps and GitHub to Defender for Cloud, which does the first part. In this article, we will look more into integrating security right into DevOps pipelines.    To integrate the security checks in your pipeline and, Ideally, also in your pull request annotations, some prerequisites are needed that are by default not in Azure DevOps:     The Microsoft Security DevOps extensions: This is an Azure DevOps extension required to do a security scan within Azure DevOps. The SARIF SAST Scans Tab extensions: This Azure DevOps extension adds a Scan tab to the Azure DevOps pipeline that shows scan results.    These extensions can be installed from the Visual Studio Marketplace:     Microsoft Security DevOps - Visual Studio Marketplace SARIF SAST Scans Tab - Visual Studio Marketplace    The Microsoft Security DevOps Extension    The Microsoft Security DevOps extension is a wrapper around the Microsoft.Security.DevOps.Cli. The CLI is the Microsoft Security DevOps (MSDO), a command-line application that integrates static analysis tools into the development cycle.     The tool installs and configures static analysis tools and saves the results in a format called SARIF. In the table below, the tools it uses are listed.        Name Language     Bandit Python   BinSkim binary - Windows, ELF   ESlint JavaScript   Template Analyzer Infrastructure-as-code (IaC), ARM templates   Terrascan Infrastructure-as-code (IaC), Terraform (HCL2), Kubernetes (JSON/YAML), Helm v3, Kustomize, Dockerfiles   Trivy container images, file systems, and git repositories      Getting Started    First, create a new Pipeline in Azure DevOps and make sure that the pipeline supports \".Net 3.1\" and \".Net 6.0\". These are required to run the Security DevOps Extension, which can be done by adding the tasks below to the pipeline.    - task: UseDotNet@2   displayName: 'Use dotnet 3.1'   inputs:     version: 3.1.x - task: UseDotNet@2   displayName: 'Use dotnet 6.0'   inputs:     version: 6.0.x   These actions must be run before the extension itself to ensure that all components on the build agent are configured, and the Security for DevOps scan can run successfully.    - task: MicrosoftSecurityDevOps@1   displayName: 'Run Microsoft Security DevOps'   The above task executes the scanner and publishes the result by default in the \"CodeAnalysisLogs\" artifact. To display the scan results, this artifact needs to be published. When published, the scan results will appear in the pipeline's \"Scans\" tab. To publish the results, add the below Publish task.    - task: PublishBuildArtifacts@1   condition: always()   inputs:     PathtoPublish: '$(Build.ArtifactStagingDirectory)'     ArtifactName: 'CodeAnalysisLogs'     publishLocation: 'Container'   Putting all of this together ensures that the results are published and viewable in the \"Scans\" tab when you run the pipeline. The example below shows the result of a scan on one of my repos that contains some bicep files.         The complete pipeline YAML file looks like below. Some parts are left out for simplicity.    trigger: - main  pool:   vmImage: windows-latest  variables:   - name: system.debug     value: true  steps: - task: UseDotNet@2   displayName: 'Use dotnet 3.1'   inputs:     version: 3.1.x - task: UseDotNet@2   displayName: 'Use dotnet 6.0'   inputs:     version: 6.0.x - task: MicrosoftSecurityDevOps@1   displayName: 'Run Microsoft Security DevOps' - task: PublishBuildArtifacts@1   condition: always()   inputs:     PathtoPublish: '$(Build.ArtifactStagingDirectory)'     ArtifactName: 'CodeAnalysisLogs'     publishLocation: 'Container'   Information    If you want to learn more about Microsoft Defender for Cloud or especially Defender for DevOps, check out the following resources:     Look Inside: Microsoft Defender for Cloud | Microsoft Press Store Microsoft Defender for Cloud documentation | Microsoft Learn Microsoft Defender for DevOps - the benefits and features | Microsoft Learn ","categories": ["Azure","Azure DevOps"],
        "tags": ["Azure DevOps","Defender","Security","Shift Left"],
        "url": "/2022/10/shift-left-security-with-microsoft-security-devops-msdo/",
        "teaser": null
      },{
        "title": "Centrally manage your App Configurations",
        "excerpt":"The application landscape in Azure has grown significantly in recent years, with a wide range of tools and services available to help businesses build, deploy, and manage their applications in the cloud. From infrastructure as a service (IaaS) offerings like virtual machines and storage to the platform as a service (PaaS) offerings like Azure App Service and Azure Functions, Azure provides a comprehensive set of tools and services to meet the needs of businesses of all sizes.    This also has an impact on the application landscape of businesses within Azure. As Azure also evolves, the applications/services that are used evolve. This is not always going in a correct manner where there is time to remove technical debt. As the landscape expands, new services are created, and configurations are added.     This also surfaces a problem: when a configuration needs to be changed, this must be done on multiple locations, and you are bound to forget one.    Azure App Configuration    This is where Azure App Configuration comes in. Azure App Configuration is a fully managed service that lets you centralize your application's configuration and feature management. It helps to store and manage configuration data and feature flags in a centralized location, which multiple applications and environments can access.    One of the key benefits of using Azure App Configuration is that it allows you to manage the configuration of your applications in a consistent and organized manner. Instead of hardcoding configuration values into your application's codebase, you can store them in Azure App Configuration and retrieve them at runtime. This makes it easier to manage and update your application's configuration without redeploying your code.     App Configuration is already a pervasive solution that (at the time of writing this article) has the following capabilities:      Automatic refresh without restarting an application   Data encryption (transit/rest)   Point-in-time snapshots   Configuration comparison   Feature Management   Import / Export   Geo-Replication (preview)   Soft Delete   AAD authentication   Private Endpoint support    App Configuration can be used within many frameworks by using a specific client or provider or by using the Rest API:      .Net Core   ASP.Net Core   .Net Framework   Java Spring   Javascript   Python    When you would like to use it in, for example, PowerShell, you could leverage the API.    KeyVault integration    As the feature list above isn't already enough, Azure App Configuration also has KeyVault integration. With the KeyVault integration, you can add configurations referencing a KeyVault secret. Azure App Configuration will redirect you (your principal with a correct token) to retrieve the value from the KeyVault without noticing anything.    Getting Started    Of course, you can get started by using the Azure Portal, PowerShell, or the CLI, but let's check if we can create the service using Bicep.    Bicep    The bicep for setting up Azure App Configuration is very easy. Let's take a look at the example below.    resource configStore 'Microsoft.AppConfiguration/configurationStores@2021-10-01-preview' = {     name: 'azappconfiguration-${name}'     location: location     sku: {         name: 'standard'     }     properties:{         disableLocalAuth: true         enablePurgeProtection: true         softDeleteRetentionInDays:7     } }   The code snippet creates a configuration store with the 'standard' SKU, enables purge protection, and sets the soft delete retention to 7 days. Next to that, it also disables the local authentication, meaning that you cannot authenticate to the configuration store by using a key but are required to use a token to authenticate.    Configurations, secret references, and features can also be added by using Bicep. For this, I created a handy module.    param configStoreName string  param configItems array  resource configStore 'Microsoft.AppConfiguration/configurationStores@2021-10-01-preview' existing = {   name: configStoreName }  resource configStoreKeyValue 'Microsoft.AppConfiguration/configurationStores/keyValues@2021-10-01-preview' = [for item in configItems:  {   parent: configStore   name: (!item.featureFlag) ? item.name : '.appconfig.featureflag~2F${item.name}'   properties: {     value: (!item.featureFlag) ? item.value : '{\"id\": \"${item.name}\", \"description\": \"\", \"enabled\": false, \"conditions\": {\"client_filters\":[]}}'     tags: item.tags     contentType:item.contentType   } }]   The item will be configured correctly based on the array supplied as a parameter. A sample array for the configuration could look like the snippet below.    [     {         name: 'Bicep:Config:Value'         value: 'Test from Bicep'         contenttype: ''         featureFlag: false         tags: {             Bicep: 'Deployed'         }     }         {         name: 'Bicep:Secret:KeyVault'         value: 'https://azkv-appconfiguration123.vault.azure.net/secrets/bicep-configuration-secret'         contenttype: 'application/vnd.microsoft.appconfig.keyvaultref+json;charset=utf-8'         featureFlag: false         tags: {             Bicep: 'Deployed'         }     }     {         name: 'bicep-featureflag'         value: ''         contenttype: 'application/vnd.microsoft.appconfig.ff+json;charset=utf-8'         featureFlag: true         tags: {             Bicep: 'Deployed'         }     } ]   Getting it in Code    Adding Azure App Configuration in code is very easy. This article will look into C# and .Net Core 6. Make sure you add the following prerequisites as NuGet packages to your project.      Microsoft.Extensions.Configuration.AzureAppConfiguration version 4.1.0 or later   Microsoft.Azure.Functions.Extensions version 1.10 or later: This one is needed to incorporate the Azure Functions configurations.    The next step is to add the following code to your application startup.    var endpoint = app.Configuration[\"AppConfig:Endpoint\"]; builder.Configuration.AddAzureAppConfiguration(options => {     options.Connect(new Uri(endpoint), new DefaultAzureCredential())             .ConfigureKeyVault(kv => {                 kv.SetCredential(new DefaultAzureCredential());             }) \t     .Select(\"Demo:*\", LabelFilter.Null)             .ConfigureRefresh(refreshOptions =>                 refreshOptions.Register(\"Demo:Config:Sentinel\", refreshAll: true)); });   On line 1 of the snippet, we retrieve the configuration store's endpoint and then aff Azure App Configuration to the application.  By using 'DefaultAzureCredential', we make sure that we connect to the configuration store by the managed identity of the service.  On line 4, we then set up the connection to the KeyVault to retrieve values and specify that for this, we also want to use the managed identity.    With the 'Select' we start specifying which configurations we want. In this example, we want all configurations that start with 'Demo:' and do not have a label. Using the labels, we could have specified an environment, for example.     On line 9, we then configure the refresh options to ensure that the application configurations are automatically refreshed when we update the sentinel key \"Demo:Config:Sentinel\" in the configuration store.    When you would also like to make use of feature management, you would also add the following lines    options.UseFeatureFlags(featureFlagOptions => {         featureFlagOptions.Select(\"DemoApp-*\", app.Environment.EnvironmentName);         featureFlagOptions.CacheExpirationInterval = TimeSpan.FromSeconds(30);     });   Using the configuration    Using the configuration is now very easy the below snippet is a function that retrieves a configuration value.    public class DummyFunction     {         private readonly IConfiguration _configuration;          public DummyFunction(IConfiguration configuration) {             _configuration = configuration;         }          [FunctionName(\"DummyFunction\")]         public async Task&lt;IActionResult> Run(             [HttpTrigger(AuthorizationLevel.Function, \"get\", \"post\", Route = null)] HttpRequest req,             ILogger log)         {             log.LogInformation(\"C# HTTP trigger function processed a request.\");              string configKey = \"DemoFunc:Message\";             string message = _configuration[configKey];                          log.LogInformation($\"Found the config in Azure App Configuration {message}\");              string requestBody = await new StreamReader(req.Body).ReadToEndAsync();              string responseMessage = string.IsNullOrEmpty(message)                 ? \"There is no configuration value with the key 'Demo:FunctionApp:Message' in Azure App Configuration\"                 : message;              return new OkObjectResult(responseMessage);         }     }   Also, for features, this more or less looks the same. The only difference here is that we use a so-called FeatureManager. For this snippet, we removed some lines of code for simplicity.    public class WCController : ControllerBase {                 private readonly ILogger&lt;WCController> _logger;          private readonly IFeatureManager _featureManager;          public WCController(ILogger&lt;WCController> logger, IFeatureManager manager) {             _logger = logger;             _featureManager = manager;         }          [HttpGet(Name = \"GetTeams\")]         public IEnumerable&lt;Teams> Get() {              IEnumerable&lt;Teams> retVal = new List&lt;Teams>();              if (_featureManager.IsEnabledAsync(\"DemoApi-Points\").Result) {                             } else {                             }              return retVal;         }     }   When using features, you also have some other nice options as:      Feature Gates: The feature gate attributes make sure something is only available when the specified feature is activated.        [FeatureGate(\"DemoApi-WC\")]     Feature Tag: The &lt;feature&gt;&nbsp;tag ensures that items are shown only when the&nbsp;feature flag is enabled.        &lt;feature name=\"DemoApp-Beta\">         &lt;p>Beta feature is enabled!&lt;/p>     &lt;/feature>   Azure DevOps    In a real-life scenario, configurations are mostly managed and deployed from Azure DevOps. Azure App Configuration can also help in these situations because Azure DevOps has a task that retrieves configuration values and converts them to variables.    Take a look at the pipeline in the below snippet. In the first steps, the configuration is retrieved and later displayed with a PowerShell task. Good to mention here as well is that KeyVault references are also retrieved and specified as secure variables.    trigger: - main  pool:   vmImage: ubuntu-latest  steps:  - task: AzureAppConfiguration@5   displayName: Get Azure App Configurations   inputs:     azureSubscription: 'sub-sub-sub'     AppConfigurationEndpoint: 'https://azapp-sub.azconfig.io'     KeyFilter: 'DevOps:*' - task: PowerShell@2   displayName: Display values from App Configuration   inputs:     targetType: 'inline'     script: |       Write-Host \"Regular value: $(DevOps:DemoValue)\"       Write-Host \"Secret value: $(DevOps:Secret:DevOpsSecret)\"    My Take away’s      Issues in the configuration can cause the application to break on start-up: As configurations are loaded at the application's startup, problems with the configuration or lack of permissions can break the complete application.   Do not forget to authorize the application in the Azure Key Vault: When working with KeyVault references, ensure that the identity used has the appropriate permissions on Azure App Configuration and the KeyVault.   Use labels to sort/group configurations: Labels add the ability to have different configurations for different scenarios like environments.   RBAC permissions can take time before they are in place.   Case Sensitive: All configurations added are case sensitive.       Conclusion    In conclusion, Azure App Configuration is a powerful service for managing your applications' configuration and feature management. Centralizing your configuration data and providing feature management capabilities helps you build more flexible and maintainable applications.    If you want to see the code in more detail and look at different examples, go check out my GitHub repo.    Chrismas Present    As a small present for Christmas, I also wanted to share an option we use very often. That option is using Azure App Configuration from PowerShell.     Function Get-AzAppConfigurationKey {     Param(         [parameter(Mandatory = $true)][string]$AppConfiguration,         [parameter(Mandatory = $true)][string]$Key     )  ","categories": ["Azure"],
        "tags": ["Azure","Configuration"],
        "url": "/2022/12/centrally-manage-your-app-configurations/",
        "teaser": null
      },{
        "title": "\"Exposing Azure App Registrations as Secure APIs: A Guide to Authentication with 'User Assignments Required' Turned On\"",
        "excerpt":"Azure App Registrations are a powerful tool for managing resource access and integrating applications with Microsoft's cloud services. While these registrations are typically used to grant applications access to other Azure resources, they can also be exposed as APIs, allowing external applications to interact with the registered application's resources securely.    In this blog post, we'll explore exposing an Azure App Registration as an API, including the necessary configuration to authenticate towards the application when the application is configured with 'User Assignments Required' turned on. This short guide tells you how to configure this.    This guide talks about two different Application Registrations.      The application you are authenticating to. (This is the application registered on, for example, an Azure App Service)   The application you are authenticating with. (This is the application you will use to retrieve data from, for example, an API)    Step 1: Expose an API    Ensure the application you are authenticating to (1) has an Application ID Url configured within the App Registration blade of the application.       If this is not configured, make sure to add it.    Step 2: Create an App Role    An 'App Role' needs to be defined to authenticate your application. For this, go to the 'App Role' blade for the App Registration you are authenticating to (1).       If an App Role does not exist, create a new one and fill in the required properties. Make sure also to select Applications in the allowed \"member types\" and enable it. Adding these roles makes sure that the roles are added to the token of the application.        Field Description Example   Display Name Display the name for the app role that appears in the admin consent and app assignment experiences. This value may contain spaces. Survey Writer   Allowed member types Specifies whether this app role can be assigned to users, applications, or both. Users/Groups   Value Specifying the roles' value claim that the application should expect in the token. The value should match the string referenced in the application's code, which can't contain spaces. Survey. Create   Description A more detailed description of the app role displayed during admin app assignment and consent experiences. Writers can create surveys.   Do you want to enable this app role? Specifies whether the app role is enabled. To delete an app role, deselect this checkbox and apply the change before attempting the delete operation. Checked       Step 3: Add application API Permission    On the 'API Permission' blade of the application you are authenticating with (2), the required permissions for the application need to be configured. In the blade, click 'Add permission.'    Then go to the tab \"APIs\" my organization uses and search for your App Registration. You should be able to see the name within the list.       Click on the application. On the next screen, you should be able to see the roles that you can choose. Select the permissions that are required and click on \"Add Permissions.\"       Step 4: Admin Consent    These types of app roles require an 'Admin Consent.' After adding the permission, you will be returned to the API permissions blade. In this blade, click on 'Grant admin consent for.'      ","categories": ["Azure"],
        "tags": [],
        "url": "/2023/03/unlocking-the-power-of-azure-app-registrations-a-guide-to-exposing-your-app-as-an-api-with-user-assignments-required-turned-on/",
        "teaser": null
      },{
        "title": "Creating a Logic App API Connection that uses the Managed Identity using Bicep",
        "excerpt":"Logic Apps in Azure provides a platform for building workflows that integrate with various services and APIs. When creating Logic Apps, you must often connect securely to other services like the Azure Service Bus and Azure Storage Accounts. Managed Identity is a recommended way to accomplish this.    Creating these API Connections with Infrastructure as Code isn't documented well and is challenging to figure out. It took me some time, but I figured it out by looking at the API requests that the portal does.    Figuring it out    The steps I have taken to figure it out can be applied in different scenarios for Logic Apps but, for example, also on other parts of the portal.      Create a new Logic App in the Azure portal.    Add an Identity to the Logic App and give that identity access to the Azure Service you want to use. In this example, we will be using an Azure Service Bus.   Add a Service Bus trigger to the Logic App and fill in the information below.         Before you click 'create,' open de Edge DevTools (F12) and open de network tab         Clear the network items list and click 'Create.'   When the trigger is refreshed, stop the network trace and search for an item that puts information into Azure. When using the Service Bus, the item will look like below.         On the right side of the network trace, click on the Payload tab to find the information sent to the Azure Resource Manager. This contains the information that needs to be used in Bicep.       {   \"id\": \"/subscriptions/f124b668-7e3d-4b53-ba80-09c364def1f3/providers/Microsoft.Web/locations/westeurope/managedApis/servicebus\",   \"parameterValueSet\": {     \"name\": \"managedIdentityAuth\",     \"values\": {       \"namespaceEndpoint\": {         \"value\": \"sb://azsb-temp.servicebus.windows.net\"       }     }   },   \"displayName\": \"servicebus-auth\",   \"kind\": \"V1\",   \"location\": \"westeurope\" }   The outcome in Bicep    With the findings, the specific Bicep code can be written. Below are three different API connections that use Managed Identities for the connection.    Azure Storage Account API Connection with Managed Identity    resource storageaccountApiConnectionAuth 'Microsoft.Web/connections@2016-06-01'= {   name: 'azuretables-auth'   location: location   properties: {     api: {       id: 'subscriptions/${subscription().subscriptionId}/providers/Microsoft.Web/locations/${location}/managedApis/azuretables'     }     parameterValueSet: {       name: 'managedIdentityAuth'       values: {}     }     displayName: 'azuretables-auth'   } }   Azure Service Bus API Connection with Managed Identity    resource azla_apiconnection_servicebus_auth 'Microsoft.Web/connections@2016-06-01' = {   name: 'servicebus-auth'   location: location   properties: {     displayName: 'servicebus-auth'     api: {       id: subscriptionResourceId('Microsoft.Web/locations/managedApis', location, 'servicebus')     }     parameterValueSet: {       name: 'managedIdentityAuth'       values: {         namespaceEndpoint: {           value: 'sb://${serviceBus}.servicebus.windows.net/'         }       }     }   } }   I hope that this helps you in creating epic Bicep files. If you are looking for more information, be sure to look at the following:      Learn Bicep   Microsoft.Web connections   ","categories": ["Azure","Development"],
        "tags": ["Azure","bicep","Logic Apps"],
        "url": "/2023/05/creating-a-logic-app-api-connection-that-uses-the-managed-identity-using-bicep/",
        "teaser": null
      }]
